{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'\n",
    "X_path = 'data/X.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    # Read the files\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Get stats for each file based on name\n",
    "    files_ = []\n",
    "    for file in files:\n",
    "        data = {'file': file,\n",
    "                's': file[0:4],\n",
    "                'c': file[4:8],\n",
    "                'p': file[8:12],\n",
    "                'r': file[12:16],\n",
    "                'a': file[16:20]\n",
    "                }\n",
    "        files_.append(data)\n",
    "\n",
    "    return files_\n",
    "files_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load X from pickle\n",
      "X loaded from pickle\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load X and Y from pickle before generating them\n",
    "X = {}\n",
    "try:\n",
    "    print('Attempting to load X from pickle')\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print('X loaded from pickle')\n",
    "except:\n",
    "    print('Could not load X and Y, generating them now')\n",
    "    \n",
    "    # Read the files\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Get stats for each file based on name\n",
    "    files_ = []\n",
    "    for file in files:\n",
    "        data = {'file': file,\n",
    "                's': file[0:4],\n",
    "                'c': file[4:8],\n",
    "                'p': file[8:12],\n",
    "                'r': file[12:16],\n",
    "                'a': file[16:20]\n",
    "                }\n",
    "        files_.append(data)\n",
    "\n",
    "    # Generate X and Y\n",
    "    for file_ in tqdm(files_, desc='Files Parsed', position=0):\n",
    "        try:\n",
    "            file = join(path, file_['file'])\n",
    "            data = open(file, 'r')\n",
    "            lines = data.readlines()\n",
    "            frames_count = int(lines.pop(0).replace('\\n', ''))\n",
    "            file_['frames'] = frames_count\n",
    "        except UnicodeDecodeError: # .DS_Store file\n",
    "            print('UnicodeDecodeError: ', file)\n",
    "            continue\n",
    "\n",
    "        # Add filename as key to X\n",
    "        X[file_['file']] = []\n",
    "\n",
    "        # Skip file if 2 actors\n",
    "        if lines[0].replace('\\n', '') != '1': continue\n",
    "\n",
    "        for f in tqdm(range(frames_count), desc='Frames Parsed', position=1, leave=False):\n",
    "            try:\n",
    "                # Get actor count\n",
    "                actors = int(lines.pop(0).replace('\\n', ''))\n",
    "            \n",
    "                # Get actor info\n",
    "                t = lines.pop(0)\n",
    "\n",
    "                # Get joint count\n",
    "                joint_count = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                # Get joint info\n",
    "                d = []\n",
    "                for j in range(joint_count):\n",
    "                    joint = lines.pop(0).replace('\\n', '').split(' ')\n",
    "                    d.extend(joint[0:3])\n",
    "\n",
    "                # Skip if not 25 joints\n",
    "                if len(d) != 75: continue\n",
    "\n",
    "                # Convert to numpy array\n",
    "                d = np.array(d)\n",
    "\n",
    "                # Append to X and Y\n",
    "                X[file_['file']].append(d)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X[file_['file']] = np.array(X[file_['file']], dtype=np.float16)\n",
    "\n",
    "        # Pad X size to 300 frames (300 is max frames in dataset)\n",
    "        X[file_['file']] = np.pad(X[file_['file']], ((0, 300-X[file_['file']].shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "    print('X Generated, saving to pickle...')\n",
    "\n",
    "    # Save the data\n",
    "    with open(X_path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    print('X Saved to pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in X:\n",
    "    # Clip X to 50 frames\n",
    "    X[file] = X[file][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816130bb92a945518d9e6c3db10e79bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f1164fc9d04719944c7ba1aa6b0536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c532c7424eb409fa8f92509483c7489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "same_samples_per_actor = 100\n",
    "diff_samples_per_actor = 100\n",
    "\n",
    "\n",
    "def data_generator_per_actor(X, same_samples_per_actor=1000, diff_samples_per_actor=1000, train=True):\n",
    "    actor_data = {}\n",
    "    for file in X:\n",
    "        actor = int(file[9:12])\n",
    "        action = int(file[17:20])\n",
    "\n",
    "        if train: \n",
    "            if action > 60:\n",
    "                continue\n",
    "        else:\n",
    "            if action <= 60:\n",
    "                continue\n",
    "\n",
    "        if actor not in actor_data:\n",
    "            actor_data[actor] = []\n",
    "        if len(X[file]) == 0:\n",
    "            continue\n",
    "        actor_data[actor].append(X[file])\n",
    "\n",
    "    actor_keys = list(actor_data.keys())\n",
    "\n",
    "    samples = []\n",
    "    for actor in tqdm(actor_keys):\n",
    "        for _ in range(same_samples_per_actor):\n",
    "            same_video1 = random.choice(actor_data[actor])\n",
    "            same_video2 = random.choice(actor_data[actor])\n",
    "            samples.append(((tf.convert_to_tensor(same_video1, dtype=tf.float32), tf.convert_to_tensor(same_video2, dtype=tf.float32)), tf.constant([1.0], dtype=tf.float32)))\n",
    "\n",
    "        for _ in range(diff_samples_per_actor):\n",
    "            while True:\n",
    "                diff_actor = random.choice(actor_keys)\n",
    "                if diff_actor != actor:\n",
    "                    break\n",
    "            same_video1 = random.choice(actor_data[actor])\n",
    "            diff_video = random.choice(actor_data[diff_actor])\n",
    "            samples.append(((tf.convert_to_tensor(same_video1, dtype=tf.float32), tf.convert_to_tensor(diff_video, dtype=tf.float32)), tf.constant([0.0], dtype=tf.float32)))\n",
    "\n",
    "        random.shuffle(samples)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "train_gen = data_generator_per_actor(X, same_samples_per_actor, diff_samples_per_actor, train=True)\n",
    "val_gen = data_generator_per_actor(X, same_samples_per_actor, diff_samples_per_actor, train=False)\n",
    "test_gen = data_generator_per_actor(X, same_samples_per_actor, diff_samples_per_actor, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output, _ = self.mha(x, x, return_attention_scores=True)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class SimpleTransformer(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.transformer_layers = [TransformerBlock(d_model, num_heads, dff) for _ in range(num_layers)]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x_a = inputs[0]\n",
    "        x_b = inputs[1]\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x_a = transformer_layer(x_a, training=training)\n",
    "            x_b = transformer_layer(x_b, training=training)\n",
    "        x_a = self.global_average_pooling(x_a)\n",
    "        x_b = self.global_average_pooling(x_b)\n",
    "        x = x_a*x_b\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "model = SimpleTransformer(num_layers=3, d_model=75, num_heads=5, dff=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "125/125 [==============================] - 25s 161ms/step - loss: 0.7577 - accuracy: 0.5059 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.6988 - accuracy: 0.4919 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.6972 - accuracy: 0.4959 - val_loss: 0.6958 - val_accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "125/125 [==============================] - 11s 91ms/step - loss: 0.6964 - accuracy: 0.4870 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6957 - accuracy: 0.4885 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 6/1000\n",
      "125/125 [==============================] - 12s 96ms/step - loss: 0.6955 - accuracy: 0.4921 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 7/1000\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.6951 - accuracy: 0.4947 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "125/125 [==============================] - 10s 79ms/step - loss: 0.6948 - accuracy: 0.4964 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "125/125 [==============================] - 10s 82ms/step - loss: 0.6946 - accuracy: 0.4971 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.6946 - accuracy: 0.4996 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "125/125 [==============================] - 15s 124ms/step - loss: 0.6944 - accuracy: 0.4994 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.6944 - accuracy: 0.4974 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "125/125 [==============================] - 12s 95ms/step - loss: 0.6944 - accuracy: 0.4942 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.6942 - accuracy: 0.4950 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "125/125 [==============================] - 13s 104ms/step - loss: 0.6941 - accuracy: 0.4971 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 0.6922 - accuracy: 0.5080 - val_loss: 0.6906 - val_accuracy: 0.5081\n",
      "Epoch 17/1000\n",
      "125/125 [==============================] - 9s 74ms/step - loss: 0.6945 - accuracy: 0.5121 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.6942 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "125/125 [==============================] - 12s 95ms/step - loss: 0.6846 - accuracy: 0.5329 - val_loss: 0.6900 - val_accuracy: 0.5412\n",
      "Epoch 20/1000\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.6659 - accuracy: 0.5941 - val_loss: 0.6851 - val_accuracy: 0.5609\n",
      "Epoch 21/1000\n",
      "125/125 [==============================] - 12s 98ms/step - loss: 0.6593 - accuracy: 0.5985 - val_loss: 0.6855 - val_accuracy: 0.5640\n",
      "Epoch 22/1000\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.6563 - accuracy: 0.6036 - val_loss: 0.6812 - val_accuracy: 0.5694\n",
      "Epoch 23/1000\n",
      "125/125 [==============================] - 11s 87ms/step - loss: 0.6531 - accuracy: 0.6077 - val_loss: 0.6828 - val_accuracy: 0.5646\n",
      "Epoch 24/1000\n",
      "125/125 [==============================] - 10s 82ms/step - loss: 0.6496 - accuracy: 0.6154 - val_loss: 0.6848 - val_accuracy: 0.5609\n",
      "Epoch 25/1000\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.6494 - accuracy: 0.6158 - val_loss: 0.6816 - val_accuracy: 0.5604\n",
      "Epoch 26/1000\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.6481 - accuracy: 0.6148 - val_loss: 0.6855 - val_accuracy: 0.5650\n",
      "Epoch 27/1000\n",
      "125/125 [==============================] - 9s 76ms/step - loss: 0.6479 - accuracy: 0.6205 - val_loss: 0.6856 - val_accuracy: 0.5638\n",
      "Epoch 28/1000\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.6428 - accuracy: 0.6190 - val_loss: 0.6864 - val_accuracy: 0.5637\n",
      "Epoch 29/1000\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.6456 - accuracy: 0.6194 - val_loss: 0.6845 - val_accuracy: 0.5612\n",
      "Epoch 30/1000\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.6406 - accuracy: 0.6241 - val_loss: 0.6820 - val_accuracy: 0.5658\n",
      "Epoch 31/1000\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.6440 - accuracy: 0.6192 - val_loss: 0.6853 - val_accuracy: 0.5604\n",
      "Epoch 32/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.6409 - accuracy: 0.6258 - val_loss: 0.6857 - val_accuracy: 0.5565\n",
      "Epoch 33/1000\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.6366 - accuracy: 0.6273 - val_loss: 0.6879 - val_accuracy: 0.5640\n",
      "Epoch 34/1000\n",
      "125/125 [==============================] - 13s 108ms/step - loss: 0.6372 - accuracy: 0.6242 - val_loss: 0.6829 - val_accuracy: 0.5575\n",
      "Epoch 35/1000\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.6346 - accuracy: 0.6329 - val_loss: 0.6876 - val_accuracy: 0.5596\n",
      "Epoch 36/1000\n",
      "125/125 [==============================] - 14s 110ms/step - loss: 0.6299 - accuracy: 0.6373 - val_loss: 0.6927 - val_accuracy: 0.5645\n",
      "Epoch 37/1000\n",
      "125/125 [==============================] - 13s 108ms/step - loss: 0.6333 - accuracy: 0.6341 - val_loss: 0.6845 - val_accuracy: 0.5841\n",
      "Epoch 38/1000\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.6302 - accuracy: 0.6375 - val_loss: 0.6880 - val_accuracy: 0.5714\n",
      "Epoch 39/1000\n",
      "125/125 [==============================] - 10s 84ms/step - loss: 0.6496 - accuracy: 0.6081 - val_loss: 0.6819 - val_accuracy: 0.5714\n",
      "Epoch 40/1000\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6493 - accuracy: 0.6198 - val_loss: 0.6801 - val_accuracy: 0.5780\n",
      "Epoch 41/1000\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.6420 - accuracy: 0.6252 - val_loss: 0.6746 - val_accuracy: 0.5872\n",
      "Epoch 42/1000\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 0.6283 - accuracy: 0.6409 - val_loss: 0.6957 - val_accuracy: 0.5479\n",
      "Epoch 43/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.6273 - accuracy: 0.6435 - val_loss: 0.6918 - val_accuracy: 0.5625\n",
      "Epoch 44/1000\n",
      "125/125 [==============================] - 12s 98ms/step - loss: 0.6260 - accuracy: 0.6430 - val_loss: 0.6938 - val_accuracy: 0.5536\n",
      "Epoch 45/1000\n",
      "125/125 [==============================] - 11s 86ms/step - loss: 0.6200 - accuracy: 0.6461 - val_loss: 0.7161 - val_accuracy: 0.5361\n",
      "Epoch 46/1000\n",
      "125/125 [==============================] - 11s 87ms/step - loss: 0.6270 - accuracy: 0.6348 - val_loss: 0.6783 - val_accuracy: 0.5779\n",
      "Epoch 47/1000\n",
      "125/125 [==============================] - 11s 91ms/step - loss: 0.6193 - accuracy: 0.6478 - val_loss: 0.6798 - val_accuracy: 0.5739\n",
      "Epoch 48/1000\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.6211 - accuracy: 0.6515 - val_loss: 0.6970 - val_accuracy: 0.5489\n",
      "Epoch 49/1000\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.6205 - accuracy: 0.6515 - val_loss: 0.6847 - val_accuracy: 0.5607\n",
      "Epoch 50/1000\n",
      "125/125 [==============================] - 11s 86ms/step - loss: 0.6081 - accuracy: 0.6578 - val_loss: 0.6875 - val_accuracy: 0.5612\n",
      "Epoch 51/1000\n",
      "125/125 [==============================] - 12s 93ms/step - loss: 0.6081 - accuracy: 0.6593 - val_loss: 0.7076 - val_accuracy: 0.5508\n",
      "Epoch 52/1000\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.6063 - accuracy: 0.6625 - val_loss: 0.7072 - val_accuracy: 0.5468\n",
      "Epoch 53/1000\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.6107 - accuracy: 0.6643 - val_loss: 0.6884 - val_accuracy: 0.5647\n",
      "Epoch 54/1000\n",
      "125/125 [==============================] - 12s 96ms/step - loss: 0.6013 - accuracy: 0.6700 - val_loss: 0.7098 - val_accuracy: 0.5376\n",
      "Epoch 55/1000\n",
      "125/125 [==============================] - 10s 77ms/step - loss: 0.6121 - accuracy: 0.6570 - val_loss: 0.6970 - val_accuracy: 0.5825\n",
      "Epoch 56/1000\n",
      "125/125 [==============================] - 12s 93ms/step - loss: 0.6062 - accuracy: 0.6696 - val_loss: 0.6990 - val_accuracy: 0.5582\n",
      "Epoch 57/1000\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 0.6115 - accuracy: 0.6609 - val_loss: 0.6863 - val_accuracy: 0.5789\n",
      "Epoch 58/1000\n",
      "125/125 [==============================] - 13s 104ms/step - loss: 0.5953 - accuracy: 0.6795 - val_loss: 0.7017 - val_accuracy: 0.5951\n",
      "Epoch 59/1000\n",
      "125/125 [==============================] - 13s 104ms/step - loss: 0.6032 - accuracy: 0.6687 - val_loss: 0.6939 - val_accuracy: 0.5828\n",
      "Epoch 60/1000\n",
      "125/125 [==============================] - 11s 86ms/step - loss: 0.5964 - accuracy: 0.6776 - val_loss: 0.7093 - val_accuracy: 0.5480\n",
      "Epoch 61/1000\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 0.5970 - accuracy: 0.6736 - val_loss: 0.7108 - val_accuracy: 0.5480\n",
      "Epoch 62/1000\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.5930 - accuracy: 0.6749 - val_loss: 0.6882 - val_accuracy: 0.5785\n",
      "Epoch 63/1000\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.5920 - accuracy: 0.6749 - val_loss: 0.7078 - val_accuracy: 0.5532\n",
      "Epoch 64/1000\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5979 - accuracy: 0.6800 - val_loss: 0.7315 - val_accuracy: 0.5236\n",
      "Epoch 65/1000\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.5960 - accuracy: 0.6750 - val_loss: 0.7027 - val_accuracy: 0.5583\n",
      "Epoch 66/1000\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.5906 - accuracy: 0.6789 - val_loss: 0.6895 - val_accuracy: 0.5677\n",
      "Epoch 67/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5945 - accuracy: 0.6774 - val_loss: 0.7040 - val_accuracy: 0.5504\n",
      "Epoch 68/1000\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.5875 - accuracy: 0.6836 - val_loss: 0.6862 - val_accuracy: 0.5853\n",
      "Epoch 69/1000\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.5848 - accuracy: 0.6855 - val_loss: 0.6741 - val_accuracy: 0.6014\n",
      "Epoch 70/1000\n",
      "125/125 [==============================] - 14s 116ms/step - loss: 0.5873 - accuracy: 0.6850 - val_loss: 0.6903 - val_accuracy: 0.5688\n",
      "Epoch 71/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.5825 - accuracy: 0.6909 - val_loss: 0.7170 - val_accuracy: 0.5473\n",
      "Epoch 72/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.6162 - accuracy: 0.6500 - val_loss: 0.6918 - val_accuracy: 0.5196\n",
      "Epoch 73/1000\n",
      "125/125 [==============================] - 12s 96ms/step - loss: 0.6291 - accuracy: 0.6379 - val_loss: 0.6753 - val_accuracy: 0.5878\n",
      "Epoch 74/1000\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.5942 - accuracy: 0.6791 - val_loss: 0.6865 - val_accuracy: 0.5862\n",
      "Epoch 75/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5919 - accuracy: 0.6795 - val_loss: 0.6897 - val_accuracy: 0.5758\n",
      "Epoch 76/1000\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.5863 - accuracy: 0.6860 - val_loss: 0.7074 - val_accuracy: 0.5478\n",
      "Epoch 77/1000\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5839 - accuracy: 0.6846 - val_loss: 0.7047 - val_accuracy: 0.5736\n",
      "Epoch 78/1000\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.6052 - accuracy: 0.6672 - val_loss: 0.6753 - val_accuracy: 0.5798\n",
      "Epoch 79/1000\n",
      "125/125 [==============================] - 10s 84ms/step - loss: 0.5959 - accuracy: 0.6746 - val_loss: 0.6775 - val_accuracy: 0.5878\n",
      "Epoch 80/1000\n",
      "125/125 [==============================] - 11s 87ms/step - loss: 0.5927 - accuracy: 0.6790 - val_loss: 0.7178 - val_accuracy: 0.5730\n",
      "Epoch 81/1000\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.5789 - accuracy: 0.6915 - val_loss: 0.7158 - val_accuracy: 0.5633\n",
      "Epoch 82/1000\n",
      "125/125 [==============================] - 11s 90ms/step - loss: 0.5764 - accuracy: 0.6965 - val_loss: 0.6993 - val_accuracy: 0.5663\n",
      "Epoch 83/1000\n",
      "125/125 [==============================] - 10s 83ms/step - loss: 0.5762 - accuracy: 0.6927 - val_loss: 0.6924 - val_accuracy: 0.5767\n",
      "Epoch 84/1000\n",
      "125/125 [==============================] - 10s 80ms/step - loss: 0.5823 - accuracy: 0.6915 - val_loss: 0.7066 - val_accuracy: 0.5632\n",
      "Epoch 85/1000\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.5798 - accuracy: 0.6913 - val_loss: 0.7168 - val_accuracy: 0.5548\n",
      "Epoch 86/1000\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.5851 - accuracy: 0.6861 - val_loss: 0.7245 - val_accuracy: 0.5538\n",
      "Epoch 87/1000\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 0.5781 - accuracy: 0.6936 - val_loss: 0.7132 - val_accuracy: 0.5549\n",
      "Epoch 88/1000\n",
      "125/125 [==============================] - 9s 74ms/step - loss: 0.5751 - accuracy: 0.6946 - val_loss: 0.6967 - val_accuracy: 0.5658\n",
      "Epoch 89/1000\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 0.5721 - accuracy: 0.6980 - val_loss: 0.7126 - val_accuracy: 0.5517\n",
      "Epoch 90/1000\n",
      "125/125 [==============================] - 10s 80ms/step - loss: 0.5761 - accuracy: 0.6936 - val_loss: 0.6984 - val_accuracy: 0.5578\n",
      "Epoch 91/1000\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 0.5790 - accuracy: 0.6961 - val_loss: 0.6910 - val_accuracy: 0.6064\n",
      "Epoch 92/1000\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.5736 - accuracy: 0.6952 - val_loss: 0.7190 - val_accuracy: 0.5522\n",
      "Epoch 93/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.5708 - accuracy: 0.6979 - val_loss: 0.6978 - val_accuracy: 0.5638\n",
      "Epoch 94/1000\n",
      "125/125 [==============================] - 13s 108ms/step - loss: 0.5807 - accuracy: 0.6936 - val_loss: 0.7172 - val_accuracy: 0.5620\n",
      "Epoch 95/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5705 - accuracy: 0.7029 - val_loss: 0.7131 - val_accuracy: 0.5544\n",
      "Epoch 96/1000\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.6132 - accuracy: 0.6595 - val_loss: 0.6964 - val_accuracy: 0.5476\n",
      "Epoch 97/1000\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.5985 - accuracy: 0.6674 - val_loss: 0.7023 - val_accuracy: 0.5747\n",
      "Epoch 98/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5935 - accuracy: 0.6741 - val_loss: 0.6998 - val_accuracy: 0.5776\n",
      "Epoch 99/1000\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.6015 - accuracy: 0.6762 - val_loss: 0.6935 - val_accuracy: 0.5915\n",
      "Epoch 100/1000\n",
      "125/125 [==============================] - 17s 140ms/step - loss: 0.5968 - accuracy: 0.6727 - val_loss: 0.7013 - val_accuracy: 0.5775\n",
      "Epoch 101/1000\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.5937 - accuracy: 0.6783 - val_loss: 0.7078 - val_accuracy: 0.5872\n",
      "Epoch 102/1000\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.6106 - accuracy: 0.6600 - val_loss: 0.6978 - val_accuracy: 0.5688\n",
      "Epoch 103/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.6207 - accuracy: 0.6540 - val_loss: 0.6687 - val_accuracy: 0.6075\n",
      "Epoch 104/1000\n",
      "125/125 [==============================] - 16s 132ms/step - loss: 0.5943 - accuracy: 0.6740 - val_loss: 0.6885 - val_accuracy: 0.5952\n",
      "Epoch 105/1000\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.5996 - accuracy: 0.6719 - val_loss: 0.6847 - val_accuracy: 0.5622\n",
      "Epoch 106/1000\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.6363 - accuracy: 0.6227 - val_loss: 0.6927 - val_accuracy: 0.5482\n",
      "Epoch 107/1000\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.6159 - accuracy: 0.6472 - val_loss: 0.6925 - val_accuracy: 0.5896\n",
      "Epoch 108/1000\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.6006 - accuracy: 0.6662 - val_loss: 0.6846 - val_accuracy: 0.5983\n",
      "Epoch 109/1000\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.5860 - accuracy: 0.6849 - val_loss: 0.6888 - val_accuracy: 0.5904\n",
      "Epoch 110/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5828 - accuracy: 0.6866 - val_loss: 0.6976 - val_accuracy: 0.5956\n",
      "Epoch 111/1000\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.5902 - accuracy: 0.6802 - val_loss: 0.6920 - val_accuracy: 0.5861\n",
      "Epoch 112/1000\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.5827 - accuracy: 0.6869 - val_loss: 0.7039 - val_accuracy: 0.5804\n",
      "Epoch 113/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5854 - accuracy: 0.6849 - val_loss: 0.7079 - val_accuracy: 0.5699\n",
      "Epoch 114/1000\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.6281 - accuracy: 0.6435 - val_loss: 0.6870 - val_accuracy: 0.5554\n",
      "Epoch 115/1000\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.5912 - accuracy: 0.6795 - val_loss: 0.6927 - val_accuracy: 0.5772\n",
      "Epoch 116/1000\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.5802 - accuracy: 0.6909 - val_loss: 0.6862 - val_accuracy: 0.5947\n",
      "Epoch 117/1000\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5741 - accuracy: 0.6908 - val_loss: 0.6913 - val_accuracy: 0.5886\n",
      "Epoch 118/1000\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5791 - accuracy: 0.6921 - val_loss: 0.6997 - val_accuracy: 0.5872\n",
      "Epoch 119/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5758 - accuracy: 0.6914 - val_loss: 0.6970 - val_accuracy: 0.5807\n",
      "Epoch 120/1000\n",
      "125/125 [==============================] - 21s 138ms/step - loss: 0.5737 - accuracy: 0.6936 - val_loss: 0.6910 - val_accuracy: 0.5902\n",
      "Epoch 121/1000\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.5695 - accuracy: 0.6999 - val_loss: 0.7046 - val_accuracy: 0.5864\n",
      "Epoch 122/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5683 - accuracy: 0.6989 - val_loss: 0.7041 - val_accuracy: 0.5864\n",
      "Epoch 123/1000\n",
      "125/125 [==============================] - 16s 133ms/step - loss: 0.5721 - accuracy: 0.6934 - val_loss: 0.7022 - val_accuracy: 0.5836\n",
      "Epoch 124/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5757 - accuracy: 0.6964 - val_loss: 0.6872 - val_accuracy: 0.6057\n",
      "Epoch 125/1000\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.5713 - accuracy: 0.7011 - val_loss: 0.6911 - val_accuracy: 0.5912\n",
      "Epoch 126/1000\n",
      "125/125 [==============================] - 19s 148ms/step - loss: 0.5652 - accuracy: 0.7075 - val_loss: 0.6949 - val_accuracy: 0.5951\n",
      "Epoch 127/1000\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.5639 - accuracy: 0.7041 - val_loss: 0.7117 - val_accuracy: 0.5762\n",
      "Epoch 128/1000\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.5726 - accuracy: 0.6959 - val_loss: 0.7042 - val_accuracy: 0.5838\n",
      "Epoch 129/1000\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.5700 - accuracy: 0.7023 - val_loss: 0.7100 - val_accuracy: 0.5761\n",
      "Epoch 130/1000\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.5747 - accuracy: 0.6980 - val_loss: 0.7024 - val_accuracy: 0.5831\n",
      "Epoch 131/1000\n",
      "125/125 [==============================] - 18s 149ms/step - loss: 0.5646 - accuracy: 0.7055 - val_loss: 0.6926 - val_accuracy: 0.5916\n",
      "Epoch 132/1000\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.5629 - accuracy: 0.7075 - val_loss: 0.7042 - val_accuracy: 0.5706\n",
      "Epoch 133/1000\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.5813 - accuracy: 0.6944 - val_loss: 0.7043 - val_accuracy: 0.5874\n",
      "Epoch 134/1000\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.5622 - accuracy: 0.7053 - val_loss: 0.6982 - val_accuracy: 0.5847\n",
      "Epoch 135/1000\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.5661 - accuracy: 0.7010 - val_loss: 0.7067 - val_accuracy: 0.5883\n",
      "Epoch 136/1000\n",
      "125/125 [==============================] - 15s 124ms/step - loss: 0.5717 - accuracy: 0.7000 - val_loss: 0.7222 - val_accuracy: 0.5741\n",
      "Epoch 137/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.5659 - accuracy: 0.7067 - val_loss: 0.6957 - val_accuracy: 0.5789\n",
      "Epoch 138/1000\n",
      "125/125 [==============================] - 19s 144ms/step - loss: 0.5643 - accuracy: 0.7046 - val_loss: 0.7130 - val_accuracy: 0.5897\n",
      "Epoch 139/1000\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5623 - accuracy: 0.7039 - val_loss: 0.7179 - val_accuracy: 0.5791\n",
      "Epoch 140/1000\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5714 - accuracy: 0.7015 - val_loss: 0.7123 - val_accuracy: 0.5764\n",
      "Epoch 141/1000\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.5708 - accuracy: 0.7020 - val_loss: 0.7161 - val_accuracy: 0.5728\n",
      "Epoch 142/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5621 - accuracy: 0.7111 - val_loss: 0.7111 - val_accuracy: 0.5697\n",
      "Epoch 143/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5669 - accuracy: 0.7014 - val_loss: 0.7180 - val_accuracy: 0.5836\n",
      "Epoch 144/1000\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.5621 - accuracy: 0.7066 - val_loss: 0.6962 - val_accuracy: 0.5970\n",
      "Epoch 145/1000\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.5657 - accuracy: 0.7020 - val_loss: 0.6992 - val_accuracy: 0.5973\n",
      "Epoch 146/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5582 - accuracy: 0.7129 - val_loss: 0.7093 - val_accuracy: 0.5975\n",
      "Epoch 147/1000\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.5553 - accuracy: 0.7130 - val_loss: 0.7182 - val_accuracy: 0.5859\n",
      "Epoch 148/1000\n",
      "125/125 [==============================] - 15s 121ms/step - loss: 0.5615 - accuracy: 0.7080 - val_loss: 0.7231 - val_accuracy: 0.5875\n",
      "Epoch 149/1000\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.5528 - accuracy: 0.7155 - val_loss: 0.7218 - val_accuracy: 0.5889\n",
      "Epoch 150/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5538 - accuracy: 0.7143 - val_loss: 0.7213 - val_accuracy: 0.6006\n",
      "Epoch 151/1000\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.5573 - accuracy: 0.7103 - val_loss: 0.7164 - val_accuracy: 0.5888\n",
      "Epoch 152/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5532 - accuracy: 0.7141 - val_loss: 0.7247 - val_accuracy: 0.5870\n",
      "Epoch 153/1000\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.5562 - accuracy: 0.7101 - val_loss: 0.7194 - val_accuracy: 0.5945\n",
      "Epoch 154/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5607 - accuracy: 0.7080 - val_loss: 0.7064 - val_accuracy: 0.6025\n",
      "Epoch 155/1000\n",
      "125/125 [==============================] - 16s 133ms/step - loss: 0.5503 - accuracy: 0.7145 - val_loss: 0.7393 - val_accuracy: 0.5757\n",
      "Epoch 156/1000\n",
      "125/125 [==============================] - 18s 149ms/step - loss: 0.5588 - accuracy: 0.7103 - val_loss: 0.7199 - val_accuracy: 0.5958\n",
      "Epoch 157/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.5544 - accuracy: 0.7132 - val_loss: 0.7179 - val_accuracy: 0.5750\n",
      "Epoch 158/1000\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.5567 - accuracy: 0.7082 - val_loss: 0.7269 - val_accuracy: 0.5843\n",
      "Epoch 159/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5495 - accuracy: 0.7166 - val_loss: 0.7233 - val_accuracy: 0.5968\n",
      "Epoch 160/1000\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.5512 - accuracy: 0.7149 - val_loss: 0.7135 - val_accuracy: 0.5951\n",
      "Epoch 161/1000\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.5480 - accuracy: 0.7190 - val_loss: 0.7108 - val_accuracy: 0.5822\n",
      "Epoch 162/1000\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.5483 - accuracy: 0.7175 - val_loss: 0.7391 - val_accuracy: 0.5854\n",
      "Epoch 163/1000\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.5565 - accuracy: 0.7132 - val_loss: 0.7245 - val_accuracy: 0.5901\n",
      "Epoch 164/1000\n",
      "125/125 [==============================] - 16s 126ms/step - loss: 0.5609 - accuracy: 0.7082 - val_loss: 0.7227 - val_accuracy: 0.6056\n",
      "Epoch 165/1000\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5514 - accuracy: 0.7138 - val_loss: 0.7246 - val_accuracy: 0.6022\n",
      "Epoch 166/1000\n",
      "125/125 [==============================] - 17s 141ms/step - loss: 0.5451 - accuracy: 0.7176 - val_loss: 0.7052 - val_accuracy: 0.5970\n",
      "Epoch 167/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5398 - accuracy: 0.7250 - val_loss: 0.7277 - val_accuracy: 0.5841\n",
      "Epoch 168/1000\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.5426 - accuracy: 0.7188 - val_loss: 0.7341 - val_accuracy: 0.5918\n",
      "Epoch 169/1000\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.5400 - accuracy: 0.7235 - val_loss: 0.7381 - val_accuracy: 0.5891\n",
      "Epoch 170/1000\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.5403 - accuracy: 0.7235 - val_loss: 0.7335 - val_accuracy: 0.5959\n",
      "Epoch 171/1000\n",
      "125/125 [==============================] - 17s 134ms/step - loss: 0.5467 - accuracy: 0.7182 - val_loss: 0.7142 - val_accuracy: 0.6037\n",
      "Epoch 172/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5448 - accuracy: 0.7220 - val_loss: 0.7255 - val_accuracy: 0.5999\n",
      "Epoch 173/1000\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.5394 - accuracy: 0.7245 - val_loss: 0.7214 - val_accuracy: 0.5988\n",
      "Epoch 174/1000\n",
      "125/125 [==============================] - 20s 141ms/step - loss: 0.5422 - accuracy: 0.7222 - val_loss: 0.7386 - val_accuracy: 0.5889\n",
      "Epoch 175/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5402 - accuracy: 0.7231 - val_loss: 0.7365 - val_accuracy: 0.5940\n",
      "Epoch 176/1000\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.5695 - accuracy: 0.6961 - val_loss: 0.7107 - val_accuracy: 0.5696\n",
      "Epoch 177/1000\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.5802 - accuracy: 0.6873 - val_loss: 0.7705 - val_accuracy: 0.5999\n",
      "Epoch 178/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5520 - accuracy: 0.7157 - val_loss: 0.7187 - val_accuracy: 0.5875\n",
      "Epoch 179/1000\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.5374 - accuracy: 0.7280 - val_loss: 0.7164 - val_accuracy: 0.5971\n",
      "Epoch 180/1000\n",
      "125/125 [==============================] - 18s 145ms/step - loss: 0.5560 - accuracy: 0.7084 - val_loss: 0.7393 - val_accuracy: 0.6060\n",
      "Epoch 181/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5496 - accuracy: 0.7149 - val_loss: 0.7400 - val_accuracy: 0.5779\n",
      "Epoch 182/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.5426 - accuracy: 0.7193 - val_loss: 0.7645 - val_accuracy: 0.5640\n",
      "Epoch 183/1000\n",
      "125/125 [==============================] - 13s 108ms/step - loss: 0.5445 - accuracy: 0.7175 - val_loss: 0.7487 - val_accuracy: 0.5894\n",
      "Epoch 184/1000\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 0.5356 - accuracy: 0.7280 - val_loss: 0.7490 - val_accuracy: 0.5897\n",
      "Epoch 185/1000\n",
      "125/125 [==============================] - 12s 96ms/step - loss: 0.5394 - accuracy: 0.7286 - val_loss: 0.7394 - val_accuracy: 0.5891\n",
      "Epoch 186/1000\n",
      "125/125 [==============================] - 15s 118ms/step - loss: 0.5351 - accuracy: 0.7303 - val_loss: 0.7336 - val_accuracy: 0.5893\n",
      "Epoch 187/1000\n",
      "125/125 [==============================] - 14s 110ms/step - loss: 0.5294 - accuracy: 0.7330 - val_loss: 0.7618 - val_accuracy: 0.5819\n",
      "Epoch 188/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5356 - accuracy: 0.7271 - val_loss: 0.7477 - val_accuracy: 0.5959\n",
      "Epoch 189/1000\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.5365 - accuracy: 0.7293 - val_loss: 0.7609 - val_accuracy: 0.5926\n",
      "Epoch 190/1000\n",
      "125/125 [==============================] - 15s 121ms/step - loss: 0.5334 - accuracy: 0.7287 - val_loss: 0.7626 - val_accuracy: 0.5941\n",
      "Epoch 191/1000\n",
      "125/125 [==============================] - 16s 124ms/step - loss: 0.5286 - accuracy: 0.7350 - val_loss: 0.7668 - val_accuracy: 0.5830\n",
      "Epoch 192/1000\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 0.5382 - accuracy: 0.7234 - val_loss: 0.7582 - val_accuracy: 0.5806\n",
      "Epoch 193/1000\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.5311 - accuracy: 0.7311 - val_loss: 0.7684 - val_accuracy: 0.5858\n",
      "Epoch 194/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5334 - accuracy: 0.7319 - val_loss: 0.7649 - val_accuracy: 0.5863\n",
      "Epoch 195/1000\n",
      "125/125 [==============================] - 16s 129ms/step - loss: 0.5375 - accuracy: 0.7259 - val_loss: 0.7551 - val_accuracy: 0.5836\n",
      "Epoch 196/1000\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.5390 - accuracy: 0.7230 - val_loss: 0.7664 - val_accuracy: 0.5900\n",
      "Epoch 197/1000\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 0.5419 - accuracy: 0.7262 - val_loss: 0.7027 - val_accuracy: 0.5979\n",
      "Epoch 198/1000\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.5313 - accuracy: 0.7309 - val_loss: 0.7525 - val_accuracy: 0.5884\n",
      "Epoch 199/1000\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.5234 - accuracy: 0.7410 - val_loss: 0.7524 - val_accuracy: 0.5795\n",
      "Epoch 200/1000\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.5289 - accuracy: 0.7347 - val_loss: 0.7651 - val_accuracy: 0.5775\n",
      "Epoch 201/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.5305 - accuracy: 0.7351 - val_loss: 0.7712 - val_accuracy: 0.5972\n",
      "Epoch 202/1000\n",
      "125/125 [==============================] - 15s 116ms/step - loss: 0.5311 - accuracy: 0.7329 - val_loss: 0.7607 - val_accuracy: 0.5968\n",
      "Epoch 203/1000\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.5467 - accuracy: 0.7186 - val_loss: 0.7491 - val_accuracy: 0.5967\n",
      "Epoch 204/1000\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.5321 - accuracy: 0.7321 - val_loss: 0.7787 - val_accuracy: 0.5921\n",
      "Epoch 205/1000\n",
      "125/125 [==============================] - 14s 115ms/step - loss: 0.5285 - accuracy: 0.7355 - val_loss: 0.7691 - val_accuracy: 0.5801\n",
      "Epoch 206/1000\n",
      "125/125 [==============================] - 14s 112ms/step - loss: 0.5315 - accuracy: 0.7345 - val_loss: 0.7653 - val_accuracy: 0.6004\n",
      "Epoch 207/1000\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.5239 - accuracy: 0.7383 - val_loss: 0.7846 - val_accuracy: 0.5914\n",
      "Epoch 208/1000\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.5432 - accuracy: 0.7236 - val_loss: 0.7566 - val_accuracy: 0.5920\n",
      "Epoch 209/1000\n",
      "125/125 [==============================] - 15s 121ms/step - loss: 0.5295 - accuracy: 0.7327 - val_loss: 0.7741 - val_accuracy: 0.5909\n",
      "Epoch 210/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.5278 - accuracy: 0.7345 - val_loss: 0.7635 - val_accuracy: 0.6007\n",
      "Epoch 211/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.5239 - accuracy: 0.7372 - val_loss: 0.7774 - val_accuracy: 0.5880\n",
      "Epoch 212/1000\n",
      "125/125 [==============================] - 15s 122ms/step - loss: 0.5492 - accuracy: 0.7163 - val_loss: 0.7533 - val_accuracy: 0.5999\n",
      "Epoch 213/1000\n",
      "125/125 [==============================] - 12s 93ms/step - loss: 0.5395 - accuracy: 0.7237 - val_loss: 0.7582 - val_accuracy: 0.5894\n",
      "Epoch 214/1000\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.5314 - accuracy: 0.7320 - val_loss: 0.7829 - val_accuracy: 0.5960\n",
      "Epoch 215/1000\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 0.5470 - accuracy: 0.7171 - val_loss: 0.7361 - val_accuracy: 0.5900\n",
      "Epoch 216/1000\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.5358 - accuracy: 0.7274 - val_loss: 0.7505 - val_accuracy: 0.6029\n",
      "Epoch 217/1000\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.5325 - accuracy: 0.7301 - val_loss: 0.7773 - val_accuracy: 0.5996\n",
      "Epoch 218/1000\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.5273 - accuracy: 0.7356 - val_loss: 0.7775 - val_accuracy: 0.5942\n",
      "Epoch 219/1000\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.5328 - accuracy: 0.7311 - val_loss: 0.7823 - val_accuracy: 0.6040\n",
      "Epoch 220/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5280 - accuracy: 0.7321 - val_loss: 0.7696 - val_accuracy: 0.6038\n",
      "Epoch 221/1000\n",
      "125/125 [==============================] - 17s 141ms/step - loss: 0.5279 - accuracy: 0.7354 - val_loss: 0.7748 - val_accuracy: 0.6122\n",
      "Epoch 222/1000\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.5358 - accuracy: 0.7308 - val_loss: 0.7263 - val_accuracy: 0.5933\n",
      "Epoch 223/1000\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 0.5313 - accuracy: 0.7351 - val_loss: 0.7571 - val_accuracy: 0.5912\n",
      "Epoch 224/1000\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.5171 - accuracy: 0.7479 - val_loss: 0.7830 - val_accuracy: 0.5887\n",
      "Epoch 225/1000\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.5181 - accuracy: 0.7449 - val_loss: 0.7715 - val_accuracy: 0.5935\n",
      "Epoch 226/1000\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.5191 - accuracy: 0.7431 - val_loss: 0.7879 - val_accuracy: 0.5892\n",
      "Epoch 227/1000\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.5202 - accuracy: 0.7455 - val_loss: 0.8039 - val_accuracy: 0.5880\n",
      "Epoch 228/1000\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5149 - accuracy: 0.7500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mval_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1607\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1608\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1609\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1610\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1611\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1612\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1613\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1614\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1615\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1616\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1617\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1618\u001b[0m )\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   1948\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(50, 75), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(50, 75), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(1,), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: val_gen,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(50, 75), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(50, 75), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(1,), dtype=tf.float32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "# Create and compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=1000, validation_data=val_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
