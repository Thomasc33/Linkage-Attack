{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input format\n",
    "needs to be updated\n",
    "\n",
    "[J0 X, J0 Y, J0 Z, J1 X, J1 Y, J1 Z, ..., J25 Z]\n",
    "\n",
    "### Output format\n",
    "One Hot encoded action classification len(Y) = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    # Read the files\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Get stats for each file based on name\n",
    "    files_ = []\n",
    "    for file in files:\n",
    "        data = {'file': file,\n",
    "                's': file[0:4],\n",
    "                'c': file[4:8],\n",
    "                'p': file[8:12],\n",
    "                'r': file[12:16],\n",
    "                'a': file[16:20]\n",
    "                }\n",
    "        files_.append(data)\n",
    "\n",
    "    return files_\n",
    "files_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load X from pickle\n",
      "X loaded from pickle\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load X and Y from pickle before generating them\n",
    "X = {}\n",
    "try:\n",
    "    print('Attempting to load X from pickle')\n",
    "    with open('data/X_action.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print('X loaded from pickle')\n",
    "except:\n",
    "    print('Could not load X and Y, generating them now')\n",
    "\n",
    "    # Load the files\n",
    "    if files_ is None:\n",
    "        files_ = load_files()\n",
    "\n",
    "    # Generate X and Y\n",
    "    for file_ in tqdm(files_, desc='Files Parsed', position=0):\n",
    "        try:\n",
    "            file = join(path, file_['file'])\n",
    "            data = open(file, 'r')\n",
    "            lines = data.readlines()\n",
    "            frames_count = int(lines.pop(0).replace('\\n', ''))\n",
    "            file_['frames'] = frames_count\n",
    "        except UnicodeDecodeError:  # .DS_Store file\n",
    "            print('UnicodeDecodeError: ', file)\n",
    "            continue\n",
    "\n",
    "        # Get P and add to X if not already there\n",
    "        a = file_['a']\n",
    "        if a not in X:\n",
    "            X[a] = []\n",
    "\n",
    "        # Hold all frames for the file (action)\n",
    "        frames = []\n",
    "\n",
    "        # To validate the video is good\n",
    "        good = True\n",
    "\n",
    "        for f in tqdm(range(frames_count), desc='Frames Parsed', position=1, leave=False):\n",
    "            try:\n",
    "                # Get actor count\n",
    "                actors = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                # Hold frame info\n",
    "                frame = []\n",
    "\n",
    "                # Iterate through actors\n",
    "                for ac in range(actors):\n",
    "                    # Get actor info\n",
    "                    t = lines.pop(0)\n",
    "\n",
    "                    # Get joint count\n",
    "                    joint_count = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                    # Get joint info\n",
    "                    d = []\n",
    "                    for j in range(joint_count):\n",
    "                        joint = lines.pop(0).replace('\\n', '').split(' ')\n",
    "                        d.extend(joint[0:3])\n",
    "\n",
    "                    # Convert to float\n",
    "                    d = [float(i) for i in d]\n",
    "\n",
    "                    # Skip if not 25 joints\n",
    "                    if len(d) != 75:\n",
    "                        good = False\n",
    "                        break\n",
    "\n",
    "                    # Append to frame\n",
    "                    frame.extend(d)\n",
    "\n",
    "                # Convert to numpy array\n",
    "                frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "                # Pad frame to 150 (for 2 actors)\n",
    "                if len(frame) < 150:\n",
    "                    frame = np.pad(frame, (0, 150-len(frame)), 'constant')\n",
    "\n",
    "                # Append to X and Y\n",
    "                frames.append(frame)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        if not good:\n",
    "            continue\n",
    "\n",
    "        if type(frames) != list:\n",
    "            print('Not a list: ', type(frames), frames)\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy array\n",
    "        frames = np.array(frames, dtype=np.ndarray)\n",
    "\n",
    "        # Validate frames size\n",
    "        if len(frames.shape) != 2 or frames.shape[1] < 150:\n",
    "            continue\n",
    "\n",
    "        # Pad X size to 300 frames (300 is max frames in dataset)\n",
    "        # Each frame is 25 joints * 3 coordinates * 2 potential actors = 150\n",
    "        # For the real time attack model, we can make a new prediction every 300 frames (5 second @ 60fps) to align with this\n",
    "        if frames.shape[0] < 300:\n",
    "            frames = np.pad(\n",
    "                frames, ((0, 300-frames.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "        # Validate frames size\n",
    "        if frames.shape != (300, 150):\n",
    "            continue\n",
    "\n",
    "        # Ensure the frame isnt all zeros\n",
    "        if np.sum(frames) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add frames to X\n",
    "        X[a].append(frames)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for a in X:\n",
    "        X[a] = np.array(X[a]).astype(np.float32)\n",
    "\n",
    "    print('X Generated, saving to pickle...')\n",
    "\n",
    "    # Save the data\n",
    "    with open('data/X_action.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    print('X Saved to pickle')\n",
    "\n",
    "# Print Lengths\n",
    "# for p in X:\n",
    "#     print(p, len(X[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for action in X:\n",
    "        # Get the action index\n",
    "        action_index = int(action[1:4])-1\n",
    "        # Convert to onehot encoding\n",
    "        y_ = np.zeros(120)\n",
    "        y_[action_index] = 1\n",
    "        # Append to X and Y\n",
    "        y.extend([y_]*len(X[action]))\n",
    "        x.extend(X[action])\n",
    "    return x,y\n",
    "\n",
    "X_, Y_ = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_, Y_, test_size=0.3, random_state=42)\n",
    "# Split into validation and test\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGN\n",
    "\n",
    "All code in this section is adapted from Microsoft's SGN. [Github](https://github.com/microsoft/SGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from util import make_dir, get_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters/Tuning Parameters\n",
    "network='SGN'\n",
    "dataset='NTU'\n",
    "start_epoch=0\n",
    "case=1 # 0 = Gender, 1 = Action\n",
    "batch_size=64\n",
    "max_epochs=120\n",
    "monitor='val_acc'\n",
    "lr=0.001\n",
    "weight_decay=0.0001\n",
    "lr_factor=0.1\n",
    "workers=16\n",
    "print_freq = 20\n",
    "do_train=1\n",
    "seg=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "\n",
    "        output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # clear gradients out before each mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print('Epoch-{:<3d} {:3d} batches\\t'\n",
    "                  'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accu {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                      epoch + 1, i + 1, loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, checkpoint, lable_path, pred_path):\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    np.savetxt(lable_path, label_output, fmt='%d')\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "    np.savetxt(pred_path, pred_output, fmt='%f')\n",
    "\n",
    "    print('Test: accuracy {:.3f}, time: {:.2f}s'\n",
    "          .format(acces.avg, time.time() - t_start))\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters:  721828\n",
      "The modes is: SGN\n",
      "It is using GPU!\n",
      "Train on 79083 samples, validate on 16947 samples\n",
      "0 0.001\n",
      "Epoch-1    20 batches\tloss 4.9415 (5.0253)\taccu 0.000 (1.562)\n",
      "Epoch-1    40 batches\tloss 4.6429 (4.8914)\taccu 4.688 (1.953)\n",
      "Epoch-1    60 batches\tloss 4.5075 (4.8091)\taccu 0.000 (2.135)\n",
      "Epoch-1    80 batches\tloss 4.4517 (4.7302)\taccu 4.688 (2.695)\n",
      "Epoch-1   100 batches\tloss 4.2485 (4.6539)\taccu 3.125 (3.250)\n",
      "Epoch-1   120 batches\tloss 4.1258 (4.5792)\taccu 10.938 (3.685)\n",
      "Epoch-1   140 batches\tloss 3.9829 (4.5176)\taccu 10.938 (4.174)\n",
      "Epoch-1   160 batches\tloss 4.0502 (4.4583)\taccu 7.812 (4.766)\n",
      "Epoch-1   180 batches\tloss 3.8711 (4.3982)\taccu 15.625 (5.642)\n",
      "Epoch-1   200 batches\tloss 3.9390 (4.3408)\taccu 17.188 (6.484)\n",
      "Epoch-1   220 batches\tloss 3.6703 (4.2908)\taccu 20.312 (7.045)\n",
      "Epoch-1   240 batches\tloss 3.6133 (4.2399)\taccu 23.438 (7.708)\n",
      "Epoch-1   260 batches\tloss 3.6996 (4.1898)\taccu 17.188 (8.468)\n",
      "Epoch-1   280 batches\tloss 3.3493 (4.1450)\taccu 23.438 (9.152)\n",
      "Epoch-1   300 batches\tloss 3.5170 (4.1018)\taccu 20.312 (9.766)\n",
      "Epoch-1   320 batches\tloss 3.4305 (4.0640)\taccu 25.000 (10.405)\n",
      "Epoch-1   340 batches\tloss 3.2712 (4.0223)\taccu 21.875 (11.043)\n",
      "Epoch-1   360 batches\tloss 3.5028 (3.9876)\taccu 15.625 (11.571)\n",
      "Epoch-1   380 batches\tloss 3.1172 (3.9520)\taccu 26.562 (12.105)\n",
      "Epoch-1   400 batches\tloss 3.1371 (3.9178)\taccu 37.500 (12.770)\n",
      "Epoch-1   420 batches\tloss 3.2726 (3.8844)\taccu 31.250 (13.423)\n",
      "Epoch-1   440 batches\tloss 3.3593 (3.8548)\taccu 26.562 (14.052)\n",
      "Epoch-1   460 batches\tloss 3.2874 (3.8228)\taccu 28.125 (14.711)\n",
      "Epoch-1   480 batches\tloss 3.1387 (3.7943)\taccu 29.688 (15.293)\n",
      "Epoch-1   500 batches\tloss 2.8789 (3.7647)\taccu 42.188 (16.003)\n",
      "Epoch-1   520 batches\tloss 2.9617 (3.7367)\taccu 40.625 (16.650)\n",
      "Epoch-1   540 batches\tloss 3.0590 (3.7117)\taccu 28.125 (17.173)\n",
      "Epoch-1   560 batches\tloss 3.2907 (3.6872)\taccu 25.000 (17.681)\n",
      "Epoch-1   580 batches\tloss 3.0191 (3.6636)\taccu 37.500 (18.184)\n",
      "Epoch-1   600 batches\tloss 3.0388 (3.6418)\taccu 29.688 (18.724)\n",
      "Epoch-1   620 batches\tloss 2.9171 (3.6196)\taccu 35.938 (19.317)\n",
      "Epoch-1   640 batches\tloss 3.0823 (3.5989)\taccu 32.812 (19.783)\n",
      "Epoch-1   660 batches\tloss 2.8273 (3.5779)\taccu 28.125 (20.258)\n",
      "Epoch-1   680 batches\tloss 2.9787 (3.5566)\taccu 37.500 (20.722)\n",
      "Epoch-1   700 batches\tloss 2.9329 (3.5386)\taccu 37.500 (21.132)\n",
      "Epoch-1   720 batches\tloss 3.1292 (3.5209)\taccu 39.062 (21.528)\n",
      "Epoch-1   740 batches\tloss 3.0510 (3.5013)\taccu 34.375 (22.006)\n",
      "Epoch-1   760 batches\tloss 2.7185 (3.4826)\taccu 39.062 (22.455)\n",
      "Epoch-1   780 batches\tloss 2.7575 (3.4634)\taccu 39.062 (22.959)\n",
      "Epoch-1   800 batches\tloss 2.7966 (3.4471)\taccu 40.625 (23.328)\n",
      "Epoch-1   820 batches\tloss 2.6041 (3.4297)\taccu 40.625 (23.720)\n",
      "Epoch-1   840 batches\tloss 2.7824 (3.4123)\taccu 39.062 (24.159)\n",
      "Epoch-1   860 batches\tloss 2.9723 (3.3960)\taccu 37.500 (24.569)\n",
      "Epoch-1   880 batches\tloss 2.5251 (3.3804)\taccu 53.125 (24.952)\n",
      "Epoch-1   900 batches\tloss 2.7702 (3.3656)\taccu 37.500 (25.273)\n",
      "Epoch-1   920 batches\tloss 2.8000 (3.3514)\taccu 42.188 (25.639)\n",
      "Epoch-1   940 batches\tloss 2.4734 (3.3369)\taccu 43.750 (25.992)\n",
      "Epoch-1   960 batches\tloss 2.7123 (3.3222)\taccu 34.375 (26.348)\n",
      "Epoch-1   980 batches\tloss 2.5187 (3.3072)\taccu 50.000 (26.700)\n",
      "Epoch-1   1000 batches\tloss 2.6608 (3.2934)\taccu 40.625 (27.039)\n",
      "Epoch-1   1020 batches\tloss 2.8432 (3.2795)\taccu 35.938 (27.408)\n",
      "Epoch-1   1040 batches\tloss 2.9861 (3.2674)\taccu 35.938 (27.719)\n",
      "Epoch-1   1060 batches\tloss 2.3627 (3.2543)\taccu 51.562 (28.059)\n",
      "Epoch-1   1080 batches\tloss 2.2722 (3.2423)\taccu 54.688 (28.348)\n",
      "Epoch-1   1100 batches\tloss 2.4392 (3.2305)\taccu 53.125 (28.673)\n",
      "Epoch-1   1120 batches\tloss 2.6828 (3.2188)\taccu 46.875 (28.986)\n",
      "Epoch-1   1140 batches\tloss 2.5696 (3.2067)\taccu 46.875 (29.326)\n",
      "Epoch-1   1160 batches\tloss 2.4403 (3.1945)\taccu 54.688 (29.640)\n",
      "Epoch-1   1180 batches\tloss 2.8783 (3.1833)\taccu 34.375 (29.942)\n",
      "Epoch-1   1200 batches\tloss 2.4134 (3.1721)\taccu 56.250 (30.240)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    109\u001b[0m     test(test_loader, model, checkpoint, lable_path, pred_path)\n\u001b[1;32m--> 111\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(epoch, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m t_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 64\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train(\n\u001b[0;32m     65\u001b[0m     train_loader, model, criterion, optimizer, epoch)\n\u001b[0;32m     66\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m validate(val_loader, model, criterion)\n\u001b[0;32m     67\u001b[0m log_res \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [[train_loss, train_acc\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     68\u001b[0m              val_loss, val_acc\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()]]\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m acces \u001b[39m=\u001b[39m AverageMeter()\n\u001b[0;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      8\u001b[0m     output \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m      9\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\OneDrive\\Code\\Motion Privacy\\Attacking Models\\SGN Attack Model\\data.py:146\u001b[0m, in \u001b[0;36mNTUDataLoaders.collate_fn_fix_val\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m\"\"\"Puts each data field into a tensor with outer dimension batch size\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m--> 146\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTolist_fix(x, y, train\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    147\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x))\n\u001b[0;32m    148\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\OneDrive\\Code\\Motion Privacy\\Attacking Models\\SGN Attack Model\\data.py:176\u001b[0m, in \u001b[0;36mNTUDataLoaders.Tolist_fix\u001b[1;34m(self, joints, y, train)\u001b[0m\n\u001b[0;32m    174\u001b[0m zero_row \u001b[39m=\u001b[39m []\n\u001b[0;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(seq)):\n\u001b[1;32m--> 176\u001b[0m     \u001b[39mif\u001b[39;00m (seq[i, :] \u001b[39m==\u001b[39;49m np\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m, \u001b[39m150\u001b[39;49m)))\u001b[39m.\u001b[39mall():\n\u001b[0;32m    177\u001b[0m         zero_row\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m    179\u001b[0m seq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(seq, zero_row, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    num_classes = get_num_classes(dataset, 1)\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, do_train)\n",
    "\n",
    "    total = get_n_params(model)\n",
    "    # print(model)\n",
    "    print('The number of parameters: ', total)\n",
    "    print('The modes is:', network)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('It is using GPU!')\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = LabelSmoothingLoss(num_classes, smoothing=0.1).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if monitor == 'val_acc':\n",
    "        mode = 'max'\n",
    "        monitor_op = np.greater\n",
    "        best = -np.Inf\n",
    "        str_op = 'improve'\n",
    "    elif monitor == 'val_loss':\n",
    "        mode = 'min'\n",
    "        monitor_op = np.less\n",
    "        best = np.Inf\n",
    "        str_op = 'reduce'\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 90, 110], gamma=0.1)\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=seg, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    train_loader = ntu_loaders.get_train_loader(batch_size, workers)\n",
    "    val_loader = ntu_loaders.get_val_loader(batch_size, workers)\n",
    "    train_size = ntu_loaders.get_train_size()\n",
    "    val_size = ntu_loaders.get_val_size()\n",
    "\n",
    "    test_loader = ntu_loaders.get_test_loader(32, workers)\n",
    "\n",
    "    print('Train on %d samples, validate on %d samples' %\n",
    "          (train_size, val_size))\n",
    "\n",
    "    best_epoch = 0\n",
    "    output_dir = make_dir(dataset)\n",
    "\n",
    "    save_path = os.path.join(output_dir, network)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    checkpoint = osp.join(save_path, '%s_best.pth' % case)\n",
    "    earlystop_cnt = 0\n",
    "    csv_file = osp.join(save_path, '%s_log.csv' % case)\n",
    "    log_res = list()\n",
    "\n",
    "    lable_path = osp.join(save_path, '%s_lable.txt' % case)\n",
    "    pred_path = osp.join(save_path, '%s_pred.txt' % case)\n",
    "\n",
    "    # Training\n",
    "    if do_train == 1:\n",
    "        for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "            print(epoch, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            t_start = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                train_loader, model, criterion, optimizer, epoch)\n",
    "            val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "            log_res += [[train_loss, train_acc.cpu().numpy(),\n",
    "                         val_loss, val_acc.cpu().numpy()]]\n",
    "\n",
    "            print('Epoch-{:<3d} {:.1f}s\\t'\n",
    "                  'Train: loss {:.4f}\\taccu {:.4f}\\tValid: loss {:.4f}\\taccu {:.4f}'\n",
    "                  .format(epoch + 1, time.time() - t_start, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            current = val_loss if mode == 'min' else val_acc\n",
    "\n",
    "            # store tensor in cpu\n",
    "            current = current.cpu()\n",
    "\n",
    "            if monitor_op(current, best):\n",
    "                print('Epoch %d: %s %sd from %.4f to %.4f, '\n",
    "                      'saving model to %s'\n",
    "                      % (epoch + 1, monitor, str_op, best, current, checkpoint))\n",
    "                best = current\n",
    "                best_epoch = epoch + 1\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best': best,\n",
    "                    'monitor': monitor,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "                earlystop_cnt = 0\n",
    "            else:\n",
    "                print('Epoch %d: %s did not %s' % (epoch + 1, monitor, str_op))\n",
    "                earlystop_cnt += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Best %s: %.4f from epoch-%d' % (monitor, best, best_epoch))\n",
    "        with open(csv_file, 'w') as fw:\n",
    "            cw = csv.writer(fw)\n",
    "            cw.writerow(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "            cw.writerows(log_res)\n",
    "        print('Save train and validation log into into %s' % csv_file)\n",
    "\n",
    "    # Test\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, 0)\n",
    "    model = model.cuda()\n",
    "    test(test_loader, model, checkpoint, lable_path, pred_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063dd9079dbbbc7ce9a24508feb60cfa7f5aa9bc9e0c912b3996301118c4566f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
