{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input format\n",
    "needs to be updated\n",
    "\n",
    "[J0 X, J0 Y, J0 Z, J1 X, J1 Y, J1 Z, ..., J25 Z]\n",
    "\n",
    "### Output format\n",
    "One Hot encoded action classification len(Y) = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    # Read the files\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Get stats for each file based on name\n",
    "    files_ = []\n",
    "    for file in files:\n",
    "        data = {'file': file,\n",
    "                's': file[0:4],\n",
    "                'c': file[4:8],\n",
    "                'p': file[8:12],\n",
    "                'r': file[12:16],\n",
    "                'a': file[16:20]\n",
    "                }\n",
    "        files_.append(data)\n",
    "\n",
    "    return files_\n",
    "files_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load X from pickle\n",
      "Could not load X and Y, generating them now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Parsed: 100%|██████████| 114480/114480 [1:21:47<00:00, 23.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Generated, saving to pickle...\n",
      "X Saved to pickle\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load X and Y from pickle before generating them\n",
    "X = {}\n",
    "try:\n",
    "    print('Attempting to load X from pickle')\n",
    "    with open('data/X_action.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print('X loaded from pickle')\n",
    "except:\n",
    "    print('Could not load X and Y, generating them now')\n",
    "\n",
    "    # Load the files\n",
    "    if files_ is None:\n",
    "        files_ = load_files()\n",
    "\n",
    "    # Generate X and Y\n",
    "    for file_ in tqdm(files_, desc='Files Parsed', position=0):\n",
    "        try:\n",
    "            file = join(path, file_['file'])\n",
    "            data = open(file, 'r')\n",
    "            lines = data.readlines()\n",
    "            frames_count = int(lines.pop(0).replace('\\n', ''))\n",
    "            file_['frames'] = frames_count\n",
    "        except UnicodeDecodeError:  # .DS_Store file\n",
    "            print('UnicodeDecodeError: ', file)\n",
    "            continue\n",
    "\n",
    "        # Get P and add to X if not already there\n",
    "        a = file_['a']\n",
    "        if a not in X:\n",
    "            X[a] = []\n",
    "\n",
    "        # Hold all frames for the file (action)\n",
    "        frames = []\n",
    "\n",
    "        # To validate the video is good\n",
    "        good = True\n",
    "\n",
    "        for f in tqdm(range(frames_count), desc='Frames Parsed', position=1, leave=False):\n",
    "            try:\n",
    "                # Get actor count\n",
    "                actors = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                # Hold frame info\n",
    "                frame = []\n",
    "\n",
    "                # Iterate through actors\n",
    "                for ac in range(actors):\n",
    "                    # Get actor info\n",
    "                    t = lines.pop(0)\n",
    "\n",
    "                    # Get joint count\n",
    "                    joint_count = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                    # Get joint info\n",
    "                    d = []\n",
    "                    for j in range(joint_count):\n",
    "                        joint = lines.pop(0).replace('\\n', '').split(' ')\n",
    "                        d.extend(joint[0:3])\n",
    "\n",
    "                    # Convert to float\n",
    "                    d = [float(i) for i in d]\n",
    "\n",
    "                    # Skip if not 25 joints\n",
    "                    if len(d) != 75:\n",
    "                        good = False\n",
    "                        break\n",
    "\n",
    "                    # Append to frame\n",
    "                    frame.extend(d)\n",
    "\n",
    "                # Convert to numpy array\n",
    "                frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "                # Pad frame to 150 (for 2 actors)\n",
    "                if len(frame) < 150:\n",
    "                    frame = np.pad(frame, (0, 150-len(frame)), 'constant')\n",
    "\n",
    "                # Append to X and Y\n",
    "                frames.append(frame)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        if not good:\n",
    "            continue\n",
    "\n",
    "        if type(frames) != list:\n",
    "            print('Not a list: ', type(frames), frames)\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy array\n",
    "        frames = np.array(frames, dtype=np.ndarray)\n",
    "\n",
    "        # Validate frames size\n",
    "        if len(frames.shape) != 2 or frames.shape[1] < 150:\n",
    "            continue\n",
    "\n",
    "        # Pad X size to 300 frames (300 is max frames in dataset)\n",
    "        # Each frame is 25 joints * 3 coordinates * 2 potential actors = 150\n",
    "        # For the real time attack model, we can make a new prediction every 300 frames (5 second @ 60fps) to align with this\n",
    "        if frames.shape[0] < 300:\n",
    "            frames = np.pad(\n",
    "                frames, ((0, 300-frames.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "        # Validate frames size\n",
    "        if frames.shape != (300, 150):\n",
    "            continue\n",
    "\n",
    "        # Ensure the frame isnt all zeros\n",
    "        if np.sum(frames) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add frames to X\n",
    "        X[a].append(frames)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for a in X:\n",
    "        X[a] = np.array(X[a]).astype(np.float32)\n",
    "\n",
    "    print('X Generated, saving to pickle...')\n",
    "\n",
    "    # Save the data\n",
    "    with open('data/X_action.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    print('X Saved to pickle')\n",
    "\n",
    "# Print Lengths\n",
    "# for p in X:\n",
    "#     print(p, len(X[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for action in X:\n",
    "        # Get the action index\n",
    "        action_index = int(action[1:4])-1\n",
    "        # Convert to onehot encoding\n",
    "        y_ = np.zeros(120)\n",
    "        y_[action_index] = 1\n",
    "        # Append to X and Y\n",
    "        y.extend([y_]*len(X[action]))\n",
    "        x.extend(X[action])\n",
    "    return x,y\n",
    "\n",
    "X_, Y_ = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_, Y_, test_size=0.3, random_state=42)\n",
    "# Split into validation and test\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGN\n",
    "\n",
    "All code in this section is adapted from Microsoft's SGN. [Github](https://github.com/microsoft/SGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000014FB83FCEE0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py\", line 1291, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from util import make_dir, get_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters/Tuning Parameters\n",
    "network='SGN'\n",
    "dataset='NTU'\n",
    "start_epoch=0\n",
    "case=1 # 0 = Gender, 1 = Action\n",
    "batch_size=64\n",
    "max_epochs=120\n",
    "monitor='val_acc'\n",
    "lr=0.001\n",
    "weight_decay=0.0001\n",
    "lr_factor=0.1\n",
    "workers=16\n",
    "print_freq = 20\n",
    "do_train=1\n",
    "seg=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "\n",
    "        output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # clear gradients out before each mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print('Epoch-{:<3d} {:3d} batches\\t'\n",
    "                  'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accu {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                      epoch + 1, i + 1, loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, checkpoint, lable_path, pred_path):\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    np.savetxt(lable_path, label_output, fmt='%d')\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "    np.savetxt(pred_path, pred_output, fmt='%f')\n",
    "\n",
    "    print('Test: accuracy {:.3f}, time: {:.2f}s'\n",
    "          .format(acces.avg, time.time() - t_start))\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_classes = get_num_classes(dataset, 1)\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, do_train)\n",
    "\n",
    "    total = get_n_params(model)\n",
    "    # print(model)\n",
    "    print('The number of parameters: ', total)\n",
    "    print('The modes is:', network)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('It is using GPU!')\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = LabelSmoothingLoss(num_classes, smoothing=0.1).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if monitor == 'val_acc':\n",
    "        mode = 'max'\n",
    "        monitor_op = np.greater\n",
    "        best = -np.Inf\n",
    "        str_op = 'improve'\n",
    "    elif monitor == 'val_loss':\n",
    "        mode = 'min'\n",
    "        monitor_op = np.less\n",
    "        best = np.Inf\n",
    "        str_op = 'reduce'\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 90, 110], gamma=0.1)\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=seg, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    train_loader = ntu_loaders.get_train_loader(batch_size, workers)\n",
    "    val_loader = ntu_loaders.get_val_loader(batch_size, workers)\n",
    "    train_size = ntu_loaders.get_train_size()\n",
    "    val_size = ntu_loaders.get_val_size()\n",
    "\n",
    "    test_loader = ntu_loaders.get_test_loader(32, workers)\n",
    "\n",
    "    print('Train on %d samples, validate on %d samples' %\n",
    "          (train_size, val_size))\n",
    "\n",
    "    best_epoch = 0\n",
    "    output_dir = make_dir(dataset)\n",
    "\n",
    "    save_path = os.path.join(output_dir, network)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    checkpoint = osp.join(save_path, '%s_best.pth' % case)\n",
    "    earlystop_cnt = 0\n",
    "    csv_file = osp.join(save_path, '%s_log.csv' % case)\n",
    "    log_res = list()\n",
    "\n",
    "    lable_path = osp.join(save_path, '%s_lable.txt' % case)\n",
    "    pred_path = osp.join(save_path, '%s_pred.txt' % case)\n",
    "\n",
    "    # Training\n",
    "    if do_train == 1:\n",
    "        for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "            print(epoch, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            t_start = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                train_loader, model, criterion, optimizer, epoch)\n",
    "            val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "            log_res += [[train_loss, train_acc.cpu().numpy(),\n",
    "                         val_loss, val_acc.cpu().numpy()]]\n",
    "\n",
    "            print('Epoch-{:<3d} {:.1f}s\\t'\n",
    "                  'Train: loss {:.4f}\\taccu {:.4f}\\tValid: loss {:.4f}\\taccu {:.4f}'\n",
    "                  .format(epoch + 1, time.time() - t_start, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            current = val_loss if mode == 'min' else val_acc\n",
    "\n",
    "            # store tensor in cpu\n",
    "            current = current.cpu()\n",
    "\n",
    "            if monitor_op(current, best):\n",
    "                print('Epoch %d: %s %sd from %.4f to %.4f, '\n",
    "                      'saving model to %s'\n",
    "                      % (epoch + 1, monitor, str_op, best, current, checkpoint))\n",
    "                best = current\n",
    "                best_epoch = epoch + 1\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best': best,\n",
    "                    'monitor': monitor,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "                earlystop_cnt = 0\n",
    "            else:\n",
    "                print('Epoch %d: %s did not %s' % (epoch + 1, monitor, str_op))\n",
    "                earlystop_cnt += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Best %s: %.4f from epoch-%d' % (monitor, best, best_epoch))\n",
    "        with open(csv_file, 'w') as fw:\n",
    "            cw = csv.writer(fw)\n",
    "            cw.writerow(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "            cw.writerows(log_res)\n",
    "        print('Save train and validation log into into %s' % csv_file)\n",
    "\n",
    "    # Test\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, 0)\n",
    "    model = model.cuda()\n",
    "    test(test_loader, model, checkpoint, lable_path, pred_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063dd9079dbbbc7ce9a24508feb60cfa7f5aa9bc9e0c912b3996301118c4566f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
