{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/thomas/Downloads/nturgb+d_skeletons'\n",
    "path = 'D:\\\\Datasets\\\\Motion Privacy\\\\NTU RGB+D 120\\\\Skeleton Data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input format\n",
    "needs to be updated\n",
    "\n",
    "[J0 X, J0 Y, J0 Z, J1 X, J1 Y, J1 Z, ..., J25 Z]\n",
    "\n",
    "### Output format\n",
    "0 = Female, 1 = Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Genders\n",
    "Genders = pd.read_csv('data/Genders.csv')\n",
    "\n",
    "# Convert M to 1 and F to 0\n",
    "Genders = Genders.replace('M', 1).replace('F', 0)\n",
    "\n",
    "# Convert dataframe to oject where P is the key, and Gender is the value\n",
    "Genders = Genders.set_index('P').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    # Read the files\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Get stats for each file based on name\n",
    "    files_ = []\n",
    "    for file in files:\n",
    "        data = {'file': file,\n",
    "                's': file[0:4],\n",
    "                'c': file[4:8],\n",
    "                'p': file[8:12],\n",
    "                'r': file[12:16],\n",
    "                'a': file[16:20]\n",
    "                }\n",
    "        files_.append(data)\n",
    "\n",
    "    return files_\n",
    "files_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load X from pickle\n",
      "X loaded from pickle\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load X and Y from pickle before generating them\n",
    "X = {}\n",
    "try:\n",
    "    print('Attempting to load X from pickle')\n",
    "    with open('data/X.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    print('X loaded from pickle')\n",
    "except:\n",
    "    print('Could not load X and Y, generating them now')\n",
    "\n",
    "    # Load the files\n",
    "    if files_ is None:\n",
    "        files_ = load_files()\n",
    "\n",
    "    # Generate X and Y\n",
    "    for file_ in tqdm(files_, desc='Files Parsed', position=0):\n",
    "        try:\n",
    "            file = join(path, file_['file'])\n",
    "            data = open(file, 'r')\n",
    "            lines = data.readlines()\n",
    "            frames_count = int(lines.pop(0).replace('\\n', ''))\n",
    "            file_['frames'] = frames_count\n",
    "        except UnicodeDecodeError:  # .DS_Store file\n",
    "            print('UnicodeDecodeError: ', file)\n",
    "            continue\n",
    "\n",
    "        # Get P and add to X if not already there\n",
    "        p = file_['p']\n",
    "        if p not in X:\n",
    "            X[p] = []\n",
    "\n",
    "        # Hold all frames for the file (action)\n",
    "        frames = []\n",
    "\n",
    "        # To validate the video is good\n",
    "        good = True\n",
    "\n",
    "        for f in tqdm(range(frames_count), desc='Frames Parsed', position=1, leave=False):\n",
    "            try:\n",
    "                # Get actor count\n",
    "                actors = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                # Hold frame info\n",
    "                frame = []\n",
    "\n",
    "                # Iterate through actors\n",
    "                for a in range(actors):\n",
    "                    # Get actor info\n",
    "                    t = lines.pop(0)\n",
    "\n",
    "                    # Get joint count\n",
    "                    joint_count = int(lines.pop(0).replace('\\n', ''))\n",
    "\n",
    "                    # Get joint info\n",
    "                    d = []\n",
    "                    for j in range(joint_count):\n",
    "                        joint = lines.pop(0).replace('\\n', '').split(' ')\n",
    "                        d.extend(joint[0:3])\n",
    "\n",
    "                    # Convert to float\n",
    "                    d = [float(i) for i in d]\n",
    "\n",
    "                    # Skip if not 25 joints\n",
    "                    if len(d) != 75:\n",
    "                        good = False\n",
    "                        break\n",
    "\n",
    "                    # Append to frame\n",
    "                    frame.extend(d)\n",
    "\n",
    "                # Convert to numpy array\n",
    "                frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "                # Pad frame to 150 (for 2 actors)\n",
    "                if len(frame) < 150:\n",
    "                    frame = np.pad(frame, (0, 150-len(frame)), 'constant')\n",
    "\n",
    "                # Append to X and Y\n",
    "                frames.append(frame)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        if not good:\n",
    "            continue\n",
    "\n",
    "        if type(frames) != list:\n",
    "            print('Not a list: ', type(frames), frames)\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy array\n",
    "        frames = np.array(frames, dtype=np.ndarray)\n",
    "\n",
    "        # Validate frames size\n",
    "        if len(frames.shape) != 2 or frames.shape[1] < 150:\n",
    "            continue\n",
    "\n",
    "        # Pad X size to 300 frames (300 is max frames in dataset)\n",
    "        # Each frame is 25 joints * 3 coordinates * 2 potential actors = 150\n",
    "        # For the real time attack model, we can make a new prediction every 300 frames (5 second @ 60fps) to align with this\n",
    "        if frames.shape[0] < 300:\n",
    "            frames = np.pad(\n",
    "                frames, ((0, 300-frames.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "        # Validate frames size\n",
    "        if frames.shape != (300, 150):\n",
    "            continue\n",
    "\n",
    "        # Ensure the frame isnt all zeros\n",
    "        if np.sum(frames) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add frames to X\n",
    "        X[p].append(frames)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for p in X:\n",
    "        X[p] = np.array(X[p]).astype(np.float32)\n",
    "\n",
    "    print('X Generated, saving to pickle...')\n",
    "\n",
    "    # Save the data\n",
    "    with open('data/X.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    print('X Saved to pickle')\n",
    "\n",
    "# Print Lengths\n",
    "# for p in X:\n",
    "#     print(p, len(X[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X Shape:  (55641, 300, 150)\n",
      "Train Y Shape:  (55641,)\n",
      "Test X Shape:  (57336, 300, 150)\n",
      "Test Y Shape:  (57336,)\n"
     ]
    }
   ],
   "source": [
    "# Default Male/Female split\n",
    "MALES = 30\n",
    "FEMALES = 15\n",
    "\n",
    "# To export test actors\n",
    "test_actors = []\n",
    "\n",
    "# Split into train and test\n",
    "def split_data(males, females):\n",
    "    global test_actors\n",
    "    # Choose males count random males\n",
    "    m = []\n",
    "    f = []\n",
    "    for p in Genders:\n",
    "        if Genders[p] == [1]:\n",
    "            m.append(p)\n",
    "        else:\n",
    "            f.append(p)\n",
    "    train_males = np.random.choice(list(m), males, replace=False)\n",
    "    train_females = np.random.choice(list(f), females, replace=False)\n",
    "\n",
    "    # Combine to get male/female split\n",
    "    X_male = np.concatenate([X[x] for x in train_males])\n",
    "    X_female = np.concatenate([X[x] for x in train_females])\n",
    "    Y_male = np.ones(len(X_male))\n",
    "    Y_female = np.zeros(len(X_female))\n",
    "\n",
    "    # Combine to get train data\n",
    "    train_x = np.concatenate([X_male, X_female])\n",
    "    train_y = np.concatenate([Y_male, Y_female])\n",
    "\n",
    "    # Get the actors not in the train set\n",
    "    test_males = [x for x in m if x not in train_males]\n",
    "    test_females = [x for x in f if x not in train_females]\n",
    "    test_actors = test_males+test_females\n",
    "\n",
    "    # Combine to get male/female split\n",
    "    X_male = np.concatenate([X[x] for x in test_males])\n",
    "    X_female = np.concatenate([X[x] for x in test_females])\n",
    "    Y_male = np.ones(len(X_male), dtype=np.int8)\n",
    "    Y_female = np.zeros(len(X_female), dtype=np.int8)\n",
    "\n",
    "    # Combine to get test data\n",
    "    test_x = np.concatenate([X_male, X_female])\n",
    "    test_y = np.concatenate([Y_male, Y_female])\n",
    "\n",
    "    # Print shapes\n",
    "    print('Train X Shape: ', train_x.shape)\n",
    "    print('Train Y Shape: ', train_y.shape)\n",
    "    print('Test X Shape: ', test_x.shape)\n",
    "    print('Test Y Shape: ', test_y.shape)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_data(MALES, FEMALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test into validation and test\n",
    "test_x, val_x, test_y, val_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all Y's to be onehot\n",
    "def to_categorical(y):\n",
    "    return np.array([np.array([1, 0]) if i == 0 else np.array([0, 1]) for i in y])\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "val_y = to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test Actors\n",
    "test_X = {}\n",
    "for p in test_actors:\n",
    "    test_X[p] = X[p]\n",
    "\n",
    "with open('data/test_X.pkl', 'wb') as f:\n",
    "    pickle.dump(test_X, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGN\n",
    "\n",
    "All code in this section is adapted from Microsoft's SGN. [Github](https://github.com/microsoft/SGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from util import make_dir, get_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters/Tuning Parameters\n",
    "network='SGN'\n",
    "dataset='NTU'\n",
    "start_epoch=0\n",
    "case=0\n",
    "batch_size=64\n",
    "max_epochs=120\n",
    "monitor='val_acc'\n",
    "lr=0.001\n",
    "weight_decay=0.0001\n",
    "lr_factor=0.1\n",
    "workers=16\n",
    "print_freq = 20\n",
    "do_train=1\n",
    "seg=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "\n",
    "        output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # clear gradients out before each mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print('Epoch-{:<3d} {:3d} batches\\t'\n",
    "                  'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accu {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                      epoch + 1, i + 1, loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(output.data, target)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, checkpoint, lable_path, pred_path):\n",
    "    acces = AverageMeter()\n",
    "    # load learnt model that obtained best performance on validation set\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    label_output = list()\n",
    "    pred_output = list()\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, t in enumerate(test_loader):\n",
    "        inputs = t[0]\n",
    "        target = t[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.cuda())\n",
    "            output = output.view(\n",
    "                (-1, inputs.size(0)//target.size(0), output.size(1)))\n",
    "            output = output.mean(1)\n",
    "\n",
    "        label_output.append(target.cpu().numpy())\n",
    "        pred_output.append(output.cpu().numpy())\n",
    "\n",
    "        acc = accuracy(output.data, target.cuda())\n",
    "        acces.update(acc[0], inputs.size(0))\n",
    "\n",
    "    label_output = np.concatenate(label_output, axis=0)\n",
    "    np.savetxt(lable_path, label_output, fmt='%d')\n",
    "    pred_output = np.concatenate(pred_output, axis=0)\n",
    "    np.savetxt(pred_path, pred_output, fmt='%f')\n",
    "\n",
    "    print('Test: accuracy {:.3f}, time: {:.2f}s'\n",
    "          .format(acces.avg, time.time() - t_start))\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            target = torch.argmax(target, dim=1)  # Add this line to convert one-hot targets to class indices\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters:  661294\n",
      "The modes is: SGN\n",
      "It is using GPU!\n",
      "Train on 55641 samples, validate on 28668 samples\n",
      "0 0.001\n",
      "Epoch-1    20 batches\tloss 0.6455 (0.7665)\taccu 73.438 (64.531)\n",
      "Epoch-1    40 batches\tloss 0.7897 (0.7195)\taccu 59.375 (65.508)\n",
      "Epoch-1    60 batches\tloss 0.6511 (0.6917)\taccu 70.312 (67.266)\n",
      "Epoch-1    80 batches\tloss 0.6671 (0.6738)\taccu 71.875 (68.633)\n",
      "Epoch-1   100 batches\tloss 0.5672 (0.6635)\taccu 76.562 (69.156)\n",
      "Epoch-1   120 batches\tloss 0.5462 (0.6501)\taccu 82.812 (70.312)\n",
      "Epoch-1   140 batches\tloss 0.5938 (0.6421)\taccu 79.688 (70.960)\n",
      "Epoch-1   160 batches\tloss 0.5389 (0.6340)\taccu 82.812 (71.592)\n",
      "Epoch-1   180 batches\tloss 0.5840 (0.6258)\taccu 75.000 (72.405)\n",
      "Epoch-1   200 batches\tloss 0.5645 (0.6164)\taccu 78.125 (73.312)\n",
      "Epoch-1   220 batches\tloss 0.5732 (0.6106)\taccu 82.812 (73.828)\n",
      "Epoch-1   240 batches\tloss 0.5919 (0.6088)\taccu 78.125 (73.809)\n",
      "Epoch-1   260 batches\tloss 0.4942 (0.6037)\taccu 84.375 (74.417)\n",
      "Epoch-1   280 batches\tloss 0.5890 (0.6011)\taccu 70.312 (74.632)\n",
      "Epoch-1   300 batches\tloss 0.5339 (0.5966)\taccu 78.125 (75.068)\n",
      "Epoch-1   320 batches\tloss 0.5563 (0.5917)\taccu 79.688 (75.576)\n",
      "Epoch-1   340 batches\tloss 0.5463 (0.5872)\taccu 79.688 (76.034)\n",
      "Epoch-1   360 batches\tloss 0.5612 (0.5849)\taccu 79.688 (76.202)\n",
      "Epoch-1   380 batches\tloss 0.4915 (0.5817)\taccu 84.375 (76.567)\n",
      "Epoch-1   400 batches\tloss 0.4900 (0.5785)\taccu 87.500 (76.898)\n",
      "Epoch-1   420 batches\tloss 0.4929 (0.5758)\taccu 87.500 (77.176)\n",
      "Epoch-1   440 batches\tloss 0.5586 (0.5728)\taccu 79.688 (77.489)\n",
      "Epoch-1   460 batches\tloss 0.4833 (0.5695)\taccu 85.938 (77.836)\n",
      "Epoch-1   480 batches\tloss 0.5001 (0.5668)\taccu 84.375 (78.128)\n",
      "Epoch-1   500 batches\tloss 0.4882 (0.5644)\taccu 85.938 (78.369)\n",
      "Epoch-1   520 batches\tloss 0.4316 (0.5625)\taccu 92.188 (78.546)\n",
      "Epoch-1   540 batches\tloss 0.4959 (0.5603)\taccu 82.812 (78.770)\n",
      "Epoch-1   560 batches\tloss 0.4756 (0.5586)\taccu 84.375 (78.954)\n",
      "Epoch-1   580 batches\tloss 0.4854 (0.5564)\taccu 87.500 (79.186)\n",
      "Epoch-1   600 batches\tloss 0.4706 (0.5542)\taccu 89.062 (79.404)\n",
      "Epoch-1   620 batches\tloss 0.4718 (0.5525)\taccu 87.500 (79.541)\n",
      "Epoch-1   640 batches\tloss 0.5388 (0.5500)\taccu 84.375 (79.788)\n",
      "Epoch-1   660 batches\tloss 0.4633 (0.5486)\taccu 87.500 (79.948)\n",
      "Epoch-1   680 batches\tloss 0.4815 (0.5474)\taccu 87.500 (80.085)\n",
      "Epoch-1   700 batches\tloss 0.4371 (0.5462)\taccu 89.062 (80.203)\n",
      "Epoch-1   720 batches\tloss 0.4687 (0.5455)\taccu 87.500 (80.282)\n",
      "Epoch-1   740 batches\tloss 0.5032 (0.5437)\taccu 81.250 (80.454)\n",
      "Epoch-1   760 batches\tloss 0.5484 (0.5421)\taccu 79.688 (80.602)\n",
      "Epoch-1   780 batches\tloss 0.5064 (0.5409)\taccu 84.375 (80.727)\n",
      "Epoch-1   800 batches\tloss 0.4414 (0.5396)\taccu 92.188 (80.865)\n",
      "Epoch-1   820 batches\tloss 0.4828 (0.5386)\taccu 85.938 (80.970)\n",
      "Epoch-1   840 batches\tloss 0.4601 (0.5374)\taccu 90.625 (81.084)\n",
      "Epoch-1   860 batches\tloss 0.4239 (0.5359)\taccu 92.188 (81.221)\n",
      "Epoch-1   224.2s\tTrain: loss 0.5355\taccu 81.2734\tValid: loss 0.4737\taccu 88.1082\n",
      "Epoch 1: val_acc improved from -inf to 88.1082, saving model to ./results/NTU/SGN\\0_best.pth\n",
      "1 0.001\n",
      "Epoch-2    20 batches\tloss 0.4571 (0.4784)\taccu 90.625 (87.734)\n",
      "Epoch-2    40 batches\tloss 0.4632 (0.4811)\taccu 87.500 (87.695)\n",
      "Epoch-2    60 batches\tloss 0.4742 (0.4868)\taccu 85.938 (86.615)\n",
      "Epoch-2    80 batches\tloss 0.4277 (0.4844)\taccu 92.188 (86.855)\n",
      "Epoch-2   100 batches\tloss 0.5411 (0.4827)\taccu 79.688 (86.938)\n",
      "Epoch-2   120 batches\tloss 0.4583 (0.4841)\taccu 87.500 (86.615)\n",
      "Epoch-2   140 batches\tloss 0.5468 (0.4857)\taccu 85.938 (86.607)\n",
      "Epoch-2   160 batches\tloss 0.4891 (0.4850)\taccu 85.938 (86.758)\n",
      "Epoch-2   180 batches\tloss 0.4903 (0.4854)\taccu 84.375 (86.623)\n",
      "Epoch-2   200 batches\tloss 0.4663 (0.4860)\taccu 90.625 (86.500)\n",
      "Epoch-2   220 batches\tloss 0.4848 (0.4870)\taccu 82.812 (86.342)\n",
      "Epoch-2   240 batches\tloss 0.4660 (0.4864)\taccu 87.500 (86.367)\n",
      "Epoch-2   260 batches\tloss 0.4798 (0.4865)\taccu 90.625 (86.382)\n",
      "Epoch-2   280 batches\tloss 0.4412 (0.4858)\taccu 95.312 (86.456)\n",
      "Epoch-2   300 batches\tloss 0.4793 (0.4848)\taccu 89.062 (86.557)\n",
      "Epoch-2   320 batches\tloss 0.4623 (0.4840)\taccu 90.625 (86.641)\n",
      "Epoch-2   340 batches\tloss 0.4449 (0.4835)\taccu 95.312 (86.700)\n",
      "Epoch-2   360 batches\tloss 0.4754 (0.4831)\taccu 85.938 (86.680)\n",
      "Epoch-2   380 batches\tloss 0.4907 (0.4833)\taccu 85.938 (86.641)\n",
      "Epoch-2   400 batches\tloss 0.5025 (0.4839)\taccu 84.375 (86.547)\n",
      "Epoch-2   420 batches\tloss 0.4213 (0.4836)\taccu 90.625 (86.555)\n",
      "Epoch-2   440 batches\tloss 0.4981 (0.4833)\taccu 82.812 (86.580)\n",
      "Epoch-2   460 batches\tloss 0.4229 (0.4828)\taccu 93.750 (86.620)\n",
      "Epoch-2   480 batches\tloss 0.5255 (0.4830)\taccu 78.125 (86.585)\n",
      "Epoch-2   500 batches\tloss 0.4854 (0.4835)\taccu 87.500 (86.506)\n",
      "Epoch-2   520 batches\tloss 0.5023 (0.4836)\taccu 82.812 (86.484)\n",
      "Epoch-2   540 batches\tloss 0.4410 (0.4825)\taccu 89.062 (86.600)\n",
      "Epoch-2   560 batches\tloss 0.5048 (0.4821)\taccu 84.375 (86.655)\n",
      "Epoch-2   580 batches\tloss 0.4275 (0.4812)\taccu 92.188 (86.721)\n",
      "Epoch-2   600 batches\tloss 0.4540 (0.4808)\taccu 90.625 (86.763)\n",
      "Epoch-2   620 batches\tloss 0.4510 (0.4810)\taccu 87.500 (86.709)\n",
      "Epoch-2   640 batches\tloss 0.4914 (0.4808)\taccu 87.500 (86.704)\n",
      "Epoch-2   660 batches\tloss 0.4771 (0.4811)\taccu 85.938 (86.660)\n",
      "Epoch-2   680 batches\tloss 0.4464 (0.4811)\taccu 89.062 (86.652)\n",
      "Epoch-2   700 batches\tloss 0.5528 (0.4809)\taccu 79.688 (86.672)\n",
      "Epoch-2   720 batches\tloss 0.4194 (0.4802)\taccu 90.625 (86.721)\n",
      "Epoch-2   740 batches\tloss 0.4689 (0.4800)\taccu 89.062 (86.767)\n",
      "Epoch-2   760 batches\tloss 0.5164 (0.4798)\taccu 78.125 (86.766)\n",
      "Epoch-2   780 batches\tloss 0.4334 (0.4800)\taccu 93.750 (86.765)\n",
      "Epoch-2   800 batches\tloss 0.4410 (0.4796)\taccu 89.062 (86.783)\n",
      "Epoch-2   820 batches\tloss 0.4786 (0.4791)\taccu 87.500 (86.829)\n",
      "Epoch-2   840 batches\tloss 0.4857 (0.4788)\taccu 82.812 (86.845)\n",
      "Epoch-2   860 batches\tloss 0.4721 (0.4785)\taccu 90.625 (86.888)\n",
      "Epoch-2   218.8s\tTrain: loss 0.4782\taccu 86.9246\tValid: loss 0.4633\taccu 88.8702\n",
      "Epoch 2: val_acc improved from 88.1082 to 88.8702, saving model to ./results/NTU/SGN\\0_best.pth\n",
      "2 0.001\n",
      "Epoch-3    20 batches\tloss 0.4578 (0.4633)\taccu 89.062 (88.906)\n",
      "Epoch-3    40 batches\tloss 0.4334 (0.4650)\taccu 89.062 (88.438)\n",
      "Epoch-3    60 batches\tloss 0.5606 (0.4678)\taccu 78.125 (87.995)\n",
      "Epoch-3    80 batches\tloss 0.4582 (0.4662)\taccu 90.625 (87.969)\n",
      "Epoch-3   100 batches\tloss 0.4441 (0.4626)\taccu 90.625 (88.344)\n",
      "Epoch-3   120 batches\tloss 0.4188 (0.4612)\taccu 89.062 (88.516)\n",
      "Epoch-3   140 batches\tloss 0.4560 (0.4603)\taccu 89.062 (88.527)\n",
      "Epoch-3   160 batches\tloss 0.4308 (0.4635)\taccu 90.625 (88.184)\n",
      "Epoch-3   180 batches\tloss 0.4213 (0.4612)\taccu 92.188 (88.420)\n",
      "Epoch-3   200 batches\tloss 0.4422 (0.4620)\taccu 90.625 (88.352)\n",
      "Epoch-3   220 batches\tloss 0.4626 (0.4628)\taccu 87.500 (88.239)\n",
      "Epoch-3   240 batches\tloss 0.4426 (0.4626)\taccu 92.188 (88.281)\n",
      "Epoch-3   260 batches\tloss 0.4252 (0.4622)\taccu 90.625 (88.305)\n",
      "Epoch-3   280 batches\tloss 0.4451 (0.4616)\taccu 87.500 (88.354)\n",
      "Epoch-3   300 batches\tloss 0.4896 (0.4622)\taccu 85.938 (88.245)\n",
      "Epoch-3   320 batches\tloss 0.4315 (0.4615)\taccu 92.188 (88.262)\n",
      "Epoch-3   340 batches\tloss 0.4164 (0.4620)\taccu 93.750 (88.226)\n",
      "Epoch-3   360 batches\tloss 0.4669 (0.4619)\taccu 87.500 (88.229)\n",
      "Epoch-3   380 batches\tloss 0.4569 (0.4624)\taccu 84.375 (88.170)\n",
      "Epoch-3   400 batches\tloss 0.4319 (0.4619)\taccu 89.062 (88.254)\n",
      "Epoch-3   420 batches\tloss 0.4491 (0.4615)\taccu 92.188 (88.300)\n",
      "Epoch-3   440 batches\tloss 0.5305 (0.4612)\taccu 82.812 (88.320)\n",
      "Epoch-3   460 batches\tloss 0.4446 (0.4612)\taccu 89.062 (88.325)\n",
      "Epoch-3   480 batches\tloss 0.4992 (0.4615)\taccu 84.375 (88.271)\n",
      "Epoch-3   500 batches\tloss 0.4589 (0.4616)\taccu 90.625 (88.266)\n",
      "Epoch-3   520 batches\tloss 0.4615 (0.4617)\taccu 89.062 (88.287)\n",
      "Epoch-3   540 batches\tloss 0.4768 (0.4621)\taccu 85.938 (88.206)\n",
      "Epoch-3   560 batches\tloss 0.4416 (0.4617)\taccu 90.625 (88.220)\n",
      "Epoch-3   580 batches\tloss 0.4438 (0.4610)\taccu 84.375 (88.268)\n",
      "Epoch-3   600 batches\tloss 0.4311 (0.4612)\taccu 92.188 (88.255)\n",
      "Epoch-3   620 batches\tloss 0.4416 (0.4607)\taccu 93.750 (88.306)\n",
      "Epoch-3   640 batches\tloss 0.4744 (0.4607)\taccu 84.375 (88.320)\n",
      "Epoch-3   660 batches\tloss 0.5129 (0.4605)\taccu 84.375 (88.350)\n",
      "Epoch-3   680 batches\tloss 0.4203 (0.4606)\taccu 92.188 (88.339)\n",
      "Epoch-3   700 batches\tloss 0.4177 (0.4602)\taccu 93.750 (88.371)\n",
      "Epoch-3   720 batches\tloss 0.4183 (0.4596)\taccu 93.750 (88.409)\n",
      "Epoch-3   740 batches\tloss 0.4210 (0.4593)\taccu 89.062 (88.421)\n",
      "Epoch-3   760 batches\tloss 0.4270 (0.4592)\taccu 92.188 (88.440)\n",
      "Epoch-3   780 batches\tloss 0.4941 (0.4591)\taccu 82.812 (88.460)\n",
      "Epoch-3   800 batches\tloss 0.4630 (0.4591)\taccu 89.062 (88.471)\n",
      "Epoch-3   820 batches\tloss 0.4162 (0.4588)\taccu 92.188 (88.512)\n",
      "Epoch-3   840 batches\tloss 0.4595 (0.4588)\taccu 85.938 (88.497)\n",
      "Epoch-3   860 batches\tloss 0.5149 (0.4587)\taccu 84.375 (88.514)\n",
      "Epoch-3   227.8s\tTrain: loss 0.4589\taccu 88.4997\tValid: loss 0.4436\taccu 90.4747\n",
      "Epoch 3: val_acc improved from 88.8702 to 90.4747, saving model to ./results/NTU/SGN\\0_best.pth\n",
      "3 0.001\n",
      "Epoch-4    20 batches\tloss 0.4957 (0.4541)\taccu 85.938 (89.062)\n",
      "Epoch-4    40 batches\tloss 0.3991 (0.4547)\taccu 95.312 (88.633)\n",
      "Epoch-4    60 batches\tloss 0.4747 (0.4503)\taccu 82.812 (89.063)\n",
      "Epoch-4    80 batches\tloss 0.4304 (0.4467)\taccu 92.188 (89.336)\n",
      "Epoch-4   100 batches\tloss 0.4612 (0.4484)\taccu 90.625 (89.016)\n",
      "Epoch-4   120 batches\tloss 0.4907 (0.4472)\taccu 84.375 (89.193)\n",
      "Epoch-4   140 batches\tloss 0.4815 (0.4469)\taccu 87.500 (89.241)\n",
      "Epoch-4   160 batches\tloss 0.4212 (0.4463)\taccu 92.188 (89.375)\n",
      "Epoch-4   180 batches\tloss 0.5252 (0.4465)\taccu 84.375 (89.453)\n",
      "Epoch-4   200 batches\tloss 0.4253 (0.4467)\taccu 89.062 (89.414)\n",
      "Epoch-4   220 batches\tloss 0.4507 (0.4463)\taccu 89.062 (89.460)\n",
      "Epoch-4   240 batches\tloss 0.5442 (0.4464)\taccu 81.250 (89.421)\n",
      "Epoch-4   260 batches\tloss 0.4910 (0.4463)\taccu 85.938 (89.471)\n",
      "Epoch-4   280 batches\tloss 0.4129 (0.4461)\taccu 95.312 (89.498)\n",
      "Epoch-4   300 batches\tloss 0.4140 (0.4461)\taccu 93.750 (89.536)\n",
      "Epoch-4   320 batches\tloss 0.4519 (0.4462)\taccu 90.625 (89.458)\n",
      "Epoch-4   340 batches\tloss 0.4762 (0.4463)\taccu 87.500 (89.453)\n",
      "Epoch-4   360 batches\tloss 0.4395 (0.4467)\taccu 89.062 (89.388)\n",
      "Epoch-4   380 batches\tloss 0.5171 (0.4465)\taccu 78.125 (89.437)\n",
      "Epoch-4   400 batches\tloss 0.3802 (0.4457)\taccu 95.312 (89.523)\n",
      "Epoch-4   420 batches\tloss 0.4286 (0.4455)\taccu 93.750 (89.539)\n",
      "Epoch-4   440 batches\tloss 0.4711 (0.4450)\taccu 87.500 (89.606)\n",
      "Epoch-4   460 batches\tloss 0.4403 (0.4449)\taccu 90.625 (89.626)\n",
      "Epoch-4   480 batches\tloss 0.4067 (0.4450)\taccu 93.750 (89.645)\n",
      "Epoch-4   500 batches\tloss 0.4043 (0.4451)\taccu 95.312 (89.634)\n",
      "Epoch-4   520 batches\tloss 0.4823 (0.4452)\taccu 82.812 (89.594)\n",
      "Epoch-4   540 batches\tloss 0.4451 (0.4453)\taccu 90.625 (89.575)\n",
      "Epoch-4   560 batches\tloss 0.4382 (0.4456)\taccu 92.188 (89.554)\n",
      "Epoch-4   580 batches\tloss 0.4362 (0.4452)\taccu 89.062 (89.591)\n",
      "Epoch-4   600 batches\tloss 0.4333 (0.4453)\taccu 89.062 (89.586)\n",
      "Epoch-4   620 batches\tloss 0.3825 (0.4457)\taccu 92.188 (89.572)\n",
      "Epoch-4   640 batches\tloss 0.4233 (0.4453)\taccu 90.625 (89.609)\n",
      "Epoch-4   660 batches\tloss 0.3996 (0.4453)\taccu 95.312 (89.631)\n",
      "Epoch-4   680 batches\tloss 0.4567 (0.4451)\taccu 85.938 (89.642)\n",
      "Epoch-4   700 batches\tloss 0.4111 (0.4450)\taccu 90.625 (89.634)\n",
      "Epoch-4   720 batches\tloss 0.4289 (0.4447)\taccu 90.625 (89.661)\n",
      "Epoch-4   740 batches\tloss 0.4144 (0.4448)\taccu 93.750 (89.690)\n",
      "Epoch-4   760 batches\tloss 0.3916 (0.4448)\taccu 96.875 (89.679)\n",
      "Epoch-4   780 batches\tloss 0.4408 (0.4447)\taccu 87.500 (89.706)\n",
      "Epoch-4   800 batches\tloss 0.4451 (0.4448)\taccu 85.938 (89.680)\n",
      "Epoch-4   820 batches\tloss 0.4269 (0.4446)\taccu 90.625 (89.699)\n",
      "Epoch-4   840 batches\tloss 0.5077 (0.4444)\taccu 85.938 (89.704)\n",
      "Epoch-4   860 batches\tloss 0.4962 (0.4442)\taccu 84.375 (89.735)\n",
      "Epoch-4   233.7s\tTrain: loss 0.4442\taccu 89.7368\tValid: loss 0.4473\taccu 89.7162\n",
      "Epoch 4: val_acc did not improve\n",
      "4 0.001\n",
      "Epoch-5    20 batches\tloss 0.4592 (0.4399)\taccu 85.938 (90.000)\n",
      "Epoch-5    40 batches\tloss 0.4444 (0.4363)\taccu 87.500 (90.039)\n",
      "Epoch-5    60 batches\tloss 0.4479 (0.4405)\taccu 87.500 (89.896)\n",
      "Epoch-5    80 batches\tloss 0.4312 (0.4397)\taccu 87.500 (89.883)\n",
      "Epoch-5   100 batches\tloss 0.4893 (0.4400)\taccu 85.938 (89.828)\n",
      "Epoch-5   120 batches\tloss 0.4173 (0.4376)\taccu 93.750 (90.065)\n",
      "Epoch-5   140 batches\tloss 0.3938 (0.4366)\taccu 93.750 (90.312)\n",
      "Epoch-5   160 batches\tloss 0.4703 (0.4375)\taccu 87.500 (90.234)\n",
      "Epoch-5   180 batches\tloss 0.4655 (0.4386)\taccu 87.500 (90.095)\n",
      "Epoch-5   200 batches\tloss 0.4302 (0.4377)\taccu 92.188 (90.242)\n",
      "Epoch-5   220 batches\tloss 0.4060 (0.4363)\taccu 92.188 (90.341)\n",
      "Epoch-5   240 batches\tloss 0.3668 (0.4355)\taccu 100.000 (90.521)\n",
      "Epoch-5   260 batches\tloss 0.4149 (0.4351)\taccu 90.625 (90.565)\n",
      "Epoch-5   280 batches\tloss 0.5200 (0.4355)\taccu 84.375 (90.508)\n",
      "Epoch-5   300 batches\tloss 0.4200 (0.4362)\taccu 92.188 (90.495)\n",
      "Epoch-5   320 batches\tloss 0.4397 (0.4363)\taccu 87.500 (90.503)\n",
      "Epoch-5   340 batches\tloss 0.4413 (0.4370)\taccu 87.500 (90.455)\n",
      "Epoch-5   360 batches\tloss 0.4430 (0.4370)\taccu 90.625 (90.443)\n",
      "Epoch-5   380 batches\tloss 0.4229 (0.4372)\taccu 92.188 (90.436)\n",
      "Epoch-5   400 batches\tloss 0.3937 (0.4366)\taccu 93.750 (90.480)\n",
      "Epoch-5   420 batches\tloss 0.4534 (0.4365)\taccu 89.062 (90.476)\n",
      "Epoch-5   440 batches\tloss 0.4370 (0.4369)\taccu 90.625 (90.433)\n",
      "Epoch-5   460 batches\tloss 0.4366 (0.4365)\taccu 90.625 (90.499)\n",
      "Epoch-5   480 batches\tloss 0.4465 (0.4365)\taccu 92.188 (90.479)\n",
      "Epoch-5   500 batches\tloss 0.4084 (0.4361)\taccu 92.188 (90.547)\n",
      "Epoch-5   520 batches\tloss 0.4341 (0.4362)\taccu 92.188 (90.541)\n",
      "Epoch-5   540 batches\tloss 0.4497 (0.4356)\taccu 89.062 (90.611)\n",
      "Epoch-5   560 batches\tloss 0.4571 (0.4353)\taccu 87.500 (90.619)\n",
      "Epoch-5   580 batches\tloss 0.3781 (0.4359)\taccu 96.875 (90.563)\n",
      "Epoch-5   600 batches\tloss 0.4492 (0.4361)\taccu 90.625 (90.557)\n",
      "Epoch-5   620 batches\tloss 0.4249 (0.4359)\taccu 92.188 (90.575)\n",
      "Epoch-5   640 batches\tloss 0.4137 (0.4361)\taccu 92.188 (90.566)\n",
      "Epoch-5   660 batches\tloss 0.3919 (0.4355)\taccu 96.875 (90.644)\n",
      "Epoch-5   680 batches\tloss 0.3962 (0.4350)\taccu 95.312 (90.678)\n",
      "Epoch-5   700 batches\tloss 0.4632 (0.4350)\taccu 87.500 (90.656)\n",
      "Epoch-5   720 batches\tloss 0.3999 (0.4350)\taccu 95.312 (90.658)\n",
      "Epoch-5   740 batches\tloss 0.3745 (0.4348)\taccu 96.875 (90.690)\n",
      "Epoch-5   760 batches\tloss 0.4244 (0.4351)\taccu 92.188 (90.658)\n",
      "Epoch-5   780 batches\tloss 0.4433 (0.4351)\taccu 92.188 (90.643)\n",
      "Epoch-5   800 batches\tloss 0.5792 (0.4349)\taccu 79.688 (90.672)\n",
      "Epoch-5   820 batches\tloss 0.4282 (0.4349)\taccu 90.625 (90.680)\n",
      "Epoch-5   840 batches\tloss 0.4189 (0.4350)\taccu 93.750 (90.677)\n",
      "Epoch-5   860 batches\tloss 0.4383 (0.4349)\taccu 85.938 (90.667)\n",
      "Epoch-5   225.6s\tTrain: loss 0.4348\taccu 90.6753\tValid: loss 0.4613\taccu 88.4718\n",
      "Epoch 5: val_acc did not improve\n",
      "5 0.001\n",
      "Epoch-6    20 batches\tloss 0.3980 (0.4354)\taccu 95.312 (90.469)\n",
      "Epoch-6    40 batches\tloss 0.4549 (0.4354)\taccu 87.500 (90.820)\n",
      "Epoch-6    60 batches\tloss 0.3684 (0.4314)\taccu 96.875 (91.172)\n",
      "Epoch-6    80 batches\tloss 0.4065 (0.4328)\taccu 93.750 (90.820)\n",
      "Epoch-6   100 batches\tloss 0.4429 (0.4345)\taccu 89.062 (90.516)\n",
      "Epoch-6   120 batches\tloss 0.4804 (0.4345)\taccu 85.938 (90.560)\n",
      "Epoch-6   140 batches\tloss 0.3749 (0.4332)\taccu 95.312 (90.670)\n",
      "Epoch-6   160 batches\tloss 0.3904 (0.4340)\taccu 95.312 (90.576)\n",
      "Epoch-6   180 batches\tloss 0.5021 (0.4342)\taccu 81.250 (90.625)\n",
      "Epoch-6   200 batches\tloss 0.4077 (0.4337)\taccu 92.188 (90.625)\n",
      "Epoch-6   220 batches\tloss 0.4362 (0.4346)\taccu 90.625 (90.639)\n",
      "Epoch-6   240 batches\tloss 0.3951 (0.4331)\taccu 93.750 (90.781)\n",
      "Epoch-6   260 batches\tloss 0.4737 (0.4329)\taccu 89.062 (90.781)\n",
      "Epoch-6   280 batches\tloss 0.4149 (0.4338)\taccu 93.750 (90.720)\n",
      "Epoch-6   300 batches\tloss 0.4172 (0.4328)\taccu 93.750 (90.870)\n",
      "Epoch-6   320 batches\tloss 0.3978 (0.4318)\taccu 93.750 (90.942)\n",
      "Epoch-6   340 batches\tloss 0.4386 (0.4322)\taccu 92.188 (90.910)\n",
      "Epoch-6   360 batches\tloss 0.4208 (0.4321)\taccu 92.188 (90.868)\n",
      "Epoch-6   380 batches\tloss 0.4203 (0.4309)\taccu 93.750 (90.962)\n",
      "Epoch-6   400 batches\tloss 0.4282 (0.4312)\taccu 92.188 (90.973)\n",
      "Epoch-6   420 batches\tloss 0.3855 (0.4314)\taccu 96.875 (90.964)\n",
      "Epoch-6   440 batches\tloss 0.4196 (0.4312)\taccu 90.625 (90.952)\n",
      "Epoch-6   460 batches\tloss 0.4624 (0.4312)\taccu 89.062 (90.971)\n",
      "Epoch-6   480 batches\tloss 0.3888 (0.4309)\taccu 95.312 (91.022)\n",
      "Epoch-6   500 batches\tloss 0.4063 (0.4317)\taccu 90.625 (90.944)\n",
      "Epoch-6   520 batches\tloss 0.4360 (0.4326)\taccu 89.062 (90.823)\n",
      "Epoch-6   540 batches\tloss 0.4235 (0.4325)\taccu 90.625 (90.856)\n",
      "Epoch-6   560 batches\tloss 0.3879 (0.4322)\taccu 95.312 (90.871)\n",
      "Epoch-6   580 batches\tloss 0.4053 (0.4321)\taccu 90.625 (90.894)\n",
      "Epoch-6   600 batches\tloss 0.4486 (0.4319)\taccu 90.625 (90.940)\n",
      "Epoch-6   620 batches\tloss 0.4345 (0.4319)\taccu 90.625 (90.930)\n",
      "Epoch-6   640 batches\tloss 0.4574 (0.4316)\taccu 92.188 (90.950)\n",
      "Epoch-6   660 batches\tloss 0.3932 (0.4315)\taccu 96.875 (90.973)\n",
      "Epoch-6   680 batches\tloss 0.4155 (0.4308)\taccu 92.188 (91.032)\n",
      "Epoch-6   700 batches\tloss 0.4477 (0.4305)\taccu 89.062 (91.071)\n",
      "Epoch-6   720 batches\tloss 0.3698 (0.4304)\taccu 96.875 (91.085)\n",
      "Epoch-6   740 batches\tloss 0.3934 (0.4301)\taccu 96.875 (91.106)\n",
      "Epoch-6   760 batches\tloss 0.4350 (0.4302)\taccu 89.062 (91.098)\n",
      "Epoch-6   780 batches\tloss 0.4204 (0.4301)\taccu 90.625 (91.094)\n",
      "Epoch-6   800 batches\tloss 0.4686 (0.4302)\taccu 89.062 (91.094)\n",
      "Epoch-6   820 batches\tloss 0.4004 (0.4300)\taccu 95.312 (91.107)\n",
      "Epoch-6   840 batches\tloss 0.4275 (0.4298)\taccu 92.188 (91.129)\n",
      "Epoch-6   860 batches\tloss 0.3951 (0.4297)\taccu 92.188 (91.143)\n",
      "Epoch-6   206.8s\tTrain: loss 0.4296\taccu 91.1518\tValid: loss 0.4476\taccu 89.5344\n",
      "Epoch 6: val_acc did not improve\n",
      "6 0.001\n",
      "Epoch-7    20 batches\tloss 0.4212 (0.4120)\taccu 92.188 (93.125)\n",
      "Epoch-7    40 batches\tloss 0.3998 (0.4133)\taccu 93.750 (92.852)\n",
      "Epoch-7    60 batches\tloss 0.4590 (0.4175)\taccu 92.188 (92.370)\n",
      "Epoch-7    80 batches\tloss 0.4041 (0.4199)\taccu 90.625 (92.168)\n",
      "Epoch-7   100 batches\tloss 0.4584 (0.4216)\taccu 87.500 (91.891)\n",
      "Epoch-7   120 batches\tloss 0.4287 (0.4201)\taccu 93.750 (92.057)\n",
      "Epoch-7   140 batches\tloss 0.4541 (0.4210)\taccu 93.750 (92.031)\n",
      "Epoch-7   160 batches\tloss 0.4125 (0.4213)\taccu 93.750 (92.021)\n",
      "Epoch-7   180 batches\tloss 0.4102 (0.4229)\taccu 92.188 (91.892)\n",
      "Epoch-7   200 batches\tloss 0.3982 (0.4238)\taccu 96.875 (91.805)\n",
      "Epoch-7   220 batches\tloss 0.4521 (0.4236)\taccu 92.188 (91.804)\n",
      "Epoch-7   240 batches\tloss 0.3903 (0.4228)\taccu 95.312 (91.829)\n",
      "Epoch-7   260 batches\tloss 0.4075 (0.4213)\taccu 95.312 (91.971)\n",
      "Epoch-7   280 batches\tloss 0.4889 (0.4223)\taccu 85.938 (91.881)\n",
      "Epoch-7   300 batches\tloss 0.4182 (0.4228)\taccu 90.625 (91.792)\n",
      "Epoch-7   320 batches\tloss 0.4462 (0.4223)\taccu 87.500 (91.812)\n",
      "Epoch-7   340 batches\tloss 0.4286 (0.4223)\taccu 87.500 (91.811)\n",
      "Epoch-7   360 batches\tloss 0.3995 (0.4220)\taccu 95.312 (91.840)\n",
      "Epoch-7   380 batches\tloss 0.3702 (0.4217)\taccu 96.875 (91.891)\n",
      "Epoch-7   400 batches\tloss 0.4277 (0.4217)\taccu 90.625 (91.945)\n",
      "Epoch-7   420 batches\tloss 0.4502 (0.4216)\taccu 90.625 (91.968)\n",
      "Epoch-7   440 batches\tloss 0.4243 (0.4215)\taccu 85.938 (91.957)\n",
      "Epoch-7   460 batches\tloss 0.3884 (0.4211)\taccu 95.312 (92.001)\n",
      "Epoch-7   480 batches\tloss 0.4180 (0.4211)\taccu 93.750 (92.005)\n",
      "Epoch-7   500 batches\tloss 0.4528 (0.4210)\taccu 90.625 (92.003)\n",
      "Epoch-7   520 batches\tloss 0.4167 (0.4210)\taccu 93.750 (92.013)\n",
      "Epoch-7   540 batches\tloss 0.4443 (0.4215)\taccu 92.188 (91.973)\n",
      "Epoch-7   560 batches\tloss 0.5292 (0.4213)\taccu 81.250 (91.987)\n",
      "Epoch-7   580 batches\tloss 0.4660 (0.4210)\taccu 85.938 (91.999)\n",
      "Epoch-7   600 batches\tloss 0.3966 (0.4210)\taccu 90.625 (91.979)\n",
      "Epoch-7   620 batches\tloss 0.3849 (0.4209)\taccu 93.750 (91.963)\n",
      "Epoch-7   640 batches\tloss 0.4202 (0.4211)\taccu 95.312 (91.973)\n",
      "Epoch-7   660 batches\tloss 0.4786 (0.4214)\taccu 87.500 (91.944)\n",
      "Epoch-7   680 batches\tloss 0.4177 (0.4215)\taccu 90.625 (91.939)\n",
      "Epoch-7   700 batches\tloss 0.4344 (0.4216)\taccu 89.062 (91.920)\n",
      "Epoch-7   720 batches\tloss 0.4105 (0.4216)\taccu 92.188 (91.927)\n",
      "Epoch-7   740 batches\tloss 0.4800 (0.4213)\taccu 89.062 (91.957)\n",
      "Epoch-7   760 batches\tloss 0.4372 (0.4215)\taccu 89.062 (91.935)\n",
      "Epoch-7   780 batches\tloss 0.4120 (0.4214)\taccu 93.750 (91.937)\n",
      "Epoch-7   800 batches\tloss 0.4258 (0.4216)\taccu 87.500 (91.918)\n",
      "Epoch-7   820 batches\tloss 0.4571 (0.4216)\taccu 87.500 (91.917)\n",
      "Epoch-7   840 batches\tloss 0.3720 (0.4216)\taccu 98.438 (91.910)\n",
      "Epoch-7   860 batches\tloss 0.4162 (0.4215)\taccu 95.312 (91.944)\n",
      "Epoch-7   228.9s\tTrain: loss 0.4216\taccu 91.9394\tValid: loss 0.4535\taccu 89.1569\n",
      "Epoch 7: val_acc did not improve\n",
      "7 0.001\n",
      "Epoch-8    20 batches\tloss 0.4244 (0.4070)\taccu 89.062 (93.672)\n",
      "Epoch-8    40 batches\tloss 0.4279 (0.4132)\taccu 92.188 (92.852)\n",
      "Epoch-8    60 batches\tloss 0.3783 (0.4105)\taccu 95.312 (92.969)\n",
      "Epoch-8    80 batches\tloss 0.4078 (0.4097)\taccu 93.750 (93.145)\n",
      "Epoch-8   100 batches\tloss 0.4480 (0.4114)\taccu 90.625 (92.938)\n",
      "Epoch-8   120 batches\tloss 0.5089 (0.4139)\taccu 85.938 (92.617)\n",
      "Epoch-8   140 batches\tloss 0.3932 (0.4147)\taccu 90.625 (92.545)\n",
      "Epoch-8   160 batches\tloss 0.3986 (0.4159)\taccu 95.312 (92.422)\n",
      "Epoch-8   180 batches\tloss 0.4924 (0.4178)\taccu 84.375 (92.170)\n",
      "Epoch-8   200 batches\tloss 0.4580 (0.4186)\taccu 87.500 (92.070)\n",
      "Epoch-8   220 batches\tloss 0.4426 (0.4184)\taccu 87.500 (92.102)\n",
      "Epoch-8   240 batches\tloss 0.3692 (0.4179)\taccu 96.875 (92.148)\n",
      "Epoch-8   260 batches\tloss 0.3785 (0.4166)\taccu 96.875 (92.296)\n",
      "Epoch-8   280 batches\tloss 0.4234 (0.4159)\taccu 90.625 (92.383)\n",
      "Epoch-8   300 batches\tloss 0.3792 (0.4156)\taccu 96.875 (92.380)\n",
      "Epoch-8   320 batches\tloss 0.4194 (0.4160)\taccu 93.750 (92.373)\n",
      "Epoch-8   340 batches\tloss 0.3885 (0.4166)\taccu 93.750 (92.312)\n",
      "Epoch-8   360 batches\tloss 0.3909 (0.4172)\taccu 95.312 (92.261)\n",
      "Epoch-8   380 batches\tloss 0.4171 (0.4169)\taccu 92.188 (92.278)\n",
      "Epoch-8   400 batches\tloss 0.4774 (0.4170)\taccu 89.062 (92.277)\n",
      "Epoch-8   420 batches\tloss 0.3829 (0.4171)\taccu 98.438 (92.277)\n",
      "Epoch-8   440 batches\tloss 0.3955 (0.4173)\taccu 90.625 (92.273)\n",
      "Epoch-8   460 batches\tloss 0.4630 (0.4171)\taccu 87.500 (92.289)\n",
      "Epoch-8   480 batches\tloss 0.4421 (0.4175)\taccu 90.625 (92.230)\n",
      "Epoch-8   500 batches\tloss 0.4447 (0.4177)\taccu 93.750 (92.231)\n",
      "Epoch-8   520 batches\tloss 0.4059 (0.4178)\taccu 95.312 (92.245)\n",
      "Epoch-8   540 batches\tloss 0.3670 (0.4175)\taccu 96.875 (92.271)\n",
      "Epoch-8   560 batches\tloss 0.4039 (0.4173)\taccu 92.188 (92.316)\n",
      "Epoch-8   580 batches\tloss 0.3884 (0.4172)\taccu 95.312 (92.333)\n",
      "Epoch-8   600 batches\tloss 0.3860 (0.4174)\taccu 93.750 (92.292)\n",
      "Epoch-8   620 batches\tloss 0.4847 (0.4173)\taccu 85.938 (92.306)\n",
      "Epoch-8   640 batches\tloss 0.3928 (0.4174)\taccu 92.188 (92.312)\n",
      "Epoch-8   660 batches\tloss 0.4141 (0.4176)\taccu 93.750 (92.282)\n",
      "Epoch-8   680 batches\tloss 0.3923 (0.4175)\taccu 93.750 (92.293)\n",
      "Epoch-8   700 batches\tloss 0.3730 (0.4176)\taccu 96.875 (92.275)\n",
      "Epoch-8   720 batches\tloss 0.4710 (0.4173)\taccu 89.062 (92.300)\n",
      "Epoch-8   740 batches\tloss 0.3907 (0.4175)\taccu 96.875 (92.287)\n",
      "Epoch-8   760 batches\tloss 0.4091 (0.4175)\taccu 92.188 (92.296)\n",
      "Epoch-8   780 batches\tloss 0.3654 (0.4173)\taccu 98.438 (92.322)\n",
      "Epoch-8   800 batches\tloss 0.4268 (0.4172)\taccu 89.062 (92.336)\n",
      "Epoch-8   820 batches\tloss 0.3966 (0.4171)\taccu 92.188 (92.332)\n",
      "Epoch-8   840 batches\tloss 0.4153 (0.4173)\taccu 92.188 (92.321)\n",
      "Epoch-8   860 batches\tloss 0.3857 (0.4172)\taccu 93.750 (92.329)\n",
      "Epoch-8   231.6s\tTrain: loss 0.4174\taccu 92.3170\tValid: loss 0.4545\taccu 88.9821\n",
      "Epoch 8: val_acc did not improve\n",
      "8 0.001\n",
      "Epoch-9    20 batches\tloss 0.4074 (0.4067)\taccu 93.750 (93.672)\n",
      "Epoch-9    40 batches\tloss 0.4016 (0.4103)\taccu 92.188 (92.852)\n",
      "Epoch-9    60 batches\tloss 0.4184 (0.4100)\taccu 89.062 (92.734)\n",
      "Epoch-9    80 batches\tloss 0.4090 (0.4133)\taccu 93.750 (92.539)\n",
      "Epoch-9   100 batches\tloss 0.4630 (0.4132)\taccu 85.938 (92.703)\n",
      "Epoch-9   120 batches\tloss 0.3863 (0.4115)\taccu 95.312 (92.904)\n",
      "Epoch-9   140 batches\tloss 0.3997 (0.4100)\taccu 90.625 (92.924)\n",
      "Epoch-9   160 batches\tloss 0.3621 (0.4088)\taccu 96.875 (93.027)\n",
      "Epoch-9   180 batches\tloss 0.4098 (0.4093)\taccu 93.750 (93.021)\n",
      "Epoch-9   200 batches\tloss 0.4414 (0.4112)\taccu 87.500 (92.766)\n",
      "Epoch-9   220 batches\tloss 0.3817 (0.4114)\taccu 96.875 (92.734)\n",
      "Epoch-9   240 batches\tloss 0.3732 (0.4108)\taccu 95.312 (92.832)\n",
      "Epoch-9   260 batches\tloss 0.4181 (0.4116)\taccu 90.625 (92.752)\n",
      "Epoch-9   280 batches\tloss 0.4230 (0.4120)\taccu 92.188 (92.768)\n",
      "Epoch-9   300 batches\tloss 0.4053 (0.4129)\taccu 95.312 (92.667)\n",
      "Epoch-9   320 batches\tloss 0.4127 (0.4126)\taccu 90.625 (92.715)\n",
      "Epoch-9   340 batches\tloss 0.3952 (0.4122)\taccu 95.312 (92.753)\n",
      "Epoch-9   360 batches\tloss 0.3768 (0.4125)\taccu 95.312 (92.752)\n",
      "Epoch-9   380 batches\tloss 0.4559 (0.4127)\taccu 85.938 (92.706)\n",
      "Epoch-9   400 batches\tloss 0.4065 (0.4135)\taccu 92.188 (92.617)\n",
      "Epoch-9   420 batches\tloss 0.3620 (0.4135)\taccu 98.438 (92.604)\n",
      "Epoch-9   440 batches\tloss 0.4439 (0.4134)\taccu 89.062 (92.596)\n",
      "Epoch-9   460 batches\tloss 0.3922 (0.4132)\taccu 95.312 (92.612)\n",
      "Epoch-9   480 batches\tloss 0.4160 (0.4134)\taccu 93.750 (92.588)\n",
      "Epoch-9   500 batches\tloss 0.3818 (0.4129)\taccu 95.312 (92.619)\n",
      "Epoch-9   520 batches\tloss 0.4336 (0.4140)\taccu 90.625 (92.509)\n",
      "Epoch-9   540 batches\tloss 0.4562 (0.4146)\taccu 89.062 (92.465)\n",
      "Epoch-9   560 batches\tloss 0.4242 (0.4144)\taccu 90.625 (92.492)\n",
      "Epoch-9   580 batches\tloss 0.4301 (0.4145)\taccu 90.625 (92.487)\n",
      "Epoch-9   600 batches\tloss 0.4280 (0.4144)\taccu 87.500 (92.482)\n",
      "Epoch-9   620 batches\tloss 0.3522 (0.4142)\taccu 98.438 (92.490)\n",
      "Epoch-9   640 batches\tloss 0.4886 (0.4146)\taccu 89.062 (92.456)\n",
      "Epoch-9   660 batches\tloss 0.4288 (0.4146)\taccu 85.938 (92.448)\n",
      "Epoch-9   680 batches\tloss 0.3876 (0.4145)\taccu 96.875 (92.447)\n",
      "Epoch-9   700 batches\tloss 0.3846 (0.4143)\taccu 96.875 (92.475)\n",
      "Epoch-9   720 batches\tloss 0.4307 (0.4141)\taccu 89.062 (92.493)\n",
      "Epoch-9   740 batches\tloss 0.3882 (0.4140)\taccu 95.312 (92.508)\n",
      "Epoch-9   760 batches\tloss 0.4079 (0.4139)\taccu 96.875 (92.529)\n",
      "Epoch-9   780 batches\tloss 0.4062 (0.4141)\taccu 93.750 (92.500)\n",
      "Epoch-9   800 batches\tloss 0.4289 (0.4139)\taccu 93.750 (92.529)\n",
      "Epoch-9   820 batches\tloss 0.4041 (0.4141)\taccu 93.750 (92.523)\n",
      "Epoch-9   840 batches\tloss 0.3877 (0.4138)\taccu 92.188 (92.543)\n",
      "Epoch-9   860 batches\tloss 0.4122 (0.4138)\taccu 92.188 (92.542)\n",
      "Epoch-9   223.7s\tTrain: loss 0.4140\taccu 92.5309\tValid: loss 0.4568\taccu 88.7129\n",
      "Epoch 9: val_acc did not improve\n",
      "9 0.001\n",
      "Epoch-10   20 batches\tloss 0.4305 (0.4173)\taccu 89.062 (92.188)\n",
      "Epoch-10   40 batches\tloss 0.3835 (0.4095)\taccu 92.188 (92.969)\n",
      "Epoch-10   60 batches\tloss 0.4359 (0.4109)\taccu 89.062 (92.786)\n",
      "Epoch-10   80 batches\tloss 0.3909 (0.4124)\taccu 95.312 (92.637)\n",
      "Epoch-10  100 batches\tloss 0.4147 (0.4109)\taccu 95.312 (92.844)\n",
      "Epoch-10  120 batches\tloss 0.4107 (0.4114)\taccu 93.750 (92.773)\n",
      "Epoch-10  140 batches\tloss 0.3860 (0.4113)\taccu 93.750 (92.779)\n",
      "Epoch-10  160 batches\tloss 0.4083 (0.4111)\taccu 95.312 (92.891)\n",
      "Epoch-10  180 batches\tloss 0.4100 (0.4102)\taccu 93.750 (92.917)\n",
      "Epoch-10  200 batches\tloss 0.4412 (0.4101)\taccu 90.625 (92.953)\n",
      "Epoch-10  220 batches\tloss 0.4664 (0.4103)\taccu 87.500 (92.898)\n",
      "Epoch-10  240 batches\tloss 0.4232 (0.4114)\taccu 90.625 (92.793)\n",
      "Epoch-10  260 batches\tloss 0.4006 (0.4113)\taccu 92.188 (92.788)\n",
      "Epoch-10  280 batches\tloss 0.4662 (0.4106)\taccu 90.625 (92.879)\n",
      "Epoch-10  300 batches\tloss 0.4103 (0.4100)\taccu 89.062 (92.953)\n",
      "Epoch-10  320 batches\tloss 0.3956 (0.4096)\taccu 95.312 (93.018)\n",
      "Epoch-10  340 batches\tloss 0.4566 (0.4101)\taccu 89.062 (92.955)\n",
      "Epoch-10  360 batches\tloss 0.4590 (0.4097)\taccu 90.625 (92.982)\n",
      "Epoch-10  380 batches\tloss 0.4833 (0.4099)\taccu 87.500 (92.973)\n",
      "Epoch-10  400 batches\tloss 0.3914 (0.4101)\taccu 95.312 (92.902)\n",
      "Epoch-10  420 batches\tloss 0.4269 (0.4100)\taccu 95.312 (92.920)\n",
      "Epoch-10  440 batches\tloss 0.4559 (0.4106)\taccu 85.938 (92.873)\n",
      "Epoch-10  460 batches\tloss 0.4558 (0.4111)\taccu 90.625 (92.826)\n",
      "Epoch-10  480 batches\tloss 0.4237 (0.4111)\taccu 89.062 (92.835)\n",
      "Epoch-10  500 batches\tloss 0.4297 (0.4106)\taccu 89.062 (92.897)\n",
      "Epoch-10  520 batches\tloss 0.3857 (0.4104)\taccu 95.312 (92.921)\n",
      "Epoch-10  540 batches\tloss 0.4554 (0.4107)\taccu 85.938 (92.853)\n",
      "Epoch-10  560 batches\tloss 0.3853 (0.4111)\taccu 95.312 (92.832)\n",
      "Epoch-10  580 batches\tloss 0.4161 (0.4109)\taccu 92.188 (92.842)\n",
      "Epoch-10  600 batches\tloss 0.3890 (0.4107)\taccu 95.312 (92.859)\n",
      "Epoch-10  620 batches\tloss 0.4042 (0.4109)\taccu 92.188 (92.840)\n",
      "Epoch-10  640 batches\tloss 0.4069 (0.4110)\taccu 93.750 (92.834)\n",
      "Epoch-10  660 batches\tloss 0.4480 (0.4111)\taccu 90.625 (92.839)\n",
      "Epoch-10  680 batches\tloss 0.3859 (0.4110)\taccu 92.188 (92.856)\n",
      "Epoch-10  700 batches\tloss 0.3932 (0.4108)\taccu 96.875 (92.888)\n",
      "Epoch-10  720 batches\tloss 0.4155 (0.4111)\taccu 89.062 (92.871)\n",
      "Epoch-10  740 batches\tloss 0.3647 (0.4112)\taccu 100.000 (92.846)\n",
      "Epoch-10  760 batches\tloss 0.4103 (0.4109)\taccu 92.188 (92.862)\n",
      "Epoch-10  780 batches\tloss 0.4200 (0.4108)\taccu 93.750 (92.883)\n",
      "Epoch-10  800 batches\tloss 0.4301 (0.4109)\taccu 90.625 (92.873)\n",
      "Epoch-10  820 batches\tloss 0.3604 (0.4111)\taccu 96.875 (92.858)\n",
      "Epoch-10  840 batches\tloss 0.3699 (0.4108)\taccu 95.312 (92.881)\n",
      "Epoch-10  860 batches\tloss 0.4202 (0.4108)\taccu 93.750 (92.894)\n",
      "Epoch-10  214.2s\tTrain: loss 0.4108\taccu 92.8959\tValid: loss 0.4492\taccu 89.7721\n",
      "Epoch 10: val_acc did not improve\n",
      "10 0.001\n",
      "Epoch-11   20 batches\tloss 0.4108 (0.4228)\taccu 90.625 (92.266)\n",
      "Epoch-11   40 batches\tloss 0.3780 (0.4133)\taccu 96.875 (92.773)\n",
      "Epoch-11   60 batches\tloss 0.4653 (0.4159)\taccu 90.625 (92.578)\n",
      "Epoch-11   80 batches\tloss 0.3860 (0.4133)\taccu 95.312 (92.715)\n",
      "Epoch-11  100 batches\tloss 0.4168 (0.4133)\taccu 92.188 (92.750)\n",
      "Epoch-11  120 batches\tloss 0.4136 (0.4136)\taccu 90.625 (92.773)\n",
      "Epoch-11  140 batches\tloss 0.3863 (0.4128)\taccu 95.312 (92.835)\n",
      "Epoch-11  160 batches\tloss 0.4028 (0.4120)\taccu 95.312 (92.939)\n",
      "Epoch-11  180 batches\tloss 0.3697 (0.4124)\taccu 95.312 (92.891)\n",
      "Epoch-11  200 batches\tloss 0.3752 (0.4116)\taccu 98.438 (92.945)\n",
      "Epoch-11  220 batches\tloss 0.4048 (0.4116)\taccu 92.188 (92.955)\n",
      "Epoch-11  240 batches\tloss 0.3921 (0.4110)\taccu 95.312 (92.949)\n",
      "Epoch-11  260 batches\tloss 0.4086 (0.4102)\taccu 93.750 (93.011)\n",
      "Epoch-11  280 batches\tloss 0.3786 (0.4096)\taccu 93.750 (93.086)\n",
      "Epoch-11  300 batches\tloss 0.3900 (0.4098)\taccu 95.312 (93.078)\n",
      "Epoch-11  320 batches\tloss 0.4125 (0.4095)\taccu 93.750 (93.115)\n",
      "Epoch-11  340 batches\tloss 0.4076 (0.4091)\taccu 95.312 (93.125)\n",
      "Epoch-11  360 batches\tloss 0.3808 (0.4088)\taccu 95.312 (93.160)\n",
      "Epoch-11  380 batches\tloss 0.4047 (0.4087)\taccu 90.625 (93.146)\n",
      "Epoch-11  400 batches\tloss 0.4077 (0.4087)\taccu 93.750 (93.145)\n",
      "Epoch-11  420 batches\tloss 0.4481 (0.4087)\taccu 90.625 (93.155)\n",
      "Epoch-11  440 batches\tloss 0.4200 (0.4089)\taccu 95.312 (93.136)\n",
      "Epoch-11  460 batches\tloss 0.3699 (0.4085)\taccu 98.438 (93.176)\n",
      "Epoch-11  480 batches\tloss 0.4043 (0.4083)\taccu 90.625 (93.171)\n",
      "Epoch-11  500 batches\tloss 0.4012 (0.4081)\taccu 96.875 (93.197)\n",
      "Epoch-11  520 batches\tloss 0.4452 (0.4078)\taccu 89.062 (93.230)\n",
      "Epoch-11  540 batches\tloss 0.4743 (0.4078)\taccu 85.938 (93.238)\n",
      "Epoch-11  560 batches\tloss 0.4088 (0.4077)\taccu 93.750 (93.237)\n",
      "Epoch-11  580 batches\tloss 0.4309 (0.4078)\taccu 89.062 (93.219)\n",
      "Epoch-11  600 batches\tloss 0.3658 (0.4077)\taccu 96.875 (93.221)\n",
      "Epoch-11  620 batches\tloss 0.3841 (0.4078)\taccu 95.312 (93.188)\n",
      "Epoch-11  640 batches\tloss 0.4439 (0.4081)\taccu 90.625 (93.176)\n",
      "Epoch-11  660 batches\tloss 0.3606 (0.4081)\taccu 100.000 (93.175)\n",
      "Epoch-11  680 batches\tloss 0.3991 (0.4081)\taccu 95.312 (93.157)\n",
      "Epoch-11  700 batches\tloss 0.3966 (0.4079)\taccu 95.312 (93.174)\n",
      "Epoch-11  720 batches\tloss 0.3731 (0.4077)\taccu 96.875 (93.190)\n",
      "Epoch-11  740 batches\tloss 0.4289 (0.4077)\taccu 90.625 (93.214)\n",
      "Epoch-11  760 batches\tloss 0.4040 (0.4074)\taccu 93.750 (93.232)\n",
      "Epoch-11  780 batches\tloss 0.3805 (0.4074)\taccu 96.875 (93.231)\n",
      "Epoch-11  800 batches\tloss 0.4177 (0.4073)\taccu 90.625 (93.234)\n",
      "Epoch-11  820 batches\tloss 0.4050 (0.4068)\taccu 92.188 (93.276)\n",
      "Epoch-11  840 batches\tloss 0.4102 (0.4066)\taccu 93.750 (93.292)\n",
      "Epoch-11  860 batches\tloss 0.3951 (0.4065)\taccu 95.312 (93.305)\n",
      "Epoch-11  222.5s\tTrain: loss 0.4065\taccu 93.2897\tValid: loss 0.4444\taccu 90.1916\n",
      "Epoch 11: val_acc did not improve\n",
      "11 0.001\n",
      "Epoch-12   20 batches\tloss 0.4154 (0.4182)\taccu 92.188 (92.109)\n",
      "Epoch-12   40 batches\tloss 0.3537 (0.4002)\taccu 98.438 (94.219)\n",
      "Epoch-12   60 batches\tloss 0.4077 (0.4036)\taccu 93.750 (93.776)\n",
      "Epoch-12   80 batches\tloss 0.4090 (0.4065)\taccu 93.750 (93.418)\n",
      "Epoch-12  100 batches\tloss 0.3721 (0.4058)\taccu 96.875 (93.453)\n",
      "Epoch-12  120 batches\tloss 0.4458 (0.4055)\taccu 87.500 (93.464)\n",
      "Epoch-12  140 batches\tloss 0.4100 (0.4058)\taccu 90.625 (93.449)\n",
      "Epoch-12  160 batches\tloss 0.4007 (0.4057)\taccu 93.750 (93.438)\n",
      "Epoch-12  180 batches\tloss 0.3893 (0.4054)\taccu 95.312 (93.464)\n",
      "Epoch-12  200 batches\tloss 0.4405 (0.4056)\taccu 90.625 (93.461)\n",
      "Epoch-12  220 batches\tloss 0.4015 (0.4067)\taccu 93.750 (93.366)\n",
      "Epoch-12  240 batches\tloss 0.3948 (0.4068)\taccu 95.312 (93.281)\n",
      "Epoch-12  260 batches\tloss 0.4367 (0.4069)\taccu 93.750 (93.311)\n",
      "Epoch-12  280 batches\tloss 0.3964 (0.4061)\taccu 93.750 (93.404)\n",
      "Epoch-12  300 batches\tloss 0.4428 (0.4062)\taccu 90.625 (93.391)\n",
      "Epoch-12  320 batches\tloss 0.4041 (0.4060)\taccu 93.750 (93.374)\n",
      "Epoch-12  340 batches\tloss 0.3747 (0.4067)\taccu 98.438 (93.300)\n",
      "Epoch-12  360 batches\tloss 0.4034 (0.4064)\taccu 93.750 (93.312)\n",
      "Epoch-12  380 batches\tloss 0.4382 (0.4060)\taccu 90.625 (93.339)\n",
      "Epoch-12  400 batches\tloss 0.3869 (0.4054)\taccu 95.312 (93.379)\n",
      "Epoch-12  420 batches\tloss 0.3937 (0.4054)\taccu 95.312 (93.359)\n",
      "Epoch-12  440 batches\tloss 0.3919 (0.4060)\taccu 92.188 (93.303)\n",
      "Epoch-12  460 batches\tloss 0.3921 (0.4067)\taccu 92.188 (93.237)\n",
      "Epoch-12  480 batches\tloss 0.3619 (0.4069)\taccu 96.875 (93.203)\n",
      "Epoch-12  500 batches\tloss 0.4600 (0.4073)\taccu 90.625 (93.178)\n",
      "Epoch-12  520 batches\tloss 0.4174 (0.4074)\taccu 95.312 (93.188)\n",
      "Epoch-12  540 batches\tloss 0.4189 (0.4082)\taccu 93.750 (93.108)\n",
      "Epoch-12  560 batches\tloss 0.4233 (0.4080)\taccu 90.625 (93.103)\n",
      "Epoch-12  580 batches\tloss 0.3859 (0.4077)\taccu 92.188 (93.114)\n",
      "Epoch-12  600 batches\tloss 0.3609 (0.4076)\taccu 98.438 (93.130)\n",
      "Epoch-12  620 batches\tloss 0.4693 (0.4077)\taccu 82.812 (93.110)\n",
      "Epoch-12  640 batches\tloss 0.4215 (0.4075)\taccu 89.062 (93.147)\n",
      "Epoch-12  660 batches\tloss 0.3846 (0.4073)\taccu 96.875 (93.182)\n",
      "Epoch-12  680 batches\tloss 0.4199 (0.4070)\taccu 92.188 (93.212)\n",
      "Epoch-12  700 batches\tloss 0.4204 (0.4068)\taccu 90.625 (93.248)\n",
      "Epoch-12  720 batches\tloss 0.4141 (0.4065)\taccu 95.312 (93.273)\n",
      "Epoch-12  740 batches\tloss 0.3713 (0.4063)\taccu 98.438 (93.275)\n",
      "Epoch-12  760 batches\tloss 0.4438 (0.4061)\taccu 87.500 (93.300)\n",
      "Epoch-12  780 batches\tloss 0.3707 (0.4063)\taccu 95.312 (93.285)\n",
      "Epoch-12  800 batches\tloss 0.4121 (0.4064)\taccu 93.750 (93.268)\n",
      "Epoch-12  820 batches\tloss 0.4040 (0.4061)\taccu 92.188 (93.285)\n",
      "Epoch-12  840 batches\tloss 0.4247 (0.4060)\taccu 89.062 (93.287)\n",
      "Epoch-12  860 batches\tloss 0.3957 (0.4058)\taccu 93.750 (93.310)\n",
      "Epoch-12  217.4s\tTrain: loss 0.4059\taccu 93.3077\tValid: loss 0.4461\taccu 90.0203\n",
      "Epoch 12: val_acc did not improve\n",
      "12 0.001\n",
      "Epoch-13   20 batches\tloss 0.3796 (0.4046)\taccu 96.875 (93.672)\n",
      "Epoch-13   40 batches\tloss 0.3876 (0.4046)\taccu 95.312 (93.633)\n",
      "Epoch-13   60 batches\tloss 0.3888 (0.3993)\taccu 95.312 (94.089)\n",
      "Epoch-13   80 batches\tloss 0.3871 (0.3988)\taccu 96.875 (94.043)\n",
      "Epoch-13  100 batches\tloss 0.3809 (0.4002)\taccu 95.312 (93.891)\n",
      "Epoch-13  120 batches\tloss 0.4385 (0.4014)\taccu 89.062 (93.737)\n",
      "Epoch-13  140 batches\tloss 0.3810 (0.4026)\taccu 96.875 (93.627)\n",
      "Epoch-13  160 batches\tloss 0.4588 (0.4028)\taccu 85.938 (93.604)\n",
      "Epoch-13  180 batches\tloss 0.3892 (0.4029)\taccu 96.875 (93.611)\n",
      "Epoch-13  200 batches\tloss 0.3824 (0.4037)\taccu 95.312 (93.539)\n",
      "Epoch-13  220 batches\tloss 0.3666 (0.4035)\taccu 96.875 (93.572)\n",
      "Epoch-13  240 batches\tloss 0.3930 (0.4031)\taccu 93.750 (93.620)\n",
      "Epoch-13  260 batches\tloss 0.4045 (0.4028)\taccu 95.312 (93.648)\n",
      "Epoch-13  280 batches\tloss 0.3946 (0.4019)\taccu 95.312 (93.722)\n",
      "Epoch-13  300 batches\tloss 0.4003 (0.4024)\taccu 93.750 (93.703)\n",
      "Epoch-13  320 batches\tloss 0.4044 (0.4024)\taccu 93.750 (93.701)\n",
      "Epoch-13  340 batches\tloss 0.3754 (0.4021)\taccu 95.312 (93.727)\n",
      "Epoch-13  360 batches\tloss 0.4058 (0.4022)\taccu 92.188 (93.733)\n",
      "Epoch-13  380 batches\tloss 0.4635 (0.4025)\taccu 81.250 (93.684)\n",
      "Epoch-13  400 batches\tloss 0.4094 (0.4026)\taccu 93.750 (93.664)\n",
      "Epoch-13  420 batches\tloss 0.4098 (0.4027)\taccu 92.188 (93.657)\n",
      "Epoch-13  440 batches\tloss 0.4546 (0.4025)\taccu 89.062 (93.707)\n",
      "Epoch-13  460 batches\tloss 0.4413 (0.4028)\taccu 90.625 (93.665)\n",
      "Epoch-13  480 batches\tloss 0.4182 (0.4028)\taccu 95.312 (93.678)\n",
      "Epoch-13  500 batches\tloss 0.4247 (0.4030)\taccu 90.625 (93.647)\n",
      "Epoch-13  520 batches\tloss 0.3964 (0.4027)\taccu 93.750 (93.666)\n",
      "Epoch-13  540 batches\tloss 0.3764 (0.4029)\taccu 96.875 (93.628)\n",
      "Epoch-13  560 batches\tloss 0.4145 (0.4027)\taccu 92.188 (93.641)\n",
      "Epoch-13  580 batches\tloss 0.4087 (0.4027)\taccu 93.750 (93.631)\n",
      "Epoch-13  600 batches\tloss 0.3845 (0.4026)\taccu 93.750 (93.646)\n",
      "Epoch-13  620 batches\tloss 0.3976 (0.4027)\taccu 92.188 (93.634)\n",
      "Epoch-13  640 batches\tloss 0.4114 (0.4025)\taccu 92.188 (93.643)\n",
      "Epoch-13  660 batches\tloss 0.4020 (0.4027)\taccu 93.750 (93.617)\n",
      "Epoch-13  680 batches\tloss 0.3846 (0.4024)\taccu 98.438 (93.658)\n",
      "Epoch-13  700 batches\tloss 0.3658 (0.4024)\taccu 96.875 (93.645)\n",
      "Epoch-13  720 batches\tloss 0.3594 (0.4026)\taccu 98.438 (93.633)\n",
      "Epoch-13  740 batches\tloss 0.3933 (0.4022)\taccu 96.875 (93.689)\n",
      "Epoch-13  760 batches\tloss 0.3701 (0.4018)\taccu 96.875 (93.711)\n",
      "Epoch-13  780 batches\tloss 0.4179 (0.4019)\taccu 92.188 (93.702)\n",
      "Epoch-13  800 batches\tloss 0.4202 (0.4023)\taccu 92.188 (93.650)\n",
      "Epoch-13  820 batches\tloss 0.4610 (0.4024)\taccu 87.500 (93.657)\n",
      "Epoch-13  840 batches\tloss 0.3866 (0.4021)\taccu 93.750 (93.687)\n",
      "Epoch-13  860 batches\tloss 0.4069 (0.4022)\taccu 93.750 (93.665)\n",
      "Epoch-13  218.2s\tTrain: loss 0.4021\taccu 93.6763\tValid: loss 0.4485\taccu 89.8595\n",
      "Epoch 13: val_acc did not improve\n",
      "13 0.001\n",
      "Epoch-14   20 batches\tloss 0.4099 (0.3923)\taccu 90.625 (94.531)\n",
      "Epoch-14   40 batches\tloss 0.3953 (0.3966)\taccu 93.750 (93.828)\n",
      "Epoch-14   60 batches\tloss 0.3963 (0.4027)\taccu 93.750 (93.047)\n",
      "Epoch-14   80 batches\tloss 0.4265 (0.4049)\taccu 90.625 (92.871)\n",
      "Epoch-14  100 batches\tloss 0.4028 (0.4028)\taccu 93.750 (93.156)\n",
      "Epoch-14  120 batches\tloss 0.4159 (0.4016)\taccu 92.188 (93.372)\n",
      "Epoch-14  140 batches\tloss 0.3563 (0.4027)\taccu 98.438 (93.304)\n",
      "Epoch-14  160 batches\tloss 0.4661 (0.4038)\taccu 85.938 (93.164)\n",
      "Epoch-14  180 batches\tloss 0.4479 (0.4025)\taccu 90.625 (93.316)\n",
      "Epoch-14  200 batches\tloss 0.3605 (0.4002)\taccu 98.438 (93.562)\n",
      "Epoch-14  220 batches\tloss 0.3899 (0.4008)\taccu 95.312 (93.523)\n",
      "Epoch-14  240 batches\tloss 0.3638 (0.3991)\taccu 96.875 (93.704)\n",
      "Epoch-14  260 batches\tloss 0.3818 (0.3989)\taccu 96.875 (93.750)\n",
      "Epoch-14  280 batches\tloss 0.3808 (0.3988)\taccu 96.875 (93.811)\n",
      "Epoch-14  300 batches\tloss 0.4161 (0.3987)\taccu 90.625 (93.854)\n",
      "Epoch-14  320 batches\tloss 0.4139 (0.3994)\taccu 92.188 (93.730)\n",
      "Epoch-14  340 batches\tloss 0.4443 (0.3996)\taccu 89.062 (93.741)\n",
      "Epoch-14  360 batches\tloss 0.4008 (0.4000)\taccu 93.750 (93.707)\n",
      "Epoch-14  380 batches\tloss 0.3965 (0.4001)\taccu 96.875 (93.692)\n",
      "Epoch-14  400 batches\tloss 0.4671 (0.4006)\taccu 89.062 (93.664)\n",
      "Epoch-14  420 batches\tloss 0.3778 (0.4003)\taccu 95.312 (93.720)\n",
      "Epoch-14  440 batches\tloss 0.3820 (0.4001)\taccu 95.312 (93.725)\n",
      "Epoch-14  460 batches\tloss 0.3914 (0.4005)\taccu 93.750 (93.702)\n",
      "Epoch-14  480 batches\tloss 0.4154 (0.4008)\taccu 92.188 (93.656)\n",
      "Epoch-14  500 batches\tloss 0.4026 (0.4007)\taccu 95.312 (93.663)\n",
      "Epoch-14  520 batches\tloss 0.3755 (0.4007)\taccu 95.312 (93.663)\n",
      "Epoch-14  540 batches\tloss 0.3496 (0.4005)\taccu 98.438 (93.689)\n",
      "Epoch-14  560 batches\tloss 0.3669 (0.4007)\taccu 98.438 (93.686)\n",
      "Epoch-14  580 batches\tloss 0.4162 (0.4005)\taccu 93.750 (93.715)\n",
      "Epoch-14  600 batches\tloss 0.4033 (0.4004)\taccu 92.188 (93.729)\n",
      "Epoch-14  620 batches\tloss 0.4278 (0.4007)\taccu 90.625 (93.705)\n",
      "Epoch-14  640 batches\tloss 0.4135 (0.4006)\taccu 92.188 (93.723)\n",
      "Epoch-14  660 batches\tloss 0.3645 (0.4003)\taccu 98.438 (93.757)\n",
      "Epoch-14  680 batches\tloss 0.3684 (0.4007)\taccu 96.875 (93.725)\n",
      "Epoch-14  700 batches\tloss 0.3867 (0.4004)\taccu 93.750 (93.768)\n",
      "Epoch-14  720 batches\tloss 0.4984 (0.4005)\taccu 85.938 (93.757)\n",
      "Epoch-14  740 batches\tloss 0.4291 (0.4003)\taccu 89.062 (93.790)\n",
      "Epoch-14  760 batches\tloss 0.3809 (0.4003)\taccu 93.750 (93.793)\n",
      "Epoch-14  780 batches\tloss 0.3876 (0.4004)\taccu 95.312 (93.772)\n",
      "Epoch-14  800 batches\tloss 0.4262 (0.4002)\taccu 92.188 (93.799)\n",
      "Epoch-14  820 batches\tloss 0.4289 (0.4000)\taccu 92.188 (93.815)\n",
      "Epoch-14  840 batches\tloss 0.4116 (0.4000)\taccu 92.188 (93.821)\n",
      "Epoch-14  860 batches\tloss 0.3542 (0.3997)\taccu 98.438 (93.848)\n",
      "Epoch-14  232.0s\tTrain: loss 0.3997\taccu 93.8435\tValid: loss 0.4540\taccu 89.2303\n",
      "Epoch 14: val_acc did not improve\n",
      "14 0.001\n",
      "Epoch-15   20 batches\tloss 0.4266 (0.4068)\taccu 90.625 (93.047)\n",
      "Epoch-15   40 batches\tloss 0.4341 (0.4030)\taccu 90.625 (93.750)\n",
      "Epoch-15   60 batches\tloss 0.4028 (0.4013)\taccu 92.188 (93.750)\n",
      "Epoch-15   80 batches\tloss 0.4151 (0.4024)\taccu 93.750 (93.535)\n",
      "Epoch-15  100 batches\tloss 0.4101 (0.3993)\taccu 92.188 (93.875)\n",
      "Epoch-15  120 batches\tloss 0.3917 (0.3983)\taccu 95.312 (94.089)\n",
      "Epoch-15  140 batches\tloss 0.4207 (0.3986)\taccu 93.750 (94.029)\n",
      "Epoch-15  160 batches\tloss 0.4000 (0.3989)\taccu 92.188 (93.955)\n",
      "Epoch-15  180 batches\tloss 0.3947 (0.3989)\taccu 96.875 (93.915)\n",
      "Epoch-15  200 batches\tloss 0.3725 (0.3986)\taccu 98.438 (93.875)\n",
      "Epoch-15  220 batches\tloss 0.4159 (0.3986)\taccu 93.750 (93.920)\n",
      "Epoch-15  240 batches\tloss 0.4188 (0.3988)\taccu 90.625 (93.893)\n",
      "Epoch-15  260 batches\tloss 0.4165 (0.3986)\taccu 93.750 (93.846)\n",
      "Epoch-15  280 batches\tloss 0.4171 (0.3983)\taccu 89.062 (93.850)\n",
      "Epoch-15  300 batches\tloss 0.4028 (0.3976)\taccu 93.750 (93.911)\n",
      "Epoch-15  320 batches\tloss 0.3563 (0.3981)\taccu 98.438 (93.872)\n",
      "Epoch-15  340 batches\tloss 0.4869 (0.3985)\taccu 85.938 (93.814)\n",
      "Epoch-15  360 batches\tloss 0.3906 (0.3985)\taccu 93.750 (93.789)\n",
      "Epoch-15  380 batches\tloss 0.4477 (0.3985)\taccu 89.062 (93.812)\n",
      "Epoch-15  400 batches\tloss 0.3739 (0.3982)\taccu 96.875 (93.852)\n",
      "Epoch-15  420 batches\tloss 0.4314 (0.3981)\taccu 90.625 (93.865)\n",
      "Epoch-15  440 batches\tloss 0.3885 (0.3983)\taccu 96.875 (93.857)\n",
      "Epoch-15  460 batches\tloss 0.3871 (0.3988)\taccu 96.875 (93.811)\n",
      "Epoch-15  480 batches\tloss 0.4421 (0.3992)\taccu 90.625 (93.786)\n",
      "Epoch-15  500 batches\tloss 0.4055 (0.3994)\taccu 93.750 (93.784)\n",
      "Epoch-15  520 batches\tloss 0.3811 (0.3996)\taccu 96.875 (93.768)\n",
      "Epoch-15  540 batches\tloss 0.3917 (0.3990)\taccu 93.750 (93.837)\n",
      "Epoch-15  560 batches\tloss 0.4086 (0.3991)\taccu 93.750 (93.836)\n",
      "Epoch-15  580 batches\tloss 0.3684 (0.3988)\taccu 98.438 (93.866)\n",
      "Epoch-15  600 batches\tloss 0.4354 (0.3992)\taccu 92.188 (93.839)\n",
      "Epoch-15  620 batches\tloss 0.3970 (0.3991)\taccu 95.312 (93.851)\n",
      "Epoch-15  640 batches\tloss 0.4233 (0.3993)\taccu 93.750 (93.826)\n",
      "Epoch-15  660 batches\tloss 0.3838 (0.3993)\taccu 95.312 (93.833)\n",
      "Epoch-15  680 batches\tloss 0.3723 (0.3993)\taccu 96.875 (93.840)\n",
      "Epoch-15  700 batches\tloss 0.3966 (0.3994)\taccu 95.312 (93.819)\n",
      "Epoch-15  720 batches\tloss 0.3783 (0.3993)\taccu 95.312 (93.837)\n",
      "Epoch-15  740 batches\tloss 0.3739 (0.3990)\taccu 95.312 (93.853)\n",
      "Epoch-15  760 batches\tloss 0.3816 (0.3989)\taccu 96.875 (93.863)\n",
      "Epoch-15  780 batches\tloss 0.3929 (0.3988)\taccu 90.625 (93.870)\n",
      "Epoch-15  800 batches\tloss 0.3857 (0.3987)\taccu 95.312 (93.881)\n",
      "Epoch-15  820 batches\tloss 0.3919 (0.3986)\taccu 92.188 (93.895)\n",
      "Epoch-15  840 batches\tloss 0.4559 (0.3987)\taccu 85.938 (93.882)\n",
      "Epoch-15  860 batches\tloss 0.4698 (0.3988)\taccu 87.500 (93.861)\n",
      "Epoch-15  204.2s\tTrain: loss 0.3990\taccu 93.8507\tValid: loss 0.4488\taccu 89.6008\n",
      "Epoch 15: val_acc did not improve\n",
      "15 0.001\n",
      "Epoch-16   20 batches\tloss 0.4300 (0.3929)\taccu 89.062 (94.531)\n",
      "Epoch-16   40 batches\tloss 0.3648 (0.3942)\taccu 96.875 (93.984)\n",
      "Epoch-16   60 batches\tloss 0.4406 (0.3954)\taccu 89.062 (93.958)\n",
      "Epoch-16   80 batches\tloss 0.4053 (0.3947)\taccu 95.312 (94.258)\n",
      "Epoch-16  100 batches\tloss 0.3767 (0.3934)\taccu 96.875 (94.516)\n",
      "Epoch-16  120 batches\tloss 0.3791 (0.3940)\taccu 95.312 (94.427)\n",
      "Epoch-16  140 batches\tloss 0.4619 (0.3930)\taccu 85.938 (94.531)\n",
      "Epoch-16  160 batches\tloss 0.3843 (0.3929)\taccu 95.312 (94.473)\n",
      "Epoch-16  180 batches\tloss 0.4016 (0.3924)\taccu 92.188 (94.557)\n",
      "Epoch-16  200 batches\tloss 0.3739 (0.3930)\taccu 93.750 (94.500)\n",
      "Epoch-16  220 batches\tloss 0.3988 (0.3930)\taccu 90.625 (94.453)\n",
      "Epoch-16  240 batches\tloss 0.3910 (0.3941)\taccu 95.312 (94.336)\n",
      "Epoch-16  260 batches\tloss 0.3622 (0.3944)\taccu 98.438 (94.345)\n",
      "Epoch-16  280 batches\tloss 0.4010 (0.3947)\taccu 92.188 (94.353)\n",
      "Epoch-16  300 batches\tloss 0.3630 (0.3940)\taccu 96.875 (94.438)\n",
      "Epoch-16  320 batches\tloss 0.3696 (0.3939)\taccu 95.312 (94.434)\n",
      "Epoch-16  340 batches\tloss 0.3881 (0.3938)\taccu 95.312 (94.476)\n",
      "Epoch-16  360 batches\tloss 0.3749 (0.3939)\taccu 95.312 (94.444)\n",
      "Epoch-16  380 batches\tloss 0.4362 (0.3936)\taccu 92.188 (94.478)\n",
      "Epoch-16  400 batches\tloss 0.3557 (0.3938)\taccu 96.875 (94.426)\n",
      "Epoch-16  420 batches\tloss 0.3630 (0.3938)\taccu 98.438 (94.416)\n",
      "Epoch-16  440 batches\tloss 0.3785 (0.3932)\taccu 95.312 (94.503)\n",
      "Epoch-16  460 batches\tloss 0.3899 (0.3937)\taccu 92.188 (94.433)\n",
      "Epoch-16  480 batches\tloss 0.4144 (0.3943)\taccu 90.625 (94.346)\n",
      "Epoch-16  500 batches\tloss 0.3590 (0.3948)\taccu 98.438 (94.288)\n",
      "Epoch-16  520 batches\tloss 0.3706 (0.3949)\taccu 95.312 (94.276)\n",
      "Epoch-16  540 batches\tloss 0.3899 (0.3949)\taccu 96.875 (94.268)\n",
      "Epoch-16  560 batches\tloss 0.4044 (0.3948)\taccu 93.750 (94.283)\n",
      "Epoch-16  580 batches\tloss 0.3704 (0.3947)\taccu 95.312 (94.289)\n",
      "Epoch-16  600 batches\tloss 0.4254 (0.3945)\taccu 89.062 (94.302)\n",
      "Epoch-16  620 batches\tloss 0.3990 (0.3948)\taccu 95.312 (94.279)\n",
      "Epoch-16  640 batches\tloss 0.4078 (0.3949)\taccu 92.188 (94.258)\n",
      "Epoch-16  660 batches\tloss 0.3818 (0.3950)\taccu 96.875 (94.245)\n",
      "Epoch-16  680 batches\tloss 0.3651 (0.3947)\taccu 95.312 (94.262)\n",
      "Epoch-16  700 batches\tloss 0.3760 (0.3952)\taccu 96.875 (94.219)\n",
      "Epoch-16  720 batches\tloss 0.4003 (0.3953)\taccu 90.625 (94.210)\n",
      "Epoch-16  740 batches\tloss 0.4221 (0.3953)\taccu 92.188 (94.210)\n",
      "Epoch-16  760 batches\tloss 0.3738 (0.3953)\taccu 95.312 (94.206)\n",
      "Epoch-16  780 batches\tloss 0.4274 (0.3952)\taccu 92.188 (94.231)\n",
      "Epoch-16  800 batches\tloss 0.3821 (0.3952)\taccu 93.750 (94.229)\n",
      "Epoch-16  820 batches\tloss 0.3776 (0.3953)\taccu 96.875 (94.205)\n",
      "Epoch-16  840 batches\tloss 0.3772 (0.3953)\taccu 95.312 (94.209)\n",
      "Epoch-16  860 batches\tloss 0.3843 (0.3953)\taccu 93.750 (94.208)\n",
      "Epoch-16  202.3s\tTrain: loss 0.3953\taccu 94.2067\tValid: loss 0.4510\taccu 89.4295\n",
      "Epoch 16: val_acc did not improve\n",
      "16 0.001\n",
      "Epoch-17   20 batches\tloss 0.3885 (0.3900)\taccu 92.188 (94.766)\n",
      "Epoch-17   40 batches\tloss 0.3621 (0.3878)\taccu 98.438 (95.117)\n",
      "Epoch-17   60 batches\tloss 0.3891 (0.3868)\taccu 95.312 (95.026)\n",
      "Epoch-17   80 batches\tloss 0.3955 (0.3863)\taccu 93.750 (95.059)\n",
      "Epoch-17  100 batches\tloss 0.4295 (0.3876)\taccu 92.188 (94.906)\n",
      "Epoch-17  120 batches\tloss 0.3794 (0.3884)\taccu 95.312 (94.792)\n",
      "Epoch-17  140 batches\tloss 0.4328 (0.3899)\taccu 90.625 (94.766)\n",
      "Epoch-17  160 batches\tloss 0.3854 (0.3897)\taccu 92.188 (94.756)\n",
      "Epoch-17  180 batches\tloss 0.4403 (0.3903)\taccu 87.500 (94.705)\n",
      "Epoch-17  200 batches\tloss 0.3866 (0.3917)\taccu 93.750 (94.547)\n",
      "Epoch-17  220 batches\tloss 0.3843 (0.3918)\taccu 93.750 (94.567)\n",
      "Epoch-17  240 batches\tloss 0.4252 (0.3924)\taccu 92.188 (94.531)\n",
      "Epoch-17  260 batches\tloss 0.4138 (0.3928)\taccu 92.188 (94.447)\n",
      "Epoch-17  280 batches\tloss 0.3993 (0.3934)\taccu 92.188 (94.353)\n",
      "Epoch-17  300 batches\tloss 0.4462 (0.3942)\taccu 90.625 (94.276)\n",
      "Epoch-17  320 batches\tloss 0.4008 (0.3952)\taccu 95.312 (94.214)\n",
      "Epoch-17  340 batches\tloss 0.4477 (0.3955)\taccu 89.062 (94.233)\n",
      "Epoch-17  360 batches\tloss 0.3602 (0.3953)\taccu 98.438 (94.240)\n",
      "Epoch-17  380 batches\tloss 0.4025 (0.3948)\taccu 92.188 (94.280)\n",
      "Epoch-17  400 batches\tloss 0.4150 (0.3953)\taccu 92.188 (94.230)\n",
      "Epoch-17  420 batches\tloss 0.3542 (0.3953)\taccu 98.438 (94.219)\n",
      "Epoch-17  440 batches\tloss 0.4422 (0.3955)\taccu 90.625 (94.197)\n",
      "Epoch-17  460 batches\tloss 0.3898 (0.3952)\taccu 95.312 (94.222)\n",
      "Epoch-17  480 batches\tloss 0.4187 (0.3951)\taccu 92.188 (94.232)\n",
      "Epoch-17  500 batches\tloss 0.3574 (0.3948)\taccu 98.438 (94.253)\n",
      "Epoch-17  520 batches\tloss 0.3799 (0.3947)\taccu 96.875 (94.249)\n",
      "Epoch-17  540 batches\tloss 0.3783 (0.3947)\taccu 96.875 (94.256)\n",
      "Epoch-17  560 batches\tloss 0.3885 (0.3946)\taccu 93.750 (94.261)\n",
      "Epoch-17  580 batches\tloss 0.4146 (0.3946)\taccu 90.625 (94.259)\n",
      "Epoch-17  600 batches\tloss 0.3922 (0.3946)\taccu 95.312 (94.242)\n",
      "Epoch-17  620 batches\tloss 0.4055 (0.3947)\taccu 93.750 (94.221)\n",
      "Epoch-17  640 batches\tloss 0.4159 (0.3949)\taccu 92.188 (94.192)\n",
      "Epoch-17  660 batches\tloss 0.4719 (0.3955)\taccu 89.062 (94.141)\n",
      "Epoch-17  680 batches\tloss 0.3693 (0.3954)\taccu 96.875 (94.152)\n",
      "Epoch-17  700 batches\tloss 0.3626 (0.3952)\taccu 96.875 (94.179)\n",
      "Epoch-17  720 batches\tloss 0.3778 (0.3951)\taccu 95.312 (94.173)\n",
      "Epoch-17  740 batches\tloss 0.3615 (0.3950)\taccu 98.438 (94.196)\n",
      "Epoch-17  760 batches\tloss 0.3806 (0.3951)\taccu 95.312 (94.180)\n",
      "Epoch-17  780 batches\tloss 0.4342 (0.3952)\taccu 90.625 (94.161)\n",
      "Epoch-17  800 batches\tloss 0.3860 (0.3954)\taccu 93.750 (94.148)\n",
      "Epoch-17  820 batches\tloss 0.4427 (0.3956)\taccu 89.062 (94.141)\n",
      "Epoch-17  840 batches\tloss 0.5016 (0.3959)\taccu 85.938 (94.126)\n",
      "Epoch-17  860 batches\tloss 0.3691 (0.3960)\taccu 98.438 (94.124)\n",
      "Epoch-17  201.5s\tTrain: loss 0.3960\taccu 94.1330\tValid: loss 0.4508\taccu 89.7791\n",
      "Epoch 17: val_acc did not improve\n",
      "17 0.001\n",
      "Epoch-18   20 batches\tloss 0.3908 (0.3850)\taccu 96.875 (94.922)\n",
      "Epoch-18   40 batches\tloss 0.3555 (0.3862)\taccu 98.438 (94.922)\n",
      "Epoch-18   60 batches\tloss 0.4306 (0.3862)\taccu 95.312 (95.156)\n",
      "Epoch-18   80 batches\tloss 0.4198 (0.3875)\taccu 90.625 (95.059)\n",
      "Epoch-18  100 batches\tloss 0.3893 (0.3906)\taccu 93.750 (94.641)\n",
      "Epoch-18  120 batches\tloss 0.4562 (0.3918)\taccu 90.625 (94.622)\n",
      "Epoch-18  140 batches\tloss 0.4008 (0.3906)\taccu 93.750 (94.754)\n",
      "Epoch-18  160 batches\tloss 0.3842 (0.3911)\taccu 93.750 (94.678)\n",
      "Epoch-18  180 batches\tloss 0.3978 (0.3903)\taccu 93.750 (94.792)\n",
      "Epoch-18  200 batches\tloss 0.4022 (0.3906)\taccu 95.312 (94.719)\n",
      "Epoch-18  220 batches\tloss 0.4157 (0.3903)\taccu 92.188 (94.751)\n",
      "Epoch-18  240 batches\tloss 0.4006 (0.3904)\taccu 95.312 (94.740)\n",
      "Epoch-18  260 batches\tloss 0.3713 (0.3906)\taccu 98.438 (94.700)\n",
      "Epoch-18  280 batches\tloss 0.3616 (0.3909)\taccu 95.312 (94.710)\n",
      "Epoch-18  300 batches\tloss 0.3643 (0.3913)\taccu 95.312 (94.682)\n",
      "Epoch-18  320 batches\tloss 0.3899 (0.3920)\taccu 95.312 (94.614)\n",
      "Epoch-18  340 batches\tloss 0.4243 (0.3922)\taccu 92.188 (94.573)\n",
      "Epoch-18  360 batches\tloss 0.3848 (0.3928)\taccu 95.312 (94.544)\n",
      "Epoch-18  380 batches\tloss 0.3530 (0.3925)\taccu 100.000 (94.564)\n",
      "Epoch-18  400 batches\tloss 0.3944 (0.3920)\taccu 93.750 (94.582)\n",
      "Epoch-18  420 batches\tloss 0.3748 (0.3915)\taccu 93.750 (94.621)\n",
      "Epoch-18  440 batches\tloss 0.3628 (0.3913)\taccu 96.875 (94.620)\n",
      "Epoch-18  460 batches\tloss 0.3990 (0.3912)\taccu 93.750 (94.623)\n",
      "Epoch-18  480 batches\tloss 0.4164 (0.3917)\taccu 92.188 (94.587)\n",
      "Epoch-18  500 batches\tloss 0.4144 (0.3918)\taccu 90.625 (94.566)\n",
      "Epoch-18  520 batches\tloss 0.4067 (0.3918)\taccu 92.188 (94.558)\n",
      "Epoch-18  540 batches\tloss 0.4236 (0.3919)\taccu 89.062 (94.546)\n",
      "Epoch-18  560 batches\tloss 0.4124 (0.3921)\taccu 92.188 (94.534)\n",
      "Epoch-18  580 batches\tloss 0.4031 (0.3923)\taccu 95.312 (94.491)\n",
      "Epoch-18  600 batches\tloss 0.3605 (0.3922)\taccu 98.438 (94.471)\n",
      "Epoch-18  620 batches\tloss 0.4075 (0.3926)\taccu 89.062 (94.430)\n",
      "Epoch-18  640 batches\tloss 0.4362 (0.3927)\taccu 92.188 (94.441)\n",
      "Epoch-18  660 batches\tloss 0.3890 (0.3925)\taccu 92.188 (94.451)\n",
      "Epoch-18  680 batches\tloss 0.3704 (0.3923)\taccu 96.875 (94.469)\n",
      "Epoch-18  700 batches\tloss 0.3664 (0.3920)\taccu 98.438 (94.507)\n",
      "Epoch-18  720 batches\tloss 0.4056 (0.3921)\taccu 90.625 (94.475)\n",
      "Epoch-18  740 batches\tloss 0.3662 (0.3922)\taccu 96.875 (94.453)\n",
      "Epoch-18  760 batches\tloss 0.3519 (0.3926)\taccu 98.438 (94.422)\n",
      "Epoch-18  780 batches\tloss 0.3575 (0.3925)\taccu 98.438 (94.439)\n",
      "Epoch-18  800 batches\tloss 0.4049 (0.3925)\taccu 96.875 (94.441)\n",
      "Epoch-18  820 batches\tloss 0.3660 (0.3923)\taccu 96.875 (94.455)\n",
      "Epoch-18  840 batches\tloss 0.4125 (0.3924)\taccu 90.625 (94.436)\n",
      "Epoch-18  860 batches\tloss 0.3923 (0.3924)\taccu 95.312 (94.435)\n",
      "Epoch-18  201.5s\tTrain: loss 0.3924\taccu 94.4333\tValid: loss 0.4561\taccu 88.9576\n",
      "Epoch 18: val_acc did not improve\n",
      "18 0.001\n",
      "Epoch-19   20 batches\tloss 0.3657 (0.4004)\taccu 95.312 (93.984)\n",
      "Epoch-19   40 batches\tloss 0.3512 (0.3965)\taccu 98.438 (94.375)\n",
      "Epoch-19   60 batches\tloss 0.3855 (0.3949)\taccu 92.188 (94.453)\n",
      "Epoch-19   80 batches\tloss 0.3801 (0.3957)\taccu 96.875 (94.395)\n",
      "Epoch-19  100 batches\tloss 0.3572 (0.3933)\taccu 98.438 (94.578)\n",
      "Epoch-19  120 batches\tloss 0.4028 (0.3930)\taccu 96.875 (94.531)\n",
      "Epoch-19  140 batches\tloss 0.4054 (0.3925)\taccu 92.188 (94.598)\n",
      "Epoch-19  160 batches\tloss 0.3812 (0.3928)\taccu 96.875 (94.531)\n",
      "Epoch-19  180 batches\tloss 0.4498 (0.3927)\taccu 90.625 (94.523)\n",
      "Epoch-19  200 batches\tloss 0.4046 (0.3930)\taccu 93.750 (94.445)\n",
      "Epoch-19  220 batches\tloss 0.3703 (0.3936)\taccu 98.438 (94.389)\n",
      "Epoch-19  240 batches\tloss 0.3965 (0.3927)\taccu 95.312 (94.460)\n",
      "Epoch-19  260 batches\tloss 0.4070 (0.3929)\taccu 93.750 (94.435)\n",
      "Epoch-19  280 batches\tloss 0.3663 (0.3927)\taccu 95.312 (94.442)\n",
      "Epoch-19  300 batches\tloss 0.3827 (0.3926)\taccu 93.750 (94.453)\n",
      "Epoch-19  320 batches\tloss 0.4109 (0.3921)\taccu 90.625 (94.482)\n",
      "Epoch-19  340 batches\tloss 0.3638 (0.3921)\taccu 96.875 (94.462)\n",
      "Epoch-19  360 batches\tloss 0.3798 (0.3922)\taccu 96.875 (94.431)\n",
      "Epoch-19  380 batches\tloss 0.3814 (0.3928)\taccu 95.312 (94.383)\n",
      "Epoch-19  400 batches\tloss 0.3756 (0.3919)\taccu 96.875 (94.512)\n",
      "Epoch-19  420 batches\tloss 0.3641 (0.3921)\taccu 96.875 (94.501)\n",
      "Epoch-19  440 batches\tloss 0.4058 (0.3926)\taccu 90.625 (94.428)\n",
      "Epoch-19  460 batches\tloss 0.4337 (0.3926)\taccu 93.750 (94.426)\n",
      "Epoch-19  480 batches\tloss 0.3579 (0.3928)\taccu 96.875 (94.411)\n",
      "Epoch-19  500 batches\tloss 0.3676 (0.3928)\taccu 95.312 (94.375)\n",
      "Epoch-19  520 batches\tloss 0.3653 (0.3930)\taccu 96.875 (94.345)\n",
      "Epoch-19  540 batches\tloss 0.3734 (0.3932)\taccu 95.312 (94.343)\n",
      "Epoch-19  560 batches\tloss 0.4037 (0.3931)\taccu 92.188 (94.364)\n",
      "Epoch-19  580 batches\tloss 0.3967 (0.3930)\taccu 90.625 (94.362)\n",
      "Epoch-19  600 batches\tloss 0.3801 (0.3931)\taccu 95.312 (94.344)\n",
      "Epoch-19  620 batches\tloss 0.4736 (0.3933)\taccu 87.500 (94.332)\n",
      "Epoch-19  640 batches\tloss 0.4528 (0.3934)\taccu 85.938 (94.297)\n",
      "Epoch-19  660 batches\tloss 0.4068 (0.3934)\taccu 92.188 (94.311)\n",
      "Epoch-19  680 batches\tloss 0.4172 (0.3935)\taccu 90.625 (94.297)\n",
      "Epoch-19  700 batches\tloss 0.3495 (0.3934)\taccu 98.438 (94.310)\n",
      "Epoch-19  720 batches\tloss 0.3825 (0.3933)\taccu 96.875 (94.303)\n",
      "Epoch-19  740 batches\tloss 0.3605 (0.3935)\taccu 96.875 (94.295)\n",
      "Epoch-19  760 batches\tloss 0.4064 (0.3933)\taccu 90.625 (94.295)\n",
      "Epoch-19  780 batches\tloss 0.3445 (0.3929)\taccu 100.000 (94.343)\n",
      "Epoch-19  800 batches\tloss 0.4179 (0.3926)\taccu 90.625 (94.371)\n",
      "Epoch-19  820 batches\tloss 0.3822 (0.3927)\taccu 95.312 (94.364)\n",
      "Epoch-19  840 batches\tloss 0.3603 (0.3926)\taccu 98.438 (94.373)\n",
      "Epoch-19  860 batches\tloss 0.4463 (0.3926)\taccu 90.625 (94.370)\n",
      "Epoch-19  205.7s\tTrain: loss 0.3926\taccu 94.3757\tValid: loss 0.4389\taccu 90.5586\n",
      "Epoch 19: val_acc improved from 90.4747 to 90.5586, saving model to ./results/NTU/SGN\\0_best.pth\n",
      "19 0.001\n",
      "Epoch-20   20 batches\tloss 0.4209 (0.3956)\taccu 92.188 (93.906)\n",
      "Epoch-20   40 batches\tloss 0.3644 (0.3907)\taccu 96.875 (94.453)\n",
      "Epoch-20   60 batches\tloss 0.4061 (0.3897)\taccu 93.750 (94.635)\n",
      "Epoch-20   80 batches\tloss 0.3403 (0.3881)\taccu 98.438 (94.824)\n",
      "Epoch-20  100 batches\tloss 0.3778 (0.3884)\taccu 95.312 (94.844)\n",
      "Epoch-20  120 batches\tloss 0.4073 (0.3873)\taccu 93.750 (94.987)\n",
      "Epoch-20  140 batches\tloss 0.3385 (0.3867)\taccu 100.000 (95.112)\n",
      "Epoch-20  160 batches\tloss 0.4044 (0.3866)\taccu 93.750 (95.000)\n",
      "Epoch-20  180 batches\tloss 0.4260 (0.3870)\taccu 92.188 (94.957)\n",
      "Epoch-20  200 batches\tloss 0.3675 (0.3861)\taccu 96.875 (95.031)\n",
      "Epoch-20  220 batches\tloss 0.3594 (0.3860)\taccu 96.875 (95.036)\n",
      "Epoch-20  240 batches\tloss 0.3883 (0.3863)\taccu 95.312 (95.033)\n",
      "Epoch-20  260 batches\tloss 0.3544 (0.3866)\taccu 98.438 (94.946)\n",
      "Epoch-20  280 batches\tloss 0.4032 (0.3876)\taccu 92.188 (94.855)\n",
      "Epoch-20  300 batches\tloss 0.4308 (0.3878)\taccu 92.188 (94.818)\n",
      "Epoch-20  320 batches\tloss 0.3684 (0.3881)\taccu 95.312 (94.795)\n",
      "Epoch-20  340 batches\tloss 0.4409 (0.3887)\taccu 87.500 (94.743)\n",
      "Epoch-20  360 batches\tloss 0.4011 (0.3885)\taccu 93.750 (94.727)\n",
      "Epoch-20  380 batches\tloss 0.3689 (0.3881)\taccu 95.312 (94.766)\n",
      "Epoch-20  400 batches\tloss 0.3687 (0.3883)\taccu 95.312 (94.730)\n",
      "Epoch-20  420 batches\tloss 0.3568 (0.3879)\taccu 98.438 (94.795)\n",
      "Epoch-20  440 batches\tloss 0.3581 (0.3882)\taccu 98.438 (94.759)\n",
      "Epoch-20  460 batches\tloss 0.4411 (0.3886)\taccu 89.062 (94.725)\n",
      "Epoch-20  480 batches\tloss 0.4380 (0.3885)\taccu 90.625 (94.749)\n",
      "Epoch-20  500 batches\tloss 0.4379 (0.3885)\taccu 92.188 (94.738)\n",
      "Epoch-20  520 batches\tloss 0.4277 (0.3887)\taccu 90.625 (94.727)\n",
      "Epoch-20  540 batches\tloss 0.3609 (0.3885)\taccu 96.875 (94.748)\n",
      "Epoch-20  560 batches\tloss 0.3895 (0.3889)\taccu 93.750 (94.715)\n",
      "Epoch-20  580 batches\tloss 0.4067 (0.3893)\taccu 93.750 (94.693)\n",
      "Epoch-20  600 batches\tloss 0.3623 (0.3892)\taccu 98.438 (94.721)\n",
      "Epoch-20  620 batches\tloss 0.3708 (0.3892)\taccu 98.438 (94.733)\n",
      "Epoch-20  640 batches\tloss 0.4053 (0.3890)\taccu 93.750 (94.736)\n",
      "Epoch-20  660 batches\tloss 0.3662 (0.3895)\taccu 96.875 (94.706)\n",
      "Epoch-20  680 batches\tloss 0.3758 (0.3897)\taccu 95.312 (94.692)\n",
      "Epoch-20  700 batches\tloss 0.4150 (0.3894)\taccu 92.188 (94.712)\n",
      "Epoch-20  720 batches\tloss 0.3831 (0.3893)\taccu 93.750 (94.716)\n",
      "Epoch-20  740 batches\tloss 0.3804 (0.3894)\taccu 95.312 (94.711)\n",
      "Epoch-20  760 batches\tloss 0.3814 (0.3896)\taccu 96.875 (94.692)\n",
      "Epoch-20  780 batches\tloss 0.3831 (0.3896)\taccu 95.312 (94.690)\n",
      "Epoch-20  800 batches\tloss 0.3900 (0.3895)\taccu 96.875 (94.693)\n",
      "Epoch-20  820 batches\tloss 0.3779 (0.3896)\taccu 93.750 (94.678)\n",
      "Epoch-20  840 batches\tloss 0.3839 (0.3898)\taccu 96.875 (94.647)\n",
      "Epoch-20  860 batches\tloss 0.4711 (0.3899)\taccu 89.062 (94.628)\n",
      "Epoch-20  210.6s\tTrain: loss 0.3899\taccu 94.6310\tValid: loss 0.4815\taccu 86.9757\n",
      "Epoch 20: val_acc did not improve\n",
      "20 0.001\n",
      "Epoch-21   20 batches\tloss 0.3684 (0.3885)\taccu 96.875 (94.922)\n",
      "Epoch-21   40 batches\tloss 0.3609 (0.3878)\taccu 98.438 (95.078)\n",
      "Epoch-21   60 batches\tloss 0.3767 (0.3905)\taccu 95.312 (94.609)\n",
      "Epoch-21   80 batches\tloss 0.3791 (0.3894)\taccu 96.875 (94.727)\n",
      "Epoch-21  100 batches\tloss 0.4043 (0.3911)\taccu 92.188 (94.609)\n",
      "Epoch-21  120 batches\tloss 0.4155 (0.3928)\taccu 92.188 (94.427)\n",
      "Epoch-21  140 batches\tloss 0.3960 (0.3931)\taccu 95.312 (94.420)\n",
      "Epoch-21  160 batches\tloss 0.4000 (0.3919)\taccu 93.750 (94.570)\n",
      "Epoch-21  180 batches\tloss 0.4081 (0.3915)\taccu 95.312 (94.531)\n",
      "Epoch-21  200 batches\tloss 0.3962 (0.3901)\taccu 92.188 (94.688)\n",
      "Epoch-21  220 batches\tloss 0.3584 (0.3892)\taccu 96.875 (94.766)\n",
      "Epoch-21  240 batches\tloss 0.3708 (0.3890)\taccu 95.312 (94.733)\n",
      "Epoch-21  260 batches\tloss 0.3508 (0.3889)\taccu 98.438 (94.772)\n",
      "Epoch-21  280 batches\tloss 0.4156 (0.3887)\taccu 90.625 (94.754)\n",
      "Epoch-21  300 batches\tloss 0.4386 (0.3887)\taccu 87.500 (94.755)\n",
      "Epoch-21  320 batches\tloss 0.4085 (0.3883)\taccu 93.750 (94.775)\n",
      "Epoch-21  340 batches\tloss 0.4254 (0.3893)\taccu 89.062 (94.665)\n",
      "Epoch-21  360 batches\tloss 0.4189 (0.3892)\taccu 92.188 (94.679)\n",
      "Epoch-21  380 batches\tloss 0.4108 (0.3891)\taccu 92.188 (94.671)\n",
      "Epoch-21  400 batches\tloss 0.3568 (0.3893)\taccu 98.438 (94.676)\n",
      "Epoch-21  420 batches\tloss 0.3965 (0.3892)\taccu 95.312 (94.669)\n",
      "Epoch-21  440 batches\tloss 0.4365 (0.3892)\taccu 90.625 (94.641)\n",
      "Epoch-21  460 batches\tloss 0.4810 (0.3892)\taccu 85.938 (94.637)\n",
      "Epoch-21  480 batches\tloss 0.3625 (0.3892)\taccu 96.875 (94.645)\n",
      "Epoch-21  500 batches\tloss 0.3515 (0.3887)\taccu 98.438 (94.709)\n",
      "Epoch-21  520 batches\tloss 0.3878 (0.3886)\taccu 96.875 (94.742)\n",
      "Epoch-21  540 batches\tloss 0.3878 (0.3883)\taccu 95.312 (94.777)\n",
      "Epoch-21  560 batches\tloss 0.4057 (0.3885)\taccu 93.750 (94.774)\n",
      "Epoch-21  580 batches\tloss 0.4515 (0.3886)\taccu 89.062 (94.774)\n",
      "Epoch-21  600 batches\tloss 0.3929 (0.3887)\taccu 93.750 (94.753)\n",
      "Epoch-21  620 batches\tloss 0.3385 (0.3884)\taccu 100.000 (94.776)\n",
      "Epoch-21  640 batches\tloss 0.3808 (0.3889)\taccu 95.312 (94.717)\n",
      "Epoch-21  660 batches\tloss 0.3891 (0.3890)\taccu 95.312 (94.709)\n",
      "Epoch-21  680 batches\tloss 0.3712 (0.3890)\taccu 96.875 (94.708)\n",
      "Epoch-21  700 batches\tloss 0.4323 (0.3892)\taccu 89.062 (94.701)\n",
      "Epoch-21  720 batches\tloss 0.4594 (0.3896)\taccu 87.500 (94.672)\n",
      "Epoch-21  740 batches\tloss 0.3702 (0.3896)\taccu 95.312 (94.671)\n",
      "Epoch-21  760 batches\tloss 0.3774 (0.3894)\taccu 96.875 (94.692)\n",
      "Epoch-21  780 batches\tloss 0.3837 (0.3898)\taccu 98.438 (94.677)\n",
      "Epoch-21  800 batches\tloss 0.4450 (0.3902)\taccu 89.062 (94.652)\n",
      "Epoch-21  820 batches\tloss 0.3716 (0.3900)\taccu 96.875 (94.667)\n",
      "Epoch-21  840 batches\tloss 0.3710 (0.3901)\taccu 96.875 (94.652)\n",
      "Epoch-21  860 batches\tloss 0.3503 (0.3897)\taccu 96.875 (94.689)\n",
      "Epoch-21  201.5s\tTrain: loss 0.3897\taccu 94.6868\tValid: loss 0.4446\taccu 90.1846\n",
      "Epoch 21: val_acc did not improve\n",
      "21 0.001\n",
      "Epoch-22   20 batches\tloss 0.3971 (0.3856)\taccu 92.188 (95.234)\n",
      "Epoch-22   40 batches\tloss 0.3575 (0.3849)\taccu 98.438 (95.195)\n",
      "Epoch-22   60 batches\tloss 0.3640 (0.3845)\taccu 98.438 (95.260)\n",
      "Epoch-22   80 batches\tloss 0.3383 (0.3844)\taccu 100.000 (95.195)\n",
      "Epoch-22  100 batches\tloss 0.3884 (0.3841)\taccu 96.875 (95.203)\n",
      "Epoch-22  120 batches\tloss 0.3870 (0.3837)\taccu 92.188 (95.130)\n",
      "Epoch-22  140 batches\tloss 0.3912 (0.3845)\taccu 95.312 (95.078)\n",
      "Epoch-22  160 batches\tloss 0.4193 (0.3866)\taccu 92.188 (94.873)\n",
      "Epoch-22  180 batches\tloss 0.3894 (0.3866)\taccu 95.312 (94.861)\n",
      "Epoch-22  200 batches\tloss 0.3999 (0.3884)\taccu 93.750 (94.688)\n",
      "Epoch-22  220 batches\tloss 0.4063 (0.3886)\taccu 93.750 (94.737)\n",
      "Epoch-22  240 batches\tloss 0.3546 (0.3880)\taccu 98.438 (94.805)\n",
      "Epoch-22  260 batches\tloss 0.4166 (0.3875)\taccu 89.062 (94.856)\n",
      "Epoch-22  280 batches\tloss 0.3677 (0.3866)\taccu 96.875 (94.900)\n",
      "Epoch-22  300 batches\tloss 0.3814 (0.3863)\taccu 95.312 (94.948)\n",
      "Epoch-22  320 batches\tloss 0.3909 (0.3866)\taccu 96.875 (94.922)\n",
      "Epoch-22  340 batches\tloss 0.3794 (0.3867)\taccu 95.312 (94.926)\n",
      "Epoch-22  360 batches\tloss 0.3657 (0.3870)\taccu 96.875 (94.887)\n",
      "Epoch-22  380 batches\tloss 0.3820 (0.3867)\taccu 95.312 (94.938)\n",
      "Epoch-22  400 batches\tloss 0.3813 (0.3875)\taccu 96.875 (94.855)\n",
      "Epoch-22  420 batches\tloss 0.3680 (0.3877)\taccu 95.312 (94.829)\n",
      "Epoch-22  440 batches\tloss 0.3687 (0.3880)\taccu 95.312 (94.805)\n",
      "Epoch-22  460 batches\tloss 0.4242 (0.3886)\taccu 90.625 (94.759)\n",
      "Epoch-22  480 batches\tloss 0.3590 (0.3882)\taccu 98.438 (94.785)\n",
      "Epoch-22  500 batches\tloss 0.4147 (0.3882)\taccu 95.312 (94.797)\n",
      "Epoch-22  520 batches\tloss 0.3786 (0.3882)\taccu 98.438 (94.793)\n",
      "Epoch-22  540 batches\tloss 0.3728 (0.3884)\taccu 96.875 (94.792)\n",
      "Epoch-22  560 batches\tloss 0.3649 (0.3883)\taccu 95.312 (94.810)\n",
      "Epoch-22  580 batches\tloss 0.3853 (0.3877)\taccu 92.188 (94.852)\n",
      "Epoch-22  600 batches\tloss 0.3555 (0.3879)\taccu 98.438 (94.849)\n",
      "Epoch-22  620 batches\tloss 0.3777 (0.3882)\taccu 95.312 (94.814)\n",
      "Epoch-22  640 batches\tloss 0.3809 (0.3882)\taccu 95.312 (94.829)\n",
      "Epoch-22  660 batches\tloss 0.4074 (0.3882)\taccu 92.188 (94.825)\n",
      "Epoch-22  680 batches\tloss 0.3931 (0.3883)\taccu 95.312 (94.828)\n",
      "Epoch-22  700 batches\tloss 0.3646 (0.3885)\taccu 96.875 (94.819)\n",
      "Epoch-22  720 batches\tloss 0.3559 (0.3883)\taccu 98.438 (94.842)\n",
      "Epoch-22  740 batches\tloss 0.3910 (0.3880)\taccu 93.750 (94.873)\n",
      "Epoch-22  760 batches\tloss 0.3812 (0.3878)\taccu 95.312 (94.893)\n",
      "Epoch-22  780 batches\tloss 0.4032 (0.3877)\taccu 93.750 (94.902)\n",
      "Epoch-22  800 batches\tloss 0.4216 (0.3878)\taccu 87.500 (94.898)\n",
      "Epoch-22  820 batches\tloss 0.3459 (0.3876)\taccu 98.438 (94.918)\n",
      "Epoch-22  840 batches\tloss 0.3939 (0.3877)\taccu 96.875 (94.926)\n",
      "Epoch-22  860 batches\tloss 0.3467 (0.3877)\taccu 98.438 (94.907)\n",
      "Epoch-22  205.7s\tTrain: loss 0.3877\taccu 94.9187\tValid: loss 0.4479\taccu 89.7966\n",
      "Epoch 22: val_acc did not improve\n",
      "22 0.001\n",
      "Epoch-23   20 batches\tloss 0.3653 (0.3787)\taccu 95.312 (95.234)\n",
      "Epoch-23   40 batches\tloss 0.3885 (0.3778)\taccu 93.750 (95.547)\n",
      "Epoch-23   60 batches\tloss 0.3684 (0.3757)\taccu 95.312 (95.885)\n",
      "Epoch-23   80 batches\tloss 0.4162 (0.3789)\taccu 90.625 (95.547)\n",
      "Epoch-23  100 batches\tloss 0.4085 (0.3813)\taccu 89.062 (95.312)\n",
      "Epoch-23  120 batches\tloss 0.3667 (0.3818)\taccu 96.875 (95.208)\n",
      "Epoch-23  140 batches\tloss 0.3976 (0.3835)\taccu 95.312 (95.123)\n",
      "Epoch-23  160 batches\tloss 0.3806 (0.3838)\taccu 95.312 (95.117)\n",
      "Epoch-23  180 batches\tloss 0.4079 (0.3839)\taccu 92.188 (95.095)\n",
      "Epoch-23  200 batches\tloss 0.4050 (0.3839)\taccu 90.625 (95.125)\n",
      "Epoch-23  220 batches\tloss 0.3696 (0.3827)\taccu 96.875 (95.227)\n",
      "Epoch-23  240 batches\tloss 0.4037 (0.3826)\taccu 95.312 (95.299)\n",
      "Epoch-23  260 batches\tloss 0.3884 (0.3826)\taccu 96.875 (95.300)\n",
      "Epoch-23  280 batches\tloss 0.4111 (0.3833)\taccu 93.750 (95.257)\n",
      "Epoch-23  300 batches\tloss 0.3957 (0.3837)\taccu 92.188 (95.219)\n",
      "Epoch-23  320 batches\tloss 0.3593 (0.3845)\taccu 96.875 (95.161)\n",
      "Epoch-23  340 batches\tloss 0.3620 (0.3852)\taccu 98.438 (95.101)\n",
      "Epoch-23  360 batches\tloss 0.3526 (0.3854)\taccu 98.438 (95.074)\n",
      "Epoch-23  380 batches\tloss 0.3825 (0.3850)\taccu 96.875 (95.132)\n",
      "Epoch-23  400 batches\tloss 0.3971 (0.3848)\taccu 95.312 (95.129)\n",
      "Epoch-23  420 batches\tloss 0.4310 (0.3849)\taccu 92.188 (95.134)\n",
      "Epoch-23  440 batches\tloss 0.3808 (0.3852)\taccu 98.438 (95.099)\n",
      "Epoch-23  460 batches\tloss 0.3998 (0.3853)\taccu 96.875 (95.061)\n",
      "Epoch-23  480 batches\tloss 0.3792 (0.3857)\taccu 95.312 (95.042)\n",
      "Epoch-23  500 batches\tloss 0.3733 (0.3858)\taccu 95.312 (95.016)\n",
      "Epoch-23  520 batches\tloss 0.3688 (0.3856)\taccu 96.875 (95.042)\n",
      "Epoch-23  540 batches\tloss 0.3736 (0.3855)\taccu 98.438 (95.064)\n",
      "Epoch-23  560 batches\tloss 0.3990 (0.3857)\taccu 92.188 (95.036)\n",
      "Epoch-23  580 batches\tloss 0.4138 (0.3860)\taccu 93.750 (95.005)\n",
      "Epoch-23  600 batches\tloss 0.3607 (0.3862)\taccu 98.438 (94.987)\n",
      "Epoch-23  620 batches\tloss 0.4215 (0.3864)\taccu 89.062 (94.965)\n",
      "Epoch-23  640 batches\tloss 0.3721 (0.3866)\taccu 96.875 (94.963)\n",
      "Epoch-23  660 batches\tloss 0.3575 (0.3865)\taccu 96.875 (94.972)\n",
      "Epoch-23  680 batches\tloss 0.3701 (0.3865)\taccu 95.312 (94.968)\n",
      "Epoch-23  700 batches\tloss 0.3525 (0.3864)\taccu 96.875 (94.989)\n",
      "Epoch-23  720 batches\tloss 0.3960 (0.3864)\taccu 93.750 (94.998)\n",
      "Epoch-23  740 batches\tloss 0.3673 (0.3864)\taccu 98.438 (95.006)\n",
      "Epoch-23  760 batches\tloss 0.3612 (0.3863)\taccu 98.438 (95.010)\n",
      "Epoch-23  780 batches\tloss 0.4308 (0.3871)\taccu 90.625 (94.934)\n",
      "Epoch-23  800 batches\tloss 0.3683 (0.3871)\taccu 96.875 (94.936)\n",
      "Epoch-23  820 batches\tloss 0.3683 (0.3868)\taccu 98.438 (94.962)\n",
      "Epoch-23  840 batches\tloss 0.4212 (0.3870)\taccu 90.625 (94.935)\n",
      "Epoch-23  860 batches\tloss 0.3549 (0.3872)\taccu 95.312 (94.906)\n",
      "Epoch-23  207.3s\tTrain: loss 0.3872\taccu 94.8989\tValid: loss 0.4373\taccu 90.7334\n",
      "Epoch 23: val_acc improved from 90.5586 to 90.7334, saving model to ./results/NTU/SGN\\0_best.pth\n",
      "23 0.001\n",
      "Epoch-24   20 batches\tloss 0.4429 (0.3934)\taccu 89.062 (94.453)\n",
      "Epoch-24   40 batches\tloss 0.4537 (0.3879)\taccu 90.625 (94.922)\n",
      "Epoch-24   60 batches\tloss 0.3596 (0.3833)\taccu 96.875 (95.417)\n",
      "Epoch-24   80 batches\tloss 0.3974 (0.3829)\taccu 92.188 (95.410)\n",
      "Epoch-24  100 batches\tloss 0.3865 (0.3839)\taccu 96.875 (95.312)\n",
      "Epoch-24  120 batches\tloss 0.3631 (0.3854)\taccu 98.438 (95.143)\n",
      "Epoch-24  140 batches\tloss 0.4148 (0.3868)\taccu 92.188 (95.000)\n",
      "Epoch-24  160 batches\tloss 0.3833 (0.3867)\taccu 95.312 (95.010)\n",
      "Epoch-24  180 batches\tloss 0.4179 (0.3869)\taccu 92.188 (94.974)\n",
      "Epoch-24  200 batches\tloss 0.4020 (0.3862)\taccu 92.188 (95.016)\n",
      "Epoch-24  220 batches\tloss 0.3791 (0.3864)\taccu 95.312 (95.014)\n",
      "Epoch-24  240 batches\tloss 0.4216 (0.3861)\taccu 89.062 (95.059)\n",
      "Epoch-24  260 batches\tloss 0.3961 (0.3865)\taccu 95.312 (94.994)\n",
      "Epoch-24  280 batches\tloss 0.3981 (0.3867)\taccu 93.750 (95.000)\n",
      "Epoch-24  300 batches\tloss 0.3503 (0.3856)\taccu 98.438 (95.083)\n",
      "Epoch-24  320 batches\tloss 0.3764 (0.3862)\taccu 95.312 (95.034)\n",
      "Epoch-24  340 batches\tloss 0.4070 (0.3855)\taccu 93.750 (95.119)\n",
      "Epoch-24  360 batches\tloss 0.3548 (0.3852)\taccu 96.875 (95.130)\n",
      "Epoch-24  380 batches\tloss 0.3697 (0.3853)\taccu 95.312 (95.086)\n",
      "Epoch-24  400 batches\tloss 0.3362 (0.3845)\taccu 100.000 (95.160)\n",
      "Epoch-24  420 batches\tloss 0.3772 (0.3848)\taccu 98.438 (95.145)\n",
      "Epoch-24  440 batches\tloss 0.3850 (0.3849)\taccu 96.875 (95.128)\n",
      "Epoch-24  460 batches\tloss 0.4250 (0.3853)\taccu 90.625 (95.078)\n",
      "Epoch-24  480 batches\tloss 0.4220 (0.3854)\taccu 90.625 (95.052)\n",
      "Epoch-24  500 batches\tloss 0.4141 (0.3856)\taccu 92.188 (95.038)\n",
      "Epoch-24  520 batches\tloss 0.3676 (0.3859)\taccu 98.438 (94.997)\n",
      "Epoch-24  540 batches\tloss 0.3676 (0.3861)\taccu 96.875 (94.974)\n",
      "Epoch-24  560 batches\tloss 0.3982 (0.3864)\taccu 92.188 (94.953)\n",
      "Epoch-24  580 batches\tloss 0.3922 (0.3868)\taccu 95.312 (94.900)\n",
      "Epoch-24  600 batches\tloss 0.3471 (0.3867)\taccu 100.000 (94.922)\n",
      "Epoch-24  620 batches\tloss 0.3906 (0.3869)\taccu 95.312 (94.889)\n",
      "Epoch-24  640 batches\tloss 0.4252 (0.3871)\taccu 93.750 (94.875)\n",
      "Epoch-24  660 batches\tloss 0.3918 (0.3871)\taccu 95.312 (94.875)\n",
      "Epoch-24  680 batches\tloss 0.4393 (0.3871)\taccu 90.625 (94.869)\n",
      "Epoch-24  700 batches\tloss 0.4021 (0.3874)\taccu 93.750 (94.859)\n",
      "Epoch-24  720 batches\tloss 0.3817 (0.3874)\taccu 92.188 (94.857)\n",
      "Epoch-24  740 batches\tloss 0.4866 (0.3877)\taccu 85.938 (94.835)\n",
      "Epoch-24  760 batches\tloss 0.4455 (0.3882)\taccu 89.062 (94.790)\n",
      "Epoch-24  780 batches\tloss 0.3846 (0.3880)\taccu 95.312 (94.808)\n",
      "Epoch-24  800 batches\tloss 0.3624 (0.3880)\taccu 96.875 (94.811)\n",
      "Epoch-24  820 batches\tloss 0.3663 (0.3876)\taccu 98.438 (94.849)\n",
      "Epoch-24  840 batches\tloss 0.3554 (0.3875)\taccu 96.875 (94.866)\n",
      "Epoch-24  860 batches\tloss 0.4079 (0.3875)\taccu 89.062 (94.867)\n",
      "Epoch-24  196.8s\tTrain: loss 0.3875\taccu 94.8630\tValid: loss 0.4590\taccu 88.8982\n",
      "Epoch 24: val_acc did not improve\n",
      "24 0.001\n",
      "Epoch-25   20 batches\tloss 0.3645 (0.3852)\taccu 98.438 (95.859)\n",
      "Epoch-25   40 batches\tloss 0.4148 (0.3850)\taccu 92.188 (95.547)\n",
      "Epoch-25   60 batches\tloss 0.3971 (0.3840)\taccu 95.312 (95.391)\n",
      "Epoch-25   80 batches\tloss 0.4024 (0.3855)\taccu 93.750 (95.254)\n",
      "Epoch-25  100 batches\tloss 0.3623 (0.3835)\taccu 96.875 (95.391)\n",
      "Epoch-25  120 batches\tloss 0.3835 (0.3824)\taccu 95.312 (95.391)\n",
      "Epoch-25  140 batches\tloss 0.3798 (0.3834)\taccu 92.188 (95.312)\n",
      "Epoch-25  160 batches\tloss 0.3763 (0.3831)\taccu 96.875 (95.400)\n",
      "Epoch-25  180 batches\tloss 0.3998 (0.3827)\taccu 95.312 (95.495)\n",
      "Epoch-25  200 batches\tloss 0.4590 (0.3832)\taccu 87.500 (95.453)\n",
      "Epoch-25  220 batches\tloss 0.3990 (0.3837)\taccu 93.750 (95.391)\n",
      "Epoch-25  240 batches\tloss 0.3600 (0.3830)\taccu 96.875 (95.436)\n",
      "Epoch-25  260 batches\tloss 0.3983 (0.3834)\taccu 93.750 (95.367)\n",
      "Epoch-25  280 batches\tloss 0.3676 (0.3835)\taccu 98.438 (95.352)\n",
      "Epoch-25  300 batches\tloss 0.4607 (0.3833)\taccu 90.625 (95.375)\n",
      "Epoch-25  320 batches\tloss 0.3765 (0.3835)\taccu 93.750 (95.361)\n",
      "Epoch-25  340 batches\tloss 0.3756 (0.3835)\taccu 93.750 (95.345)\n",
      "Epoch-25  360 batches\tloss 0.3784 (0.3837)\taccu 95.312 (95.304)\n",
      "Epoch-25  380 batches\tloss 0.3748 (0.3832)\taccu 96.875 (95.333)\n",
      "Epoch-25  400 batches\tloss 0.3831 (0.3833)\taccu 95.312 (95.344)\n",
      "Epoch-25  420 batches\tloss 0.3888 (0.3840)\taccu 93.750 (95.279)\n",
      "Epoch-25  440 batches\tloss 0.3764 (0.3838)\taccu 95.312 (95.291)\n",
      "Epoch-25  460 batches\tloss 0.3862 (0.3839)\taccu 95.312 (95.296)\n",
      "Epoch-25  480 batches\tloss 0.3787 (0.3841)\taccu 96.875 (95.273)\n",
      "Epoch-25  500 batches\tloss 0.3566 (0.3847)\taccu 98.438 (95.209)\n",
      "Epoch-25  520 batches\tloss 0.4233 (0.3851)\taccu 93.750 (95.162)\n",
      "Epoch-25  540 batches\tloss 0.4418 (0.3852)\taccu 90.625 (95.153)\n",
      "Epoch-25  560 batches\tloss 0.3640 (0.3857)\taccu 96.875 (95.092)\n",
      "Epoch-25  580 batches\tloss 0.3808 (0.3856)\taccu 95.312 (95.108)\n",
      "Epoch-25  600 batches\tloss 0.3790 (0.3854)\taccu 96.875 (95.120)\n",
      "Epoch-25  620 batches\tloss 0.3746 (0.3854)\taccu 93.750 (95.123)\n",
      "Epoch-25  640 batches\tloss 0.3794 (0.3853)\taccu 95.312 (95.129)\n",
      "Epoch-25  660 batches\tloss 0.4456 (0.3856)\taccu 89.062 (95.116)\n",
      "Epoch-25  680 batches\tloss 0.3633 (0.3857)\taccu 98.438 (95.115)\n",
      "Epoch-25  700 batches\tloss 0.3992 (0.3858)\taccu 92.188 (95.092)\n",
      "Epoch-25  720 batches\tloss 0.3801 (0.3856)\taccu 95.312 (95.117)\n",
      "Epoch-25  740 batches\tloss 0.3900 (0.3854)\taccu 92.188 (95.129)\n",
      "Epoch-25  760 batches\tloss 0.3570 (0.3855)\taccu 98.438 (95.125)\n",
      "Epoch-25  780 batches\tloss 0.3743 (0.3856)\taccu 96.875 (95.122)\n",
      "Epoch-25  800 batches\tloss 0.3624 (0.3856)\taccu 93.750 (95.127)\n",
      "Epoch-25  820 batches\tloss 0.4012 (0.3854)\taccu 95.312 (95.135)\n",
      "Epoch-25  840 batches\tloss 0.3827 (0.3853)\taccu 95.312 (95.143)\n",
      "Epoch-25  860 batches\tloss 0.4406 (0.3855)\taccu 89.062 (95.114)\n",
      "Epoch-25  195.2s\tTrain: loss 0.3854\taccu 95.1255\tValid: loss 0.4445\taccu 90.0867\n",
      "Epoch 25: val_acc did not improve\n",
      "25 0.001\n",
      "Epoch-26   20 batches\tloss 0.3697 (0.3794)\taccu 95.312 (95.469)\n",
      "Epoch-26   40 batches\tloss 0.3775 (0.3851)\taccu 93.750 (95.117)\n",
      "Epoch-26   60 batches\tloss 0.3565 (0.3826)\taccu 96.875 (95.260)\n",
      "Epoch-26   80 batches\tloss 0.3791 (0.3817)\taccu 96.875 (95.410)\n",
      "Epoch-26  100 batches\tloss 0.4064 (0.3822)\taccu 90.625 (95.359)\n",
      "Epoch-26  120 batches\tloss 0.3457 (0.3808)\taccu 98.438 (95.482)\n",
      "Epoch-26  140 batches\tloss 0.3855 (0.3823)\taccu 95.312 (95.279)\n",
      "Epoch-26  160 batches\tloss 0.3954 (0.3826)\taccu 95.312 (95.215)\n",
      "Epoch-26  180 batches\tloss 0.3806 (0.3820)\taccu 93.750 (95.260)\n",
      "Epoch-26  200 batches\tloss 0.3981 (0.3817)\taccu 92.188 (95.328)\n",
      "Epoch-26  220 batches\tloss 0.3791 (0.3819)\taccu 96.875 (95.320)\n",
      "Epoch-26  240 batches\tloss 0.3834 (0.3827)\taccu 95.312 (95.208)\n",
      "Epoch-26  260 batches\tloss 0.3935 (0.3832)\taccu 93.750 (95.168)\n",
      "Epoch-26  280 batches\tloss 0.3872 (0.3837)\taccu 95.312 (95.123)\n",
      "Epoch-26  300 batches\tloss 0.3872 (0.3843)\taccu 93.750 (95.083)\n",
      "Epoch-26  320 batches\tloss 0.3613 (0.3843)\taccu 98.438 (95.073)\n",
      "Epoch-26  340 batches\tloss 0.3653 (0.3836)\taccu 98.438 (95.147)\n",
      "Epoch-26  360 batches\tloss 0.3800 (0.3836)\taccu 95.312 (95.165)\n",
      "Epoch-26  380 batches\tloss 0.4959 (0.3840)\taccu 89.062 (95.144)\n",
      "Epoch-26  400 batches\tloss 0.3659 (0.3841)\taccu 96.875 (95.145)\n",
      "Epoch-26  420 batches\tloss 0.4040 (0.3842)\taccu 92.188 (95.160)\n",
      "Epoch-26  440 batches\tloss 0.3878 (0.3843)\taccu 96.875 (95.156)\n",
      "Epoch-26  460 batches\tloss 0.3834 (0.3841)\taccu 93.750 (95.170)\n",
      "Epoch-26  480 batches\tloss 0.3626 (0.3838)\taccu 98.438 (95.208)\n",
      "Epoch-26  500 batches\tloss 0.3976 (0.3837)\taccu 92.188 (95.203)\n",
      "Epoch-26  520 batches\tloss 0.3641 (0.3839)\taccu 96.875 (95.177)\n",
      "Epoch-26  540 batches\tloss 0.3967 (0.3837)\taccu 95.312 (95.197)\n",
      "Epoch-26  560 batches\tloss 0.3635 (0.3837)\taccu 98.438 (95.206)\n",
      "Epoch-26  580 batches\tloss 0.4953 (0.3841)\taccu 82.812 (95.170)\n",
      "Epoch-26  600 batches\tloss 0.3716 (0.3839)\taccu 95.312 (95.198)\n",
      "Epoch-26  620 batches\tloss 0.4465 (0.3835)\taccu 90.625 (95.227)\n",
      "Epoch-26  640 batches\tloss 0.4207 (0.3833)\taccu 93.750 (95.259)\n",
      "Epoch-26  660 batches\tloss 0.3742 (0.3830)\taccu 96.875 (95.296)\n",
      "Epoch-26  680 batches\tloss 0.4026 (0.3831)\taccu 95.312 (95.283)\n",
      "Epoch-26  700 batches\tloss 0.3686 (0.3830)\taccu 96.875 (95.301)\n",
      "Epoch-26  720 batches\tloss 0.4023 (0.3832)\taccu 93.750 (95.282)\n",
      "Epoch-26  740 batches\tloss 0.3725 (0.3833)\taccu 95.312 (95.272)\n",
      "Epoch-26  760 batches\tloss 0.3857 (0.3833)\taccu 92.188 (95.278)\n",
      "Epoch-26  780 batches\tloss 0.3751 (0.3835)\taccu 95.312 (95.272)\n",
      "Epoch-26  800 batches\tloss 0.3566 (0.3835)\taccu 96.875 (95.275)\n",
      "Epoch-26  820 batches\tloss 0.3944 (0.3837)\taccu 92.188 (95.248)\n",
      "Epoch-26  840 batches\tloss 0.3826 (0.3838)\taccu 95.312 (95.244)\n",
      "Epoch-26  860 batches\tloss 0.4165 (0.3839)\taccu 92.188 (95.223)\n",
      "Epoch-26  205.3s\tTrain: loss 0.3839\taccu 95.2298\tValid: loss 0.4616\taccu 88.5626\n",
      "Epoch 26: val_acc did not improve\n",
      "26 0.001\n",
      "Epoch-27   20 batches\tloss 0.4095 (0.3885)\taccu 95.312 (95.156)\n",
      "Epoch-27   40 batches\tloss 0.3339 (0.3891)\taccu 100.000 (94.844)\n",
      "Epoch-27   60 batches\tloss 0.4039 (0.3887)\taccu 93.750 (94.661)\n",
      "Epoch-27   80 batches\tloss 0.3655 (0.3910)\taccu 98.438 (94.395)\n",
      "Epoch-27  100 batches\tloss 0.3872 (0.3872)\taccu 92.188 (94.812)\n",
      "Epoch-27  120 batches\tloss 0.3779 (0.3865)\taccu 92.188 (94.896)\n",
      "Epoch-27  140 batches\tloss 0.3605 (0.3851)\taccu 96.875 (95.056)\n",
      "Epoch-27  160 batches\tloss 0.4288 (0.3846)\taccu 89.062 (95.107)\n",
      "Epoch-27  180 batches\tloss 0.3721 (0.3857)\taccu 95.312 (95.052)\n",
      "Epoch-27  200 batches\tloss 0.3783 (0.3847)\taccu 95.312 (95.148)\n",
      "Epoch-27  220 batches\tloss 0.3642 (0.3841)\taccu 96.875 (95.170)\n",
      "Epoch-27  240 batches\tloss 0.3919 (0.3838)\taccu 96.875 (95.195)\n",
      "Epoch-27  260 batches\tloss 0.3958 (0.3843)\taccu 92.188 (95.150)\n",
      "Epoch-27  280 batches\tloss 0.3553 (0.3835)\taccu 96.875 (95.206)\n",
      "Epoch-27  300 batches\tloss 0.3719 (0.3831)\taccu 98.438 (95.250)\n",
      "Epoch-27  320 batches\tloss 0.3512 (0.3826)\taccu 98.438 (95.322)\n",
      "Epoch-27  340 batches\tloss 0.3581 (0.3833)\taccu 98.438 (95.271)\n",
      "Epoch-27  360 batches\tloss 0.3512 (0.3828)\taccu 98.438 (95.308)\n",
      "Epoch-27  380 batches\tloss 0.3595 (0.3824)\taccu 96.875 (95.345)\n",
      "Epoch-27  400 batches\tloss 0.3865 (0.3822)\taccu 92.188 (95.348)\n",
      "Epoch-27  420 batches\tloss 0.3525 (0.3821)\taccu 98.438 (95.357)\n",
      "Epoch-27  440 batches\tloss 0.4056 (0.3823)\taccu 93.750 (95.323)\n",
      "Epoch-27  460 batches\tloss 0.3743 (0.3825)\taccu 96.875 (95.296)\n",
      "Epoch-27  480 batches\tloss 0.4473 (0.3830)\taccu 87.500 (95.251)\n",
      "Epoch-27  500 batches\tloss 0.3996 (0.3835)\taccu 92.188 (95.209)\n",
      "Epoch-27  520 batches\tloss 0.3682 (0.3835)\taccu 96.875 (95.204)\n",
      "Epoch-27  540 batches\tloss 0.3567 (0.3835)\taccu 96.875 (95.197)\n",
      "Epoch-27  560 batches\tloss 0.4004 (0.3832)\taccu 92.188 (95.240)\n",
      "Epoch-27  580 batches\tloss 0.3663 (0.3832)\taccu 95.312 (95.237)\n",
      "Epoch-27  600 batches\tloss 0.3800 (0.3833)\taccu 96.875 (95.224)\n",
      "Epoch-27  620 batches\tloss 0.3785 (0.3832)\taccu 96.875 (95.227)\n",
      "Epoch-27  640 batches\tloss 0.3821 (0.3831)\taccu 93.750 (95.242)\n",
      "Epoch-27  660 batches\tloss 0.3720 (0.3831)\taccu 96.875 (95.251)\n",
      "Epoch-27  680 batches\tloss 0.3707 (0.3832)\taccu 96.875 (95.250)\n",
      "Epoch-27  700 batches\tloss 0.3656 (0.3834)\taccu 96.875 (95.221)\n",
      "Epoch-27  720 batches\tloss 0.3676 (0.3835)\taccu 98.438 (95.208)\n",
      "Epoch-27  740 batches\tloss 0.3473 (0.3836)\taccu 100.000 (95.194)\n",
      "Epoch-27  760 batches\tloss 0.3528 (0.3835)\taccu 98.438 (95.204)\n",
      "Epoch-27  780 batches\tloss 0.3885 (0.3837)\taccu 95.312 (95.180)\n",
      "Epoch-27  800 batches\tloss 0.3977 (0.3837)\taccu 96.875 (95.189)\n",
      "Epoch-27  820 batches\tloss 0.3880 (0.3839)\taccu 93.750 (95.162)\n",
      "Epoch-27  840 batches\tloss 0.3964 (0.3840)\taccu 92.188 (95.136)\n",
      "Epoch-27  860 batches\tloss 0.3791 (0.3839)\taccu 95.312 (95.147)\n",
      "Epoch-27  204.1s\tTrain: loss 0.3837\taccu 95.1740\tValid: loss 0.4550\taccu 89.2792\n",
      "Epoch 27: val_acc did not improve\n",
      "27 0.001\n",
      "Epoch-28   20 batches\tloss 0.3569 (0.3764)\taccu 98.438 (95.781)\n",
      "Epoch-28   40 batches\tloss 0.3718 (0.3817)\taccu 96.875 (95.234)\n",
      "Epoch-28   60 batches\tloss 0.4247 (0.3823)\taccu 93.750 (95.052)\n",
      "Epoch-28   80 batches\tloss 0.4285 (0.3848)\taccu 92.188 (94.980)\n",
      "Epoch-28  100 batches\tloss 0.3672 (0.3840)\taccu 96.875 (95.156)\n",
      "Epoch-28  120 batches\tloss 0.3552 (0.3838)\taccu 98.438 (95.130)\n",
      "Epoch-28  140 batches\tloss 0.3866 (0.3848)\taccu 95.312 (95.011)\n",
      "Epoch-28  160 batches\tloss 0.3965 (0.3845)\taccu 92.188 (95.020)\n",
      "Epoch-28  180 batches\tloss 0.3748 (0.3849)\taccu 96.875 (95.035)\n",
      "Epoch-28  200 batches\tloss 0.3885 (0.3849)\taccu 95.312 (95.023)\n",
      "Epoch-28  220 batches\tloss 0.3655 (0.3844)\taccu 98.438 (95.114)\n",
      "Epoch-28  240 batches\tloss 0.3550 (0.3837)\taccu 98.438 (95.208)\n",
      "Epoch-28  260 batches\tloss 0.3540 (0.3848)\taccu 98.438 (95.126)\n",
      "Epoch-28  280 batches\tloss 0.4182 (0.3849)\taccu 90.625 (95.123)\n",
      "Epoch-28  300 batches\tloss 0.3908 (0.3846)\taccu 95.312 (95.135)\n",
      "Epoch-28  320 batches\tloss 0.3653 (0.3852)\taccu 95.312 (95.098)\n",
      "Epoch-28  340 batches\tloss 0.3732 (0.3854)\taccu 96.875 (95.101)\n",
      "Epoch-28  360 batches\tloss 0.3661 (0.3852)\taccu 95.312 (95.130)\n",
      "Epoch-28  380 batches\tloss 0.4114 (0.3850)\taccu 95.312 (95.164)\n",
      "Epoch-28  400 batches\tloss 0.3822 (0.3856)\taccu 95.312 (95.105)\n",
      "Epoch-28  420 batches\tloss 0.3714 (0.3858)\taccu 96.875 (95.108)\n",
      "Epoch-28  440 batches\tloss 0.3555 (0.3856)\taccu 96.875 (95.114)\n",
      "Epoch-28  460 batches\tloss 0.3712 (0.3859)\taccu 96.875 (95.085)\n",
      "Epoch-28  480 batches\tloss 0.3655 (0.3860)\taccu 96.875 (95.088)\n",
      "Epoch-28  500 batches\tloss 0.3744 (0.3853)\taccu 96.875 (95.147)\n",
      "Epoch-28  520 batches\tloss 0.3582 (0.3852)\taccu 96.875 (95.138)\n",
      "Epoch-28  540 batches\tloss 0.3486 (0.3852)\taccu 98.438 (95.119)\n",
      "Epoch-28  560 batches\tloss 0.3869 (0.3853)\taccu 95.312 (95.106)\n",
      "Epoch-28  580 batches\tloss 0.3647 (0.3855)\taccu 95.312 (95.084)\n",
      "Epoch-28  600 batches\tloss 0.4046 (0.3855)\taccu 93.750 (95.081)\n",
      "Epoch-28  620 batches\tloss 0.3989 (0.3854)\taccu 93.750 (95.096)\n",
      "Epoch-28  640 batches\tloss 0.3671 (0.3852)\taccu 96.875 (95.107)\n",
      "Epoch-28  660 batches\tloss 0.3968 (0.3852)\taccu 95.312 (95.111)\n",
      "Epoch-28  680 batches\tloss 0.3946 (0.3853)\taccu 95.312 (95.097)\n",
      "Epoch-28  700 batches\tloss 0.3604 (0.3852)\taccu 95.312 (95.123)\n",
      "Epoch-28  720 batches\tloss 0.3892 (0.3849)\taccu 92.188 (95.132)\n",
      "Epoch-28  740 batches\tloss 0.3503 (0.3849)\taccu 100.000 (95.139)\n",
      "Epoch-28  760 batches\tloss 0.3645 (0.3851)\taccu 96.875 (95.136)\n",
      "Epoch-28  780 batches\tloss 0.3825 (0.3852)\taccu 98.438 (95.128)\n",
      "Epoch-28  800 batches\tloss 0.3781 (0.3851)\taccu 96.875 (95.137)\n",
      "Epoch-28  820 batches\tloss 0.3601 (0.3850)\taccu 96.875 (95.139)\n",
      "Epoch-28  840 batches\tloss 0.4087 (0.3850)\taccu 92.188 (95.145)\n",
      "Epoch-28  860 batches\tloss 0.3577 (0.3850)\taccu 95.312 (95.140)\n",
      "Epoch-28  221.3s\tTrain: loss 0.3851\taccu 95.1399\tValid: loss 0.4482\taccu 89.8909\n",
      "Epoch 28: val_acc did not improve\n",
      "28 0.001\n",
      "Epoch-29   20 batches\tloss 0.4207 (0.3810)\taccu 90.625 (95.703)\n",
      "Epoch-29   40 batches\tloss 0.3737 (0.3825)\taccu 96.875 (95.547)\n",
      "Epoch-29   60 batches\tloss 0.3621 (0.3840)\taccu 96.875 (95.208)\n",
      "Epoch-29   80 batches\tloss 0.3837 (0.3832)\taccu 95.312 (95.332)\n",
      "Epoch-29  100 batches\tloss 0.3847 (0.3827)\taccu 95.312 (95.422)\n",
      "Epoch-29  120 batches\tloss 0.4074 (0.3834)\taccu 92.188 (95.352)\n",
      "Epoch-29  140 batches\tloss 0.3739 (0.3824)\taccu 95.312 (95.402)\n",
      "Epoch-29  160 batches\tloss 0.3839 (0.3834)\taccu 93.750 (95.273)\n",
      "Epoch-29  180 batches\tloss 0.3639 (0.3825)\taccu 95.312 (95.391)\n",
      "Epoch-29  200 batches\tloss 0.3871 (0.3818)\taccu 92.188 (95.375)\n",
      "Epoch-29  220 batches\tloss 0.3856 (0.3811)\taccu 95.312 (95.440)\n",
      "Epoch-29  240 batches\tloss 0.3778 (0.3823)\taccu 95.312 (95.319)\n",
      "Epoch-29  260 batches\tloss 0.3731 (0.3820)\taccu 93.750 (95.319)\n",
      "Epoch-29  280 batches\tloss 0.3925 (0.3820)\taccu 96.875 (95.340)\n",
      "Epoch-29  300 batches\tloss 0.3432 (0.3812)\taccu 100.000 (95.380)\n",
      "Epoch-29  320 batches\tloss 0.3835 (0.3809)\taccu 95.312 (95.410)\n",
      "Epoch-29  340 batches\tloss 0.3664 (0.3809)\taccu 96.875 (95.427)\n",
      "Epoch-29  360 batches\tloss 0.3861 (0.3812)\taccu 95.312 (95.373)\n",
      "Epoch-29  380 batches\tloss 0.3531 (0.3809)\taccu 98.438 (95.391)\n",
      "Epoch-29  400 batches\tloss 0.3741 (0.3807)\taccu 96.875 (95.426)\n",
      "Epoch-29  420 batches\tloss 0.3666 (0.3807)\taccu 96.875 (95.420)\n",
      "Epoch-29  440 batches\tloss 0.3856 (0.3808)\taccu 95.312 (95.433)\n",
      "Epoch-29  460 batches\tloss 0.3740 (0.3808)\taccu 96.875 (95.414)\n",
      "Epoch-29  480 batches\tloss 0.3703 (0.3809)\taccu 96.875 (95.407)\n",
      "Epoch-29  500 batches\tloss 0.3508 (0.3808)\taccu 100.000 (95.422)\n",
      "Epoch-29  520 batches\tloss 0.4073 (0.3810)\taccu 93.750 (95.397)\n",
      "Epoch-29  540 batches\tloss 0.3464 (0.3808)\taccu 98.438 (95.405)\n",
      "Epoch-29  560 batches\tloss 0.3721 (0.3809)\taccu 96.875 (95.391)\n",
      "Epoch-29  580 batches\tloss 0.3661 (0.3808)\taccu 96.875 (95.383)\n",
      "Epoch-29  600 batches\tloss 0.3555 (0.3809)\taccu 98.438 (95.370)\n",
      "Epoch-29  620 batches\tloss 0.4060 (0.3810)\taccu 90.625 (95.353)\n",
      "Epoch-29  640 batches\tloss 0.3983 (0.3813)\taccu 95.312 (95.325)\n",
      "Epoch-29  660 batches\tloss 0.4415 (0.3823)\taccu 85.938 (95.223)\n",
      "Epoch-29  680 batches\tloss 0.4141 (0.3826)\taccu 93.750 (95.207)\n",
      "Epoch-29  700 batches\tloss 0.3656 (0.3826)\taccu 96.875 (95.219)\n",
      "Epoch-29  720 batches\tloss 0.3742 (0.3825)\taccu 93.750 (95.224)\n",
      "Epoch-29  740 batches\tloss 0.3923 (0.3824)\taccu 93.750 (95.226)\n",
      "Epoch-29  760 batches\tloss 0.3859 (0.3827)\taccu 95.312 (95.195)\n",
      "Epoch-29  780 batches\tloss 0.3923 (0.3828)\taccu 93.750 (95.192)\n",
      "Epoch-29  800 batches\tloss 0.3749 (0.3827)\taccu 95.312 (95.213)\n",
      "Epoch-29  820 batches\tloss 0.4238 (0.3826)\taccu 93.750 (95.227)\n",
      "Epoch-29  840 batches\tloss 0.3859 (0.3826)\taccu 95.312 (95.225)\n",
      "Epoch-29  860 batches\tloss 0.3846 (0.3824)\taccu 95.312 (95.243)\n",
      "Epoch-29  214.9s\tTrain: loss 0.3823\taccu 95.2604\tValid: loss 0.4548\taccu 89.6637\n",
      "Epoch 29: val_acc did not improve\n",
      "29 0.001\n",
      "Epoch-30   20 batches\tloss 0.4085 (0.3853)\taccu 92.188 (94.531)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    109\u001b[0m     test(test_loader, model, checkpoint, lable_path, pred_path)\n\u001b[1;32m--> 111\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[17], line 64\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(epoch, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m t_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 64\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train(\n\u001b[0;32m     65\u001b[0m     train_loader, model, criterion, optimizer, epoch)\n\u001b[0;32m     66\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m validate(val_loader, model, criterion)\n\u001b[0;32m     67\u001b[0m log_res \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [[train_loss, train_acc\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     68\u001b[0m              val_loss, val_acc\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()]]\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m acces \u001b[39m=\u001b[39m AverageMeter()\n\u001b[0;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      8\u001b[0m     output \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m      9\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\OneDrive\\Code\\Motion Privacy\\Attacking Models\\SGN Attack Model\\data.py:146\u001b[0m, in \u001b[0;36mNTUDataLoaders.collate_fn_fix_val\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m\"\"\"Puts each data field into a tensor with outer dimension batch size\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m--> 146\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTolist_fix(x, y, train\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    147\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x))\n\u001b[0;32m    148\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\OneDrive\\Code\\Motion Privacy\\Attacking Models\\SGN Attack Model\\data.py:174\u001b[0m, in \u001b[0;36mNTUDataLoaders.Tolist_fix\u001b[1;34m(self, joints, y, train)\u001b[0m\n\u001b[0;32m    172\u001b[0m zero_row \u001b[39m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(seq)):\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mif\u001b[39;00m (seq[i, :] \u001b[39m==\u001b[39;49m np\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m, \u001b[39m150\u001b[39;49m)))\u001b[39m.\u001b[39mall():\n\u001b[0;32m    175\u001b[0m         zero_row\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m    177\u001b[0m seq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(seq, zero_row, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    num_classes = get_num_classes(dataset)\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, do_train)\n",
    "\n",
    "    total = get_n_params(model)\n",
    "    # print(model)\n",
    "    print('The number of parameters: ', total)\n",
    "    print('The modes is:', network)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('It is using GPU!')\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = LabelSmoothingLoss(num_classes, smoothing=0.1).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if monitor == 'val_acc':\n",
    "        mode = 'max'\n",
    "        monitor_op = np.greater\n",
    "        best = -np.Inf\n",
    "        str_op = 'improve'\n",
    "    elif monitor == 'val_loss':\n",
    "        mode = 'min'\n",
    "        monitor_op = np.less\n",
    "        best = np.Inf\n",
    "        str_op = 'reduce'\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 90, 110], gamma=0.1)\n",
    "    # Data loading\n",
    "    ntu_loaders = NTUDataLoaders(dataset, case, seg=seg, train_X=train_x, train_Y=train_y, test_X=test_x, test_Y=test_y, val_X=val_x, val_Y=val_y, aug=0)\n",
    "    train_loader = ntu_loaders.get_train_loader(batch_size, workers)\n",
    "    val_loader = ntu_loaders.get_val_loader(batch_size, workers)\n",
    "    train_size = ntu_loaders.get_train_size()\n",
    "    val_size = ntu_loaders.get_val_size()\n",
    "\n",
    "    test_loader = ntu_loaders.get_test_loader(32, workers)\n",
    "\n",
    "    print('Train on %d samples, validate on %d samples' %\n",
    "          (train_size, val_size))\n",
    "\n",
    "    best_epoch = 0\n",
    "    output_dir = make_dir(dataset)\n",
    "\n",
    "    save_path = os.path.join(output_dir, network)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    checkpoint = osp.join(save_path, '%s_best.pth' % case)\n",
    "    earlystop_cnt = 0\n",
    "    csv_file = osp.join(save_path, '%s_log.csv' % case)\n",
    "    log_res = list()\n",
    "\n",
    "    lable_path = osp.join(save_path, '%s_lable.txt' % case)\n",
    "    pred_path = osp.join(save_path, '%s_pred.txt' % case)\n",
    "\n",
    "    # Training\n",
    "    if do_train == 1:\n",
    "        for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "            print(epoch, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            t_start = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                train_loader, model, criterion, optimizer, epoch)\n",
    "            val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "            log_res += [[train_loss, train_acc.cpu().numpy(),\n",
    "                         val_loss, val_acc.cpu().numpy()]]\n",
    "\n",
    "            print('Epoch-{:<3d} {:.1f}s\\t'\n",
    "                  'Train: loss {:.4f}\\taccu {:.4f}\\tValid: loss {:.4f}\\taccu {:.4f}'\n",
    "                  .format(epoch + 1, time.time() - t_start, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            current = val_loss if mode == 'min' else val_acc\n",
    "\n",
    "            # store tensor in cpu\n",
    "            current = current.cpu()\n",
    "\n",
    "            if monitor_op(current, best):\n",
    "                print('Epoch %d: %s %sd from %.4f to %.4f, '\n",
    "                      'saving model to %s'\n",
    "                      % (epoch + 1, monitor, str_op, best, current, checkpoint))\n",
    "                best = current\n",
    "                best_epoch = epoch + 1\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best': best,\n",
    "                    'monitor': monitor,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "                earlystop_cnt = 0\n",
    "            else:\n",
    "                print('Epoch %d: %s did not %s' % (epoch + 1, monitor, str_op))\n",
    "                earlystop_cnt += 1\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Best %s: %.4f from epoch-%d' % (monitor, best, best_epoch))\n",
    "        with open(csv_file, 'w') as fw:\n",
    "            cw = csv.writer(fw)\n",
    "            cw.writerow(['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "            cw.writerows(log_res)\n",
    "        print('Save train and validation log into into %s' % csv_file)\n",
    "\n",
    "    # Test\n",
    "    model = SGN(num_classes, dataset, seg, batch_size, 0)\n",
    "    model = model.cuda()\n",
    "    test(test_loader, model, checkpoint, lable_path, pred_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063dd9079dbbbc7ce9a24508feb60cfa7f5aa9bc9e0c912b3996301118c4566f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
